{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (4.67.1)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting editdistance\n",
      "  Downloading editdistance-0.8.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting h5py\n",
      "  Downloading h5py-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.11/site-packages (from opencv-python) (2.2.2)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.57.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (102 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (11.0.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting numpy>=1.21.2 (from opencv-python)\n",
      "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
      "  Downloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading matplotlib-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading editdistance-0.8.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (412 kB)\n",
      "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Downloading h5py-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.57.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Downloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Downloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
      "Installing collected packages: wrapt, pyparsing, numpy, kiwisolver, fonttools, editdistance, cycler, smart-open, scipy, opencv-python, h5py, contourpy, matplotlib, gensim\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.2\n",
      "    Uninstalling numpy-2.2.2:\n",
      "      Successfully uninstalled numpy-2.2.2\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 editdistance-0.8.1 fonttools-4.57.0 gensim-4.3.3 h5py-3.13.0 kiwisolver-1.4.8 matplotlib-3.10.1 numpy-1.26.4 opencv-python-4.11.0.86 pyparsing-3.2.3 scipy-1.13.1 smart-open-7.1.0 wrapt-1.17.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python tqdm matplotlib editdistance gensim h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apt-get install -y libgl1-mesa-glx\n",
    "# apt-get install -y libglib2.0-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "conjunct = ['091C_094D', '0915_094D', '0924_094D']\n",
    "charFolders = ['0905', '0905_0902', '0905_0903', '0906', '0907', '0908', '0909', '090A', '090F', '0910', '0913', '0914', '0915', '0915_093E', '0915_093F', '0915_0940', '0915_0941', '0915_0942', '0915_0947', '0915_0948', '0915_094B', '0915_094C', '0915_094D', '0915_094D_0937', '0915_094D_0937_0903', '0915_094D_0937_093E', '0915_094D_0937_0940', '0915_094D_0937_0941', '0915_094D_0937_0942', '0915_094D_0937_0947', '0915_094D_0937_0948', '0915_094D_0937_094B', '0915_094D_0937_094C', '0916', '0916_093E', '0916_093F', '0916_0941', '0916_0942', '0916_0948', '0916_094B', '0916_094C', '0916_094D', '0917', '0917_093E', '0917_093F', '0917_0940', '0917_0941', '0917_0942', '0917_0947', '0917_0948', '0917_094B', '0917_094C', '0917_094D', '0918', '0918_093E', '0918_093F', '0918_0940', '0918_0941', '0918_0942', '0918_0947', '0918_0948', '0918_094B', '0918_094C', '0918_094D', '0919', '0919_0902', '0919_0903', '0919_093E', '0919_093F', '0919_0940', '0919_0941', '0919_0942', '0919_0947', '0919_0948', '0919_094B', '0919_094C', '091A', '091A_0902', '091A_0903', '091A_093E', '091A_093F', '091A_0940', '091A_0941', '091A_0942', '091A_0947', '091A_0948', '091A_094B', '091A_094C', '091B', '091B_0902', '091B_0903', '091B_093E', '091B_093F', '091B_0940', '091B_0941', '091B_0942', '091B_0947', '091B_0948', '091B_094B', '091B_094C', '091C', '091C_0902', '091C_0903', '091C_093E', '091C_093F', '091C_0940', '091C_0941', '091C_0942', '091C_0947', '091C_0948', '091C_094B', '091C_094C', '091C_094D_091E', '091C_094D_091E_0902', '091C_094D_091E_0903', '091C_094D_091E_093E', '091C_094D_091E_093F', '091C_094D_091E_0940', '091C_094D_091E_0941', '091C_094D_091E_0942', '091C_094D_091E_0947', '091C_094D_091E_0948', '091C_094D_091E_094B', '091C_094D_091E_094C', '091D', '091D_0902', '091D_0903', '091D_093E', '091D_093F', '091D_0940', '091D_0941', '091D_0942', '091D_0947', '091D_0948', '091D_094B', '091D_094C', '091E', '091E_0902', '091E_0903', '091E_093E', '091E_093F', '091E_0940', '091E_0941', '091E_0942', '091E_0947', '091E_0948', '091E_094B', '091E_094C', '091F', '091F_0903', '091F_093E', '091F_093F', '091F_0940', '091F_0941', '091F_0942', '091F_0947', '091F_0948', '091F_094B', '091F_094C', '0920', '0920_0903', '0920_093E', '0920_093F', '0920_0940', '0920_0941', '0920_0942', '0920_0947', '0920_0948', '0920_094B', '0920_094C', '0921', '0921_0902', '0921_0903', '0921_093E', '0921_093F', '0921_0940', '0921_0941', '0921_0942', '0921_0947', '0921_0948', '0921_094B', '0921_094C', '0922', '0922_0902', '0922_0903', '0922_093E', '0922_093F', '0922_0940', '0922_0941', '0922_0942', '0922_0947', '0922_0948', '0922_094B', '0922_094C', '0923', '0923_0902', '0923_0903', '0923_093E', '0923_093F', '0923_0940', '0923_0941', '0923_0942', '0923_0947', '0923_0948', '0923_094B', '0923_094C', '0924', '0924_0902', '0924_0903', '0924_093E', '0924_093F', '0924_0940', '0924_0941', '0924_0942', '0924_0947', '0924_0948', '0924_094B', '0924_094C', '0924_094D_0930', '0924_094D_0930_0902', '0924_094D_0930_0903', '0924_094D_0930_093E', '0924_094D_0930_093F', '0924_094D_0930_0940', '0924_094D_0930_0941', '0924_094D_0930_0942', '0924_094D_0930_0947', '0924_094D_0930_0948', '0924_094D_0930_094B', '0924_094D_0930_094C', '0925', '0925_0902', '0925_093E', '0925_093F', '0925_0940', '0925_0941', '0925_0942', '0925_0947', '0925_0948', '0925_094B', '0925_094C', '0926', '0926_0902', '0926_0902_0903', '0926_0903', '0926_093E', '0926_093F', '0926_0940', '0926_0941', '0926_0942', '0926_0947', '0926_0948', '0926_094B', '0926_094C', '0927', '0927_0902', '0927_0903', '0927_093E', '0927_093F', '0927_0940', '0927_0941', '0927_0942', '0927_0947', '0927_0948', '0927_094B', '0927_094C', '0928', '0928_0903', '0928_093E', '0928_093F', '0928_0940', '0928_0941', '0928_0942', '0928_0947', '0928_0948', '0928_094B', '0928_094C', '092A', '092A_0902', '092A_0903', '092A_093E', '092A_093F', '092A_0940', '092A_0941', '092A_0942', '092A_0947', '092A_0948', '092A_094B', '092A_094C', '092B', '092B_0902', '092B_0903', '092B_093E', '092B_093F', '092B_0940', '092B_0941', '092B_0942', '092B_0947', '092B_0948', '092B_094B', '092B_094C', '092C', '092C_0902', '092C_0903', '092C_093E', '092C_093F', '092C_0940', '092C_0941', '092C_0942', '092C_0947', '092C_0948', '092C_094B', '092C_094C', '092D', '092D_0902', '092D_0903', '092D_093E', '092D_093F', '092D_0940', '092D_0941', '092D_0942', '092D_0947', '092D_0948', '092D_094B', '092D_094C', '092E', '092E_0902', '092E_0903', '092E_093E', '092E_093F', '092E_0940', '092E_0941', '092E_0942', '092E_0947', '092E_0948', '092E_0948_0902', '092E_094B', '092E_094C', '092F', '092F_0902', '092F_0903', '092F_093E', '092F_093F', '092F_0940', '092F_0941', '092F_0942', '092F_0947', '092F_0948', '092F_094B', '092F_094C', '0930', '0930_0902', '0930_0903', '0930_093E', '0930_093F', '0930_0940', '0930_0941', '0930_0942', '0930_0947', '0930_0948', '0930_094B', '0930_094C', '0932', '0932_0902', '0932_0903', '0932_093E', '0932_093F', '0932_0940', '0932_0941', '0932_0942', '0932_0947', '0932_0948', '0932_094B', '0932_094C', '0935', '0935_0902', '0935_0903', '0935_093E', '0935_093F', '0935_0940', '0935_0941', '0935_0942', '0935_0947', '0935_0948', '0935_094B', '0935_094C', '0936', '0936_0902', '0936_0903', '0936_093E', '0936_093F', '0936_0940', '0936_0941', '0936_0942', '0936_0947', '0936_0948', '0936_094B', '0936_094C', '0937', '0937_0902', '0937_0903', '0937_093E', '0937_093F', '0937_0940', '0937_0941', '0937_0942', '0937_0947', '0937_0948', '0937_094B', '0937_094C', '0938', '0938_0902', '0938_0903', '0938_093E', '0938_093F', '0938_0940', '0938_0941', '0938_0942', '0938_0947', '0938_0948', '0938_094B', '0938_094C', '0939', '0939_0902', '0939_0903', '0939_093E', '0939_093F', '0939_0940', '0939_0941', '0939_0942', '0939_0947', '0939_0948', '0939_094B', '0939_094C', '0966', '0967', '0968', '0969', '096A', '096B', '096C', '096D', '096E', '096F']\n",
    "\n",
    "def getLetterTokens(word: str):\n",
    "\n",
    "    wordComb = []\n",
    "    approved_words = []\n",
    "\n",
    "    # for each word in dict\n",
    "\n",
    "    hindi_word = word\n",
    "    charList = []\n",
    "\n",
    "    if(len(word) <= 31): # set word length limit to 31 characters (including matras)\n",
    "        word = word.replace('\\n', '')\n",
    "        characters = []\n",
    "        for ch in word: # convert letters to unicode representations and store\n",
    "            characters.append(('0' + hex(ord(ch))[2:]).upper())\n",
    "        \n",
    "        check = True\n",
    "        i = 0\n",
    "        # for each unicode character representation of current word\n",
    "        while check and i < len(characters):\n",
    "            check = False\n",
    "            word = ''\n",
    "\n",
    "            # add join current and next char\n",
    "            if i < len(characters) - 1:\n",
    "                word = characters[i] + '_' + characters[i+1]\n",
    "            \n",
    "            # if half (halant) letter exists in combination to next character\n",
    "            if word in conjunct and i < len(characters) - 2:\n",
    "                word2 = word + '_' + characters[i+2] # join with next character to check for more possibilities\n",
    "                if word2 in charFolders: # if the current handwritten character combination exists\n",
    "                    # if still not reached end and concatenation of next character exists in folder\n",
    "                    if i < len(characters) - 3 and word2 + '_' + characters[i+3] in charFolders:\n",
    "                        charList.append(word2 + '_' + characters[i+3]) # add to charlist\n",
    "                        check = True\n",
    "                        i += 4\n",
    "                    else: # add the next character as a seperate sequence element\n",
    "                        charList.append(word2)\n",
    "                        check = True\n",
    "                        i += 3\n",
    "            \n",
    "            # above if condition only adds char to charlist if that subsequence is found in folder names, and so check is set to true\n",
    "            # below, if check is false, only then we fall back to the word (character[i] + character[i+1])\n",
    "            # or character (character[i]) combination addition to charlist\n",
    "            \n",
    "            # check if word (character[i] + character[i+1]) combination exists and adds to charlist\n",
    "            # and sets flag to true\n",
    "            if check == False and word in charFolders:\n",
    "                check = True\n",
    "                charList.append(word)\n",
    "                i += 2\n",
    "            \n",
    "            # if word also does not exist, then only the character is added to charlist given that it exists in the folder\n",
    "            if check == False and characters[i] in charFolders:\n",
    "                check = True\n",
    "                charList.append(characters[i])\n",
    "                i += 1\n",
    "        \n",
    "        # appends all information for that word as well as the annotated word\n",
    "        if check == True:\n",
    "            wordComb.append((charList, word))\n",
    "            approved_words.append(hindi_word)\n",
    "    \n",
    "    # if(wordComb != []):\n",
    "    return wordComb[0][0], approved_words\n",
    "    # else:\n",
    "    #     return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torch.nn import init\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg # <<<--- ADD THIS IMPORT\n",
    "import numpy as np\n",
    "import random, os, re, pickle, h5py, math, glob\n",
    "import itertools, time, cv2, editdistance, cv2\n",
    "from collections import OrderedDict\n",
    "from datetime import datetime # For timestamp\n",
    "from tqdm import tqdm\n",
    "from gensim.models.fasttext import load_facebook_model\n",
    "# import pytorch_msssim, editdistance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HindiWordDataset(Dataset):\n",
    "    def __init__(self, txt_file, n_samples):\n",
    "        with open(txt_file, 'r', encoding='utf-8') as f:\n",
    "            lines = f.read().splitlines()\n",
    "        words = [line.strip() for line in lines if line.strip()]\n",
    "        if n_samples > len(words):\n",
    "            raise ValueError(f\"Requested {n_samples} samples, but file only contains {len(words)} words.\")\n",
    "        self.words = random.sample(words, n_samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.words[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairDataloader(Dataset):\n",
    "    def __init__(self, npy_dir, percentage=100.0, max_images_per_class=1000, img_height=64,\n",
    "                 full_img_width=256, images_per_row=100, background_color=0, transforms=None):\n",
    "        super().__init__()\n",
    "        self.npy_dir = npy_dir\n",
    "        self.percentage = max(0.0, min(100.0, percentage))\n",
    "        self.max_images_per_class = max_images_per_class # Added max cap\n",
    "        self.img_height = img_height\n",
    "        self.full_img_width = full_img_width\n",
    "        self.patch_width = full_img_width // 2\n",
    "        self.images_per_row = images_per_row\n",
    "        self.background_color = background_color\n",
    "        self.transforms = transforms\n",
    "        self.data_per_class = {}\n",
    "        self.class_names_loaded = []\n",
    "        self.metadata = []\n",
    "        self.cumulative_lengths = [0]\n",
    "        self.total_length = 0\n",
    "        self._load_metadata_and_data()\n",
    "\n",
    "    def _load_metadata_and_data(self):\n",
    "        npy_files = sorted(glob.glob(os.path.join(self.npy_dir, \"*.npy\")))\n",
    "        for npy_path in npy_files:\n",
    "            basename = os.path.basename(npy_path)\n",
    "            parts = basename.rsplit('_', 2)\n",
    "            if len(parts) == 3 and parts[1].isdigit() and parts[2] == \"images.npy\":\n",
    "                class_name = parts[0]\n",
    "                image_count_in_file = int(parts[1]) # Original count from filename\n",
    "                self.metadata.append((class_name, image_count_in_file, npy_path))\n",
    "\n",
    "        # --- Use tqdm wrapper for the loading loop ---\n",
    "        for class_name, image_count_in_file, npy_path in tqdm(self.metadata, desc=\"Loading .npy files\", unit=\"class\"):\n",
    "            # --- Apply max image cap ---\n",
    "            effective_image_count = min(image_count_in_file, self.max_images_per_class)\n",
    "\n",
    "            # --- Calculate based on capped count and percentage ---\n",
    "            num_images_to_load = math.ceil(effective_image_count * self.percentage / 100.0)\n",
    "\n",
    "            if num_images_to_load <= 0:\n",
    "                continue\n",
    "            try:\n",
    "                loaded_array = np.load(npy_path)\n",
    "                expected_row_width = self.images_per_row * self.full_img_width\n",
    "                if loaded_array.shape[1] != self.img_height or loaded_array.shape[2] != expected_row_width:\n",
    "                     tqdm.write(f\"Warning: Skipping {os.path.basename(npy_path)} due to unexpected shape.\") # Use tqdm.write\n",
    "                     continue\n",
    "\n",
    "                num_rows_in_file = loaded_array.shape[0]\n",
    "                split_rows = np.split(loaded_array, indices_or_sections=self.images_per_row, axis=2)\n",
    "                stacked_images = np.concatenate(split_rows, axis=0)\n",
    "\n",
    "                # --- Slice based on *original* count first, then capped count for percentage ---\n",
    "                # Ensure we only consider images that actually exist in the file\n",
    "                valid_images_in_file = stacked_images[:image_count_in_file]\n",
    "                # Now slice this based on the calculated number to load (which respects max_cap and percentage)\n",
    "                images_to_keep = valid_images_in_file[:num_images_to_load]\n",
    "\n",
    "                if images_to_keep.size == 0:\n",
    "                     continue # Skip if slicing resulted in empty\n",
    "\n",
    "                self.data_per_class[class_name] = images_to_keep\n",
    "                if class_name not in self.class_names_loaded:\n",
    "                    self.class_names_loaded.append(class_name)\n",
    "                num_loaded_for_class = len(images_to_keep)\n",
    "                self.total_length += num_loaded_for_class\n",
    "                self.cumulative_lengths.append(self.total_length)\n",
    "            except Exception as e:\n",
    "                # Use tqdm.write for errors to avoid messing up the progress bar\n",
    "                tqdm.write(f\"\\nError processing {os.path.basename(npy_path)}: {e}\")\n",
    "                pass # Continue to next file\n",
    "\n",
    "        self.class_names_loaded.sort()\n",
    "        print(f\"\\nFinished loading. Total images loaded: {self.total_length}\") # Newline after tqdm\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # --- Original index finding logic ---\n",
    "        class_offset = np.searchsorted(self.cumulative_lengths, idx, side='right') - 1\n",
    "        # Ensure class_offset is valid (can be -1 if idx is 0 and cumulative_lengths[0] != 0, but ours starts with 0)\n",
    "        class_offset = max(0, class_offset) # Safeguard for idx=0 edge case if needed\n",
    "        class_name = self.class_names_loaded[class_offset]\n",
    "        index_within_class = idx - self.cumulative_lengths[class_offset]\n",
    "        # --- End original logic ---\n",
    "\n",
    "        # --- !! ADD BOUNDARY CHECK AND CORRECTION !! ---\n",
    "        actual_class_data = self.data_per_class[class_name]\n",
    "        actual_len_for_class = actual_class_data.shape[0]\n",
    "\n",
    "        if index_within_class >= actual_len_for_class:\n",
    "            # This is the error condition!\n",
    "            # print(f\"!!! WARNING: Index Correction Applied !!!\")\n",
    "            # print(f\"    Global idx: {idx}, Total Len: {self.total_length}\")\n",
    "            # print(f\"    Target Class: '{class_name}', Offset: {class_offset}\")\n",
    "            # print(f\"    Calculated index_within_class: {index_within_class}\")\n",
    "            # print(f\"    Actual length for class: {actual_len_for_class}\")\n",
    "            # print(f\"    Cumulative lengths snippet: {self.cumulative_lengths[max(0,class_offset-1):class_offset+3]}\")\n",
    "            # Clamp the index to the maximum valid index\n",
    "            index_within_class = actual_len_for_class - 1\n",
    "            # print(f\"    Clamped index_within_class to: {index_within_class}\")\n",
    "        elif index_within_class < 0:\n",
    "             # Should not happen with current logic, but good practice\n",
    "             # print(f\"!!! WARNING: Negative Index Corrected !!!\")\n",
    "             # print(f\"    Global idx: {idx}, Calculated index_within_class: {index_within_class}\")\n",
    "             index_within_class = 0 # Clamp to 0\n",
    "        # --- !! END BOUNDARY CHECK !! ---\n",
    "\n",
    "\n",
    "        # Get the full 64x256 image using the potentially corrected index\n",
    "        full_image_np = actual_class_data[index_within_class]\n",
    "\n",
    "        # Split into input (context) and target (generation) patches\n",
    "        input_patch_np = full_image_np[:, :self.patch_width]\n",
    "        target_patch_np = full_image_np[:, self.patch_width:]\n",
    "\n",
    "        # Convert NumPy arrays to PyTorch Tensors and Normalize [0, 1]\n",
    "        input_tensor = torch.from_numpy(input_patch_np).unsqueeze(0).float() / 255.0\n",
    "        target_tensor = torch.from_numpy(target_patch_np).unsqueeze(0).float() / 255.0\n",
    "\n",
    "        if self.transforms:\n",
    "             input_tensor = self.transforms(input_tensor)\n",
    "             target_tensor = self.transforms(target_tensor)\n",
    "\n",
    "        # Return input patch, target patch, and the class name string\n",
    "        return input_tensor, target_tensor, class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dataset = HindiWordDataset(\"annotations.txt\", n_samples=200000)\n",
    "word_dataloader = DataLoader(word_dataset, batch_size=128, shuffle=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading .npy files: 100%|█████████████████████████████████████████████████████████████| 400/400 [00:02<00:00, 146.84class/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished loading. Total images loaded: 20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "NPY_FOLDER = \"./LetterClassPairsCompressed\"\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "try:\n",
    "    del dataset\n",
    "    del dataloader\n",
    "except: pass\n",
    "\n",
    "dataset = PairDataloader(\n",
    "    npy_dir=NPY_FOLDER,\n",
    "    percentage=5.0,\n",
    "    max_images_per_class=999, # Pass the cap\n",
    "    img_height=64,\n",
    "    full_img_width=256,\n",
    "    images_per_row=100,\n",
    "    background_color=0,\n",
    "    transforms=None\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    pin_memory=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes loaded: 400\n",
      "\n",
      "--- Sample Batch ---\n",
      "Input Patch Shape:  torch.Size([128, 1, 64, 128])\n",
      "Target Patch Shape: torch.Size([128, 1, 64, 128])\n",
      "Class Names (sample): ('092D', '091C_0941', '0939_094C', '092F', '091D_0902', '0969', '092F_0948', '092D_0940', '0916_094D', '0915_0948', '091D_0941', '0930_094B', '092F_094C', '0936_0941', '092D_0948', '091B_093F', '0924_0940', '092A_0941', '092A_0948', '0937_094B', '0915_094D_0937', '092E', '092A_0948', '0938_0940', '092C_093E', '0923_094B', '0915_0948', '092A_0903', '0938_0902', '092A_0941', '0925', '091F_0948', '091D_0942', '0921_094B', '092A_094C', '0921_0942', '0936', '0935_0948', '0918_094D', '0913', '0915_0941', '091C_094D_091E_0947', '0923_094C', '0928_0942', '0925_094B', '0935_094C', '092C_0940', '092C_0940', '0926_0947', '0925_093E', '091F_0940', '0919_0940', '0918_093F', '0909', '091A_0942', '091B_0902', '0920_0940', '0920', '0925_0947', '092F_094B', '091A_0902', '0924_0942', '0935_094B', '0922_0940', '0917_0942', '091E_0941', '091D_0942', '0915_094D_0937_0941', '0920_094C', '092D_0940', '0935_0902', '0915_0942', '0920_0947', '0916_094D', '0925_0941', '0917_094D', '092E_0903', '091D_094C', '0936_0941', '0927_0903', '0915_0941', '0969', '0927_0903', '0928_0942', '0926_094B', '091D_0942', '0936', '092D_093F', '0924_093E', '092F_094C', '092B_0942', '092A_094B', '0936_0941', '092F_0941', '092A_0902', '091B_0940', '0938_093E', '0939_0942', '0924_0948', '0918_0942', '0926', '0915_0941', '091C_094D_091E_0947', '0920_094B', '0930_0942', '091A_0902', '0918_0941', '0915_094D', '0923_0902', '0926_0903', '0939_093E', '0939_094C', '091B_0942', '092D_0947', '0917_0940', '091B', '0915_094D_0937_0947', '0918_0942', '0924_0942', '092C_0902', '0918_0940', '0930_0947', '091A_0942', '0923_0942', '091D_093F', '091C_094D_091E_093F', '092D_0942', '0924_094D_0930_093F')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### print(f\"Dataset Length: {len(dataset)}\")\n",
    "print(f\"Number of classes loaded: {len(dataset.class_names_loaded)}\")\n",
    "\n",
    "# Fetch one batch\n",
    "input_batch, target_batch, class_name_batch = next(iter(dataloader))\n",
    "\n",
    "print(\"\\n--- Sample Batch ---\")\n",
    "print(f\"Input Patch Shape:  {input_batch.shape}\")\n",
    "print(f\"Target Patch Shape: {target_batch.shape}\")\n",
    "print(f\"Class Names (sample): {class_name_batch[:BATCH_SIZE]}\")\n",
    "\n",
    "# Optional: Show one image pair from the batch\n",
    "# import matplotlib.pyplot as plt\n",
    "sample_idx = random.randint(0, 3)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 3))\n",
    "axes[0].imshow(input_batch[sample_idx, 0].cpu().numpy(), cmap='gray')\n",
    "axes[0].set_title(f\"Input Patch (Class '{class_name_batch[sample_idx]}')\")\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(target_batch[sample_idx, 0].cpu().numpy(), cmap='gray')\n",
    "axes[1].set_title(f\"Target Patch (Class '{class_name_batch[sample_idx]}')\")\n",
    "axes[1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.hi.300.bin.gz\n",
    "# !gunzip cc.hi.300.bin.gz\n",
    "# !ls -lh cc.hi.300.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"model\" not in globals():\n",
    "    model = load_facebook_model(\"../cc.hi.300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastTextEncoder(nn.Module):\n",
    "    def __init__(self): super().__init__()\n",
    "    def forward(self, letters_encoded):\n",
    "        \n",
    "        def unicode_to_hindi(lst):\n",
    "            return [\"\".join(chr(int(c, 16)) for c in s.split(\"_\")) for s in lst]\n",
    "        \n",
    "        def getEmbedding(text: str):\n",
    "            strings = unicode_to_hindi(text)\n",
    "            # print(strings)\n",
    "            return np.array([model.wv[x] for x in strings])\n",
    "\n",
    "        return getEmbedding(letters_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalLSTM(nn.Module):\n",
    "    def __init__(self, nIn, nHidden, nOut):\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "        self.rnn = nn.LSTM(nIn, nHidden, bidirectional=True)\n",
    "        self.embedding = nn.Linear(nHidden * 2, nOut)\n",
    "\n",
    "    def forward(self, input):\n",
    "        recurrent, _ = self.rnn(input)\n",
    "        T, b, h = recurrent.size()\n",
    "        t_rec = recurrent.view(T * b, h)\n",
    "        output = self.embedding(t_rec)  # [T * b, nOut]\n",
    "        output = output.view(T, b, -1)\n",
    "        return output\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, opt, leakyRelu=False):\n",
    "        super(CRNN, self).__init__()\n",
    "        self.name = 'OCR'\n",
    "        assert opt.imgH % 16 == 0, 'imgH has to be a multiple of 16'\n",
    "\n",
    "        ks = [3, 3, 3, 3, 3, 3, 2]\n",
    "        ps = [1, 1, 1, 1, 1, 1, 0]\n",
    "        ss = [1, 1, 1, 1, 1, 1, 1]\n",
    "        nm = [64, 128, 256, 256, 512, 512, 512]\n",
    "\n",
    "        cnn = nn.Sequential()\n",
    "        # Use opt.input_nc instead of opt.num_layers_OCR for input channels\n",
    "        nIn_initial = opt.input_nc\n",
    "        nh = opt.hidden_size_OCR\n",
    "        dealwith_lossnone = opt.dealwith_lossnone # whether to replace all nan/inf in gradients to zero\n",
    "\n",
    "        def convRelu(i, batchNormalization=False):\n",
    "            nIn = nIn_initial if i == 0 else nm[i - 1]\n",
    "            nOut = nm[i]\n",
    "            cnn.add_module('conv{0}'.format(i),\n",
    "                           nn.Conv2d(nIn, nOut, ks[i], ss[i], ps[i]))\n",
    "            if batchNormalization:\n",
    "                cnn.add_module('batchnorm{0}'.format(i), nn.BatchNorm2d(nOut))\n",
    "            if leakyRelu:\n",
    "                cnn.add_module('relu{0}'.format(i),\n",
    "                               nn.LeakyReLU(0.2, inplace=True))\n",
    "            else:\n",
    "                cnn.add_module('relu{0}'.format(i), nn.ReLU(True))\n",
    "\n",
    "        convRelu(0)\n",
    "        cnn.add_module('pooling{0}'.format(0), nn.MaxPool2d(2, 2))  # 64 -> 32\n",
    "        convRelu(1)\n",
    "        cnn.add_module('pooling{0}'.format(1), nn.MaxPool2d(2, 2))  # 32 -> 16\n",
    "        convRelu(2, True)\n",
    "        convRelu(3)\n",
    "        cnn.add_module('pooling{0}'.format(2),\n",
    "                       nn.MaxPool2d((2, 2), (2, 1), (0, 1)))  # 16 -> 8\n",
    "        convRelu(4, True)\n",
    "        # Removed the conditional pooling based on opt.resolution==63 as it wasn't mentioned\n",
    "        # if opt.resolution==63:\n",
    "        #     cnn.add_module('pooling{0}'.format(3),\n",
    "        #                    nn.MaxPool2d((2, 2), (2, 1), (0, 1)))\n",
    "        convRelu(5)\n",
    "        cnn.add_module('pooling{0}'.format(4),\n",
    "                       nn.MaxPool2d((2, 2), (2, 1), (0, 1)))  # 8 -> 4\n",
    "        convRelu(6, True) # Input H=4 -> Output H = floor((4-2+0)/1 + 1) = 3.  Let's adjust pool4 or conv6\n",
    "        # Let's adjust Pool4 or Conv6 to ensure H=1 at the end.\n",
    "        # Option 1: Change Pool4 kernel/stride\n",
    "        # Option 2: Change Conv6 kernel/stride/padding\n",
    "        # Let's try changing Conv6 to ensure H=1 with input H=4\n",
    "        # Required output H = 1. Input H = 4.\n",
    "        # H_out = floor((H_in + 2*pad - dilation*(kernel-1) -1 ) / stride + 1)\n",
    "        # If stride=1, kernel=3, pad=0: floor((4 + 0 - 1*(2) - 1)/1 + 1) = floor(1) + 1 = 2. Incorrect.\n",
    "        # If stride=1, kernel=4, pad=0: floor((4 + 0 - 1*(3) - 1)/1 + 1) = floor(0) + 1 = 1. Correct.\n",
    "        # Let's modify the last Conv layer (layer 6)\n",
    "        cnn = nn.Sequential(*list(cnn.children())[:-3]) # Remove old conv6, bn6, relu6\n",
    "        nIn = nm[5]\n",
    "        nOut = nm[6]\n",
    "        # New Conv6 layer: ks=4, ps=0, ss=1\n",
    "        cnn.add_module('conv{0}'.format(6), nn.Conv2d(nIn, nOut, kernel_size=4, stride=1, padding=0)) # H: 4 -> 1\n",
    "        cnn.add_module('batchnorm{0}'.format(6), nn.BatchNorm2d(nOut))\n",
    "        if leakyRelu:\n",
    "             cnn.add_module('relu{0}'.format(6), nn.LeakyReLU(0.2, inplace=True))\n",
    "        else:\n",
    "             cnn.add_module('relu{0}'.format(6), nn.ReLU(True))\n",
    "\n",
    "        self.cnn = cnn\n",
    "        self.use_rnn = opt.use_rnn\n",
    "        if self.use_rnn:\n",
    "            self.rnn = nn.Sequential(\n",
    "                BidirectionalLSTM(nm[6], nh, nh), # Input size = 512\n",
    "                BidirectionalLSTM(nh, nh, opt.len_vocab))\n",
    "        else:\n",
    "            # Note: If not using RNN, the output shape might need adjustment depending on downstream task\n",
    "            self.linear = nn.Linear(nm[6], opt.len_vocab) # Input size = 512\n",
    "\n",
    "        # replace all nan/inf in gradients to zero\n",
    "        if dealwith_lossnone:\n",
    "            self.register_backward_hook(self.backward_hook)\n",
    "\n",
    "        # Initialize weights (if function exists and opt allows)\n",
    "        if hasattr(opt, 'skip_init') and not opt.skip_init:\n",
    "             self = init_weights(self, opt.OCR_init) # Pass init type if specified in opt\n",
    "\n",
    "    def forward(self, input):\n",
    "        # conv features\n",
    "        # print(f\"Input shape: {input.shape}\")\n",
    "        conv = self.cnn(input)\n",
    "        # print(f\"CNN output shape: {conv.shape}\")\n",
    "        b, c, h, w = conv.size()\n",
    "\n",
    "        # **Important Check**: After modifying Conv6, let's re-verify the height\n",
    "        # Input H=64. Pool0->32. Pool1->16. Pool2->8. Pool4->4. Conv6(k=4,s=1,p=0)->1.\n",
    "        assert h == 1, f\"ERROR: the height of conv features must be 1, but got {h}. Check CNN pooling/conv layers.\"\n",
    "\n",
    "        conv = conv.squeeze(2) # Remove height dim: [b, c, w]\n",
    "        conv = conv.permute(2, 0, 1)  # [w, b, c] - Sequence first for RNN\n",
    "\n",
    "        if self.use_rnn:\n",
    "            # rnn features\n",
    "            output = self.rnn(conv) # Output shape [w, b, num_classes]\n",
    "        else:\n",
    "            # Apply linear layer to each time step\n",
    "             output = self.linear(conv) # Output shape [w, b, num_classes]\n",
    "\n",
    "        return output # Shape: [sequence_length, batch_size, num_classes]\n",
    "\n",
    "    def backward_hook(self, module, grad_input, grad_output):\n",
    "        # print(\"Backward hook called\") # For debugging\n",
    "        for i, g in enumerate(grad_input):\n",
    "            if g is not None:\n",
    "                 # print(f\"Grad input {i} contains NaNs: {torch.isnan(g).any()}\")\n",
    "                 # print(f\"Grad input {i} contains Infs: {torch.isinf(g).any()}\")\n",
    "                 g = torch.nan_to_num(g, nan=0.0, posinf=0.0, neginf=0.0) # More robust way\n",
    "                 grad_input = list(grad_input) # grad_input is a tuple, convert to list to modify\n",
    "                 grad_input[i] = g\n",
    "                 grad_input = tuple(grad_input) # Convert back to tuple\n",
    "\n",
    "        for i, g in enumerate(grad_output):\n",
    "             if g is not None:\n",
    "                 # print(f\"Grad output {i} contains NaNs: {torch.isnan(g).any()}\")\n",
    "                 # print(f\"Grad output {i} contains Infs: {torch.isinf(g).any()}\")\n",
    "                 g = torch.nan_to_num(g, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                 grad_output = list(grad_output)\n",
    "                 grad_output[i] = g\n",
    "                 grad_output = tuple(grad_output)\n",
    "\n",
    "        return grad_input # Return modified grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn # Required for pad_sequence\n",
    "\n",
    "class strLabelConverter(object):\n",
    "    \"\"\"Convert between str and label.\n",
    "    NOTE:\n",
    "        Insert `blank` to the alphabet for CTC.\n",
    "    Args:\n",
    "        alphabet (str): set of the possible characters.\n",
    "        ignore_case (bool, default=True): whether or not to ignore all of the case.\n",
    "    \"\"\"\n",
    "    def __init__(self, alphabet, ignore_case=False):\n",
    "        self._ignore_case = ignore_case\n",
    "        if self._ignore_case:\n",
    "            alphabet = alphabet.lower()\n",
    "        # Define alphabet and dict for mapping characters to indices\n",
    "        self.alphabet = alphabet\n",
    "        self.dict = {}\n",
    "        # NOTE: Assign 0 for blank used by CTC\n",
    "        self.dict['-'] = 0 # Explicitly assign 0 to blank\n",
    "        for i, char in enumerate(alphabet):\n",
    "            # Start character indices from 1 (0 is blank)\n",
    "             if char != '-': # Don't re-assign blank\n",
    "                 self.dict[char] = i + 1\n",
    "        self.num_classes = len(alphabet) + 1 # +1 for blank\n",
    "\n",
    "    def encode(self, text_list):\n",
    "        \"\"\"Support batch or single str. Encodes using padding.\n",
    "        Args:\n",
    "            text_list (list[str] or list[bytes]): texts to convert.\n",
    "        Returns:\n",
    "            torch.LongTensor [batch_size, max_seq_len]: encoded texts padded with 0.\n",
    "            torch.IntTensor [batch_size]: length of each text BEFORE padding.\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(text_list) or isinstance(text_list, (np.ndarray, np.generic)):\n",
    "             # Handle case where numpy array or tensor might be passed (unlikely)\n",
    "             text_list = text_list.tolist()\n",
    "\n",
    "        length = []\n",
    "        results_indices = [] # List of lists containing indices for each word\n",
    "\n",
    "        for item in text_list:\n",
    "            # Ensure item is decoded to string if it's bytes\n",
    "            if isinstance(item, bytes):\n",
    "                try:\n",
    "                    item_str = item.decode('utf-8', 'strict')\n",
    "                except UnicodeDecodeError:\n",
    "                    print(f\"Warning: Cannot decode bytes: {item}. Skipping.\")\n",
    "                    length.append(0)\n",
    "                    results_indices.append([]) # Add empty list for this sample\n",
    "                    continue # Skip this problematic item\n",
    "            elif isinstance(item, str):\n",
    "                item_str = item\n",
    "            else:\n",
    "                 print(f\"Warning: Unexpected item type in text_list: {type(item)}. Skipping.\")\n",
    "                 length.append(0)\n",
    "                 results_indices.append([])\n",
    "                 continue\n",
    "\n",
    "            current_indices = []\n",
    "            for char in item_str:\n",
    "                try:\n",
    "                    index = self.dict[char.lower() if self._ignore_case else char]\n",
    "                    current_indices.append(index)\n",
    "                except KeyError:\n",
    "                    # Handle characters not in the alphabet (e.g., assign unknown or skip)\n",
    "                    print(f\"Warning: Character '{char}' not in alphabet. Skipping character.\")\n",
    "                    # Option: Assign an <UNK> token index if you have one\n",
    "                    # Or just skip the character\n",
    "\n",
    "            length.append(len(current_indices)) # Store length BEFORE padding\n",
    "            results_indices.append(torch.LongTensor(current_indices)) # Store as LongTensor\n",
    "\n",
    "        # Pad the sequences to the maximum length in the batch\n",
    "        # pad_sequence expects a list of Tensors (shape [L])\n",
    "        # Use padding_value=0 (CTC blank index)\n",
    "        padded_results = nn.utils.rnn.pad_sequence(results_indices, batch_first=True, padding_value=0)\n",
    "\n",
    "        return (padded_results, torch.IntTensor(length))\n",
    "\n",
    "    # --- Keep your decode method as is ---\n",
    "    def decode(self, encoded_sequence, sequence_lengths, raw=False):\n",
    "        \"\"\"Decode sequences.\"\"\"\n",
    "        texts = []\n",
    "        current_index = 0\n",
    "        for i in range(sequence_lengths.numel()):\n",
    "            length = sequence_lengths[i]\n",
    "            if encoded_sequence.dim() == 2:\n",
    "                t = encoded_sequence[i, :length]\n",
    "            else:\n",
    "                t = encoded_sequence[current_index : current_index + length]\n",
    "\n",
    "            if raw:\n",
    "                 chars = [self.alphabet[idx - 1] for idx in t if idx > 0 and idx <= len(self.alphabet)]\n",
    "            else:\n",
    "                char_list = []\n",
    "                for j in range(length):\n",
    "                    if t[j] != 0 and (j == 0 or t[j-1] != t[j]):\n",
    "                         if t[j] <= len(self.alphabet):\n",
    "                              char_list.append(self.alphabet[t[j] - 1])\n",
    "                chars = char_list\n",
    "            texts.append(''.join(chars))\n",
    "            if encoded_sequence.dim() != 2: current_index += length\n",
    "        return texts\n",
    "\n",
    "# --- Minimal Prediction Decoder ---\n",
    "def decode_crnn_predictions(crnn_output, converter):\n",
    "    \"\"\"Greedy decodes the raw output of the CRNN model.\"\"\"\n",
    "    # crnn_output shape: [sequence_length, batch_size, num_classes]\n",
    "    # No need for log_softmax if using greedy argmax directly\n",
    "    pred_indices = torch.argmax(crnn_output, dim=2) # [sequence_length, batch_size]\n",
    "    pred_indices = pred_indices.permute(1, 0) # -> [batch_size, sequence_length]\n",
    "\n",
    "    batch_size, seq_len = pred_indices.shape\n",
    "    # The input length for CTC decode is the full sequence length from CNN output\n",
    "    input_lengths = torch.full(size=(batch_size,), fill_value=seq_len, dtype=torch.long)\n",
    "\n",
    "    decoded_texts = converter.decode(pred_indices, input_lengths, raw=False)\n",
    "    return decoded_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # --- Minimal Label Converter ---\n",
    "# class OCRLabelConverter(object):\n",
    "#     \"\"\"Convert between str and label for CTC loss.\"\"\"\n",
    "#     def __init__(self, alphabet):\n",
    "#         self.alphabet = alphabet\n",
    "#         self.dict = {}\n",
    "#         # Assign 0 for blank, start indexing others from 1\n",
    "#         for i, char in enumerate(alphabet):\n",
    "#              self.dict[char] = i + 1\n",
    "#         self.num_classes = len(alphabet) + 1 # +1 for blank\n",
    "\n",
    "#     def encode(self, text_list):\n",
    "#         \"\"\"Encode list of strings.\"\"\"\n",
    "#         length = []\n",
    "#         result = []\n",
    "#         for item in text_list:\n",
    "#             current_length = 0\n",
    "#             for char in item:\n",
    "#                 try:\n",
    "#                     index = self.dict[char]\n",
    "#                     result.append(index)\n",
    "#                     current_length += 1\n",
    "#                 except KeyError:\n",
    "#                     # Silently skip unknown characters during inference encoding if necessary\n",
    "#                     pass\n",
    "#             length.append(current_length)\n",
    "#         encoded_text = torch.IntTensor(result)\n",
    "#         text_lengths = torch.IntTensor(length)\n",
    "#         return encoded_text, text_lengths\n",
    "\n",
    "#     def decode(self, encoded_sequence, sequence_lengths, raw=False):\n",
    "#         \"\"\"Decode sequences.\"\"\"\n",
    "#         texts = []\n",
    "#         current_index = 0\n",
    "#         for i in range(sequence_lengths.numel()):\n",
    "#             length = sequence_lengths[i]\n",
    "#             if encoded_sequence.dim() == 2:\n",
    "#                 t = encoded_sequence[i, :length]\n",
    "#             else:\n",
    "#                 t = encoded_sequence[current_index : current_index + length]\n",
    "\n",
    "#             if raw:\n",
    "#                  chars = [self.alphabet[idx - 1] for idx in t if idx > 0 and idx <= len(self.alphabet)]\n",
    "#             else:\n",
    "#                 char_list = []\n",
    "#                 for j in range(length):\n",
    "#                     if t[j] != 0 and (j == 0 or t[j-1] != t[j]):\n",
    "#                          if t[j] <= len(self.alphabet):\n",
    "#                               char_list.append(self.alphabet[t[j] - 1])\n",
    "#                 chars = char_list\n",
    "#             texts.append(''.join(chars))\n",
    "#             if encoded_sequence.dim() != 2: current_index += length\n",
    "#         return texts\n",
    "\n",
    "# # --- Minimal Prediction Decoder ---\n",
    "# def decode_crnn_predictions(crnn_output, converter):\n",
    "#     \"\"\"Greedy decodes the raw output of the CRNN model.\"\"\"\n",
    "#     # crnn_output shape: [sequence_length, batch_size, num_classes]\n",
    "#     # No need for log_softmax if using greedy argmax directly\n",
    "#     pred_indices = torch.argmax(crnn_output, dim=2) # [sequence_length, batch_size]\n",
    "#     pred_indices = pred_indices.permute(1, 0) # -> [batch_size, sequence_length]\n",
    "\n",
    "#     batch_size, seq_len = pred_indices.shape\n",
    "#     # The input length for CTC decode is the full sequence length from CNN output\n",
    "#     input_lengths = torch.full(size=(batch_size,), fill_value=seq_len, dtype=torch.long)\n",
    "\n",
    "#     decoded_texts = converter.decode(pred_indices, input_lengths, raw=False)\n",
    "#     return decoded_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converter initialized. Num classes (incl blank): 75\n"
     ]
    }
   ],
   "source": [
    "HINDI_ALPHABET = \"-ँंःअआइईउऊएऐओऔकखगघङचछजझञटठडढणतथदधनपफबभमयरलवशषसह़ािीुूृेैॉोौ्ज़ड़ढ़।०१२३४५६७८९\"\n",
    "IMG_H = 64  # Must match training\n",
    "INPUT_NC = 1 # Grayscale\n",
    "HIDDEN_OCR = 256\n",
    "USE_RNN = True # Or False, must match trained model\n",
    "LEAKY_RELU = False\n",
    "\n",
    "try:\n",
    "    converter = strLabelConverter(HINDI_ALPHABET)\n",
    "    NUM_CLASSES = converter.num_classes\n",
    "    print(f\"Converter initialized. Num classes (incl blank): {NUM_CLASSES}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing converter: {e}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Options: {'imgH': 64, 'imgW': 704, 'input_nc': 1, 'hidden_size_OCR': 256, 'use_rnn': True, 'OCR_init': 'kaiming', 'skip_init': False, 'leakyRelu': False, 'dealwith_lossnone': True, 'batch_size': 2048, 'epochs': 50, 'lr': 0.0005, 'gpu_ids': [0], 'seed': 1234, 'max_label_length': 32, 'alphabet': '-ँंःअआइईउऊएऐओऔकखगघङचछजझञटठडढणतथदधनपफबभमयरलवशषसह़ािीुूृेैॉोौ्ज़ड़ढ़।०१२३४५६७८९', 'len_vocab': 75, 'checkpoint_dir': './checkpoints', 'experiment_name': 'hindi_ocr_crnn'}\n"
     ]
    }
   ],
   "source": [
    "def run_crnn_inference(model, converter, image_batch_tensor, device):\n",
    "    \"\"\"\n",
    "    Runs inference on a batch of images and returns decoded text.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The loaded CRNN model in eval mode on the correct device.\n",
    "        converter (OCRLabelConverter): The label converter instance.\n",
    "        image_batch_tensor (torch.Tensor): Batch of images, shape [B, C, H, W],\n",
    "                                            normalized appropriately for the model.\n",
    "        device (torch.device): The device to run inference on.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of decoded text strings for the batch.\n",
    "    \"\"\"\n",
    "    model.eval() # Ensure eval mode\n",
    "    with torch.no_grad():\n",
    "        images = image_batch_tensor.to(device)\n",
    "        crnn_output = model(images) # [W, B, NumClasses]\n",
    "        decoded_texts = decode_crnn_predictions(crnn_output.cpu(), converter) # Decoding happens on CPU\n",
    "    return decoded_texts\n",
    "class TrainOptions:\n",
    "    def __init__(self):\n",
    "        # Data params\n",
    "        self.imgH = 64\n",
    "        self.imgW = 704 # Padded width\n",
    "        self.input_nc = 1 # Grayscale images\n",
    "\n",
    "        # Model params\n",
    "        self.hidden_size_OCR = 256 # Number of hidden units in LSTM\n",
    "        self.use_rnn = True # Use LSTM layers\n",
    "        self.OCR_init = 'kaiming' # Weight initialization\n",
    "        self.skip_init = False # Perform weight initialization\n",
    "        self.leakyRelu = False # Use ReLU instead of LeakyReLU\n",
    "        self.dealwith_lossnone = True # Handle NaN/Inf gradients\n",
    "\n",
    "        # Training params\n",
    "        self.batch_size = 2048 # Adjust based on P100 memory (32 or 64 might work)\n",
    "        self.epochs = 50\n",
    "        self.lr = 0.0005\n",
    "        self.gpu_ids = [0] # Use the first GPU\n",
    "        self.seed = 1234\n",
    "        self.max_label_length = 32 # Your specified max label length\n",
    "\n",
    "        # Alphabet and Vocab\n",
    "        self.alphabet = HINDI_ALPHABET\n",
    "        self.len_vocab = len(self.alphabet) + 1 # Classes + blank\n",
    "\n",
    "        # Paths (Adapt for Kaggle)\n",
    "        self.checkpoint_dir = './checkpoints' # Where to save models\n",
    "        self.experiment_name = 'hindi_ocr_crnn'\n",
    "\n",
    "opt = TrainOptions()\n",
    "print(\"Training Options:\", vars(opt))\n",
    "\n",
    "def init_weights(net, init_type='normal', init_gain=0.02):\n",
    "    \"\"\"Initialize network weights.\n",
    "    Parameters:\n",
    "        net (network)   -- network to be initialized\n",
    "        init_type (str) -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n",
    "        init_gain (float)    -- scaling factor for normal, xavier and orthogonal.\n",
    "    We use 'normal' in the original pix2pix and CycleGAN paper. But xavier and kaiming might\n",
    "    work better for some applications. Feel free to try yourself.\n",
    "    \"\"\"\n",
    "    def init_func(m):  # define the initialization function\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "            if init_type == 'normal':\n",
    "                init.normal_(m.weight.data, 0.0, init_gain)\n",
    "            elif init_type == 'xavier':\n",
    "                init.xavier_normal_(m.weight.data, gain=init_gain)\n",
    "            elif init_type == 'kaiming':\n",
    "                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "            elif init_type == 'orthogonal':\n",
    "                init.orthogonal_(m.weight.data, gain=init_gain)\n",
    "            else:\n",
    "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                init.constant_(m.bias.data, 0.0)\n",
    "        elif classname.find('BatchNorm2d') != -1:  # BatchNorm Layer's weight is not a matrix; only normal distribution applies.\n",
    "            init.normal_(m.weight.data, 1.0, init_gain)\n",
    "            init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "    print('initialize network with %s' % init_type)\n",
    "    net.apply(init_func)  # apply the initialization function <init_func>\n",
    "    return net\n",
    "\n",
    "def to_device(net, gpu_ids):\n",
    "    if len(gpu_ids) > 0:\n",
    "        assert(torch.cuda.is_available())\n",
    "        net.to(gpu_ids[0])\n",
    "        # net = torch.nn.DataParallel(net, gpu_ids)  # multi-GPUs\n",
    "        if len(gpu_ids)>1:\n",
    "            net = torch.nn.DataParallel(net, device_ids=gpu_ids).cuda()\n",
    "            # net = torch.nn.DistributedDataParallel(net)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGPerceptualLoss(nn.Module):\n",
    "    def __init__(self, feature_layers=[2, 7, 16, 25, 34]): # Default VGG19 conv layers before ReLU/pooling\n",
    "        super().__init__()\n",
    "        self.vgg = models.vgg19(pretrained=True).features.eval()\n",
    "        for param in self.vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.feature_layers = feature_layers\n",
    "        self.loss_fn = nn.L1Loss()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        input_features = self._get_features(input)\n",
    "        target_features = self._get_features(target)\n",
    "        loss = 0.0\n",
    "        for i in self.feature_layers:\n",
    "            loss += self.loss_fn(input_features[i], target_features[i])\n",
    "        return loss / len(self.feature_layers)\n",
    "\n",
    "    def _get_features(self, x):\n",
    "        # Normalize for VGG\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(x.device)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(x.device)\n",
    "        if x.size(1) == 1: # Repeat grayscale channel if needed\n",
    "            x = x.repeat(1, 3, 1, 1)\n",
    "        x = (x - mean) / std\n",
    "\n",
    "        features = {}\n",
    "        for name, layer in self.vgg._modules.items():\n",
    "            x = layer(x)\n",
    "            if int(name) in self.feature_layers:\n",
    "                 features[int(name)] = x\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    # ... (Include the SelfAttention class definition from previous examples) ...\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.chanel_in = in_dim\n",
    "        sn = nn.utils.spectral_norm\n",
    "        self.query_conv = sn(nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1, bias=False))\n",
    "        self.key_conv = sn(nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1, bias=False))\n",
    "        self.value_conv = sn(nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1, bias=False))\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    def forward(self, x):\n",
    "        m_batchsize, C, height, width = x.size()\n",
    "        proj_query = self.query_conv(x).view(m_batchsize, -1, width*height).permute(0, 2, 1)\n",
    "        proj_key = self.key_conv(x).view(m_batchsize, -1, width*height)\n",
    "        energy = torch.bmm(proj_query, proj_key)\n",
    "        attention = self.softmax(energy / (self.chanel_in // 8)**0.5)\n",
    "        proj_value = self.value_conv(x).view(m_batchsize, -1, width*height)\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "        out = out.view(m_batchsize, C, height, width)\n",
    "        out = self.gamma * out + x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Image Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_content_bounds(patch_np_uint8, background_color, threshold, smooth_window, rel_thresh, min_thresh, imgH):\n",
    "    \"\"\"Finds the start and end column of significant content.\"\"\"\n",
    "    # Use brightness threshold if background isn't perfectly uniform or 0/255\n",
    "    content_mask_np = (patch_np_uint8 > threshold)\n",
    "    # Or use background color if reliable:\n",
    "    # content_mask_np = (patch_np_uint8 != background_color)\n",
    "\n",
    "    column_sums = np.sum(content_mask_np, axis=0).astype(float)\n",
    "\n",
    "    if smooth_window > 1:\n",
    "        kernel = np.ones(smooth_window) / smooth_window\n",
    "        smoothed_sums = np.convolve(column_sums, kernel, mode='same')\n",
    "    else:\n",
    "        smoothed_sums = column_sums\n",
    "\n",
    "    dynamic_threshold = imgH * rel_thresh\n",
    "    final_threshold = max(min_thresh, dynamic_threshold)\n",
    "\n",
    "    indices_above_threshold = np.where(smoothed_sums > final_threshold)[0]\n",
    "\n",
    "    first_col = -1\n",
    "    last_col = -1\n",
    "    if indices_above_threshold.size > 0:\n",
    "        first_col = indices_above_threshold[0]\n",
    "        last_col = indices_above_threshold[-1]\n",
    "        # Optional buffer for end to catch faint tails\n",
    "        last_col = min(last_col + 2, patch_np_uint8.shape[1] - 1)\n",
    "\n",
    "    return first_col, last_col\n",
    "\n",
    "\n",
    "# --- Modified Generation Function with Correct Stitching ---\n",
    "def generate_words_autoregressive_stitched_no_overlap(\n",
    "    words_list,\n",
    "    generator,\n",
    "    encoder,\n",
    "    # ft_wv,\n",
    "    noise_dim,\n",
    "    imgH,\n",
    "    imgW_patch,\n",
    "    context_width, # Width of context Generator expects as input\n",
    "    background_color,\n",
    "    device,\n",
    "    fixed_noise=None,\n",
    "    stretch_factor=1.0,\n",
    "    debug=False\n",
    "):\n",
    "    generator.eval()\n",
    "    output_word_image_tensors = []\n",
    "    output_words = []\n",
    "    normalized_background = float(background_color / 255.0)\n",
    "\n",
    "    num_words = len(words_list)\n",
    "\n",
    "    word_iterator = tqdm(words_list, desc=\"Generating Words\", disable=debug) if debug else words_list\n",
    "    for word_idx, word_to_generate in enumerate(word_iterator):\n",
    "        # 1. Tokenize & Embed\n",
    "        try:\n",
    "             char_unicodes, _ = getLetterTokens(word_to_generate); assert char_unicodes\n",
    "             with torch.no_grad(): embeddings_np = encoder(char_unicodes); embeddings = torch.from_numpy(embeddings_np).to(device).float(); assert embeddings.shape[0] == len(char_unicodes)\n",
    "        except Exception as e: print(f\"Error inputs '{word_to_generate}': {e}. Skip.\"); continue\n",
    "\n",
    "        # 2. Get Noise\n",
    "        noise_vector = fixed_noise[word_idx:word_idx+1].to(device) if fixed_noise is not None else torch.randn(1, noise_dim).to(device)\n",
    "\n",
    "        # 3. Autoregressive Loop\n",
    "        # --- Initialize Canvas ---\n",
    "        # Start with a small blank canvas (or just the first context)\n",
    "        final_canvas = torch.full((1, 1, imgH, context_width), normalized_background, dtype=torch.float32).to(device)\n",
    "        # -------------------------\n",
    "        current_context_patch = torch.full((1, 1, imgH, imgW_patch), normalized_background, dtype=torch.float32).to(device)\n",
    "\n",
    "        # --- Setup Debug Plotting ---\n",
    "        # ... (Keep debug plot setup if debug=True) ...\n",
    "        debug_fig, debug_axes = None, None\n",
    "        if debug:\n",
    "             num_chars = len(embeddings); fig_rows = num_chars * 2; fig_height_per_row = 1.5; fig_width = 6\n",
    "             debug_fig, debug_axes = plt.subplots(fig_rows, 1, figsize=(fig_width, fig_height_per_row * fig_rows))\n",
    "             if fig_rows <= 1 : debug_axes = np.array([debug_axes])\n",
    "             debug_fig.suptitle(f\"Debug Steps: '{word_to_generate}'\", fontsize=12); debug_plot_idx = 0\n",
    "\n",
    "        for i in range(len(embeddings)):\n",
    "            char_embedding = embeddings[i:i+1]\n",
    "            noise_step = noise_vector\n",
    "\n",
    "            with torch.no_grad():\n",
    "                generated_patch = generator(current_context_patch, char_embedding, noise_step) # [1,1,H,W_patch]\n",
    "\n",
    "            # --- Find Content Bounds in Generated Patch ---\n",
    "            generated_patch_np_uint8 = (generated_patch.squeeze().cpu().numpy() * 255.0).astype(np.uint8)\n",
    "            # We need the END column based on *all* content in the patch\n",
    "            _, last_content_col = find_content_bounds(\n",
    "                generated_patch_np_uint8, background_color, FOREGROUND_THRESHOLD,\n",
    "                SMOOTHING_WINDOW_SIZE, RELATIVE_DENSITY_THRESHOLD, MIN_DENSITY_THRESHOLD, imgH\n",
    "            )\n",
    "            # We also need the START column *after* the input context area\n",
    "            # Re-run find bounds on the *generated part only*\n",
    "            generated_part_np = generated_patch_np_uint8[:, context_width:]\n",
    "            first_content_col_in_gen_part, last_content_col_in_gen_part = find_content_bounds(\n",
    "                generated_part_np, background_color, FOREGROUND_THRESHOLD,\n",
    "                SMOOTHING_WINDOW_SIZE, RELATIVE_DENSITY_THRESHOLD, MIN_DENSITY_THRESHOLD, imgH\n",
    "            )\n",
    "            # Adjust first_col to be relative to the full 64x128 patch\n",
    "            if first_content_col_in_gen_part != -1:\n",
    "                first_content_col_overall = context_width + first_content_col_in_gen_part\n",
    "            else:\n",
    "                # If generated part is blank, effectively no new content starts\n",
    "                first_content_col_overall = -1\n",
    "\n",
    "\n",
    "            # --- Stitching: Append relevant part to canvas ---\n",
    "            if first_content_col_overall != -1 and last_content_col != -1 and last_content_col >= first_content_col_overall:\n",
    "                # Extract the generated content starting from first detected pixel after context\n",
    "                # up to the last detected pixel overall\n",
    "                content_to_append = generated_patch[:, :, :, first_content_col_overall : last_content_col + 1]\n",
    "                # Concatenate to the existing canvas\n",
    "                final_canvas = torch.cat((final_canvas, content_to_append.to(final_canvas.device)), dim=3)\n",
    "            # If generated part was blank, canvas remains unchanged\n",
    "\n",
    "            # --- Prepare Context for Next Step (Based on *current* generation's end) ---\n",
    "            # Use last_content_col found from the full generated patch\n",
    "            context_end_x_for_next = last_content_col + 1\n",
    "            context_start_x_for_next = max(0, context_end_x_for_next - context_width)\n",
    "            actual_context_width_for_next = context_end_x_for_next - context_start_x_for_next\n",
    "\n",
    "            next_context_patch = torch.full_like(current_context_patch, normalized_background).to(device)\n",
    "            context_data_for_display = None\n",
    "\n",
    "            if last_content_col >= 0 and actual_context_width_for_next > 0:\n",
    "                 # Extract context from the *ORIGINAL* generated patch tensor\n",
    "                 context_data = generated_patch[:, :, :, context_start_x_for_next:context_end_x_for_next]\n",
    "                 # Place it at the *start* of the next context patch\n",
    "                 next_context_patch[:, :, :, :actual_context_width_for_next] = context_data\n",
    "                 context_data_for_display = context_data.cpu() # For debug plot\n",
    "\n",
    "            current_context_patch = next_context_patch\n",
    "\n",
    "            # --- Debug Plotting ---\n",
    "            if debug and debug_axes is not None:\n",
    "                 # Plot Original Gen Patch\n",
    "                 ax = debug_axes[debug_plot_idx]; ax.imshow(generated_patch_np_uint8, cmap='gray', vmin=0, vmax=255)\n",
    "                 ax.set_title(f\"Step {i+1} Gen (Tok: {char_unicodes[i]})\", fontsize=9)\n",
    "                 if first_content_col_overall != -1: ax.axvline(x=first_content_col_overall - 0.5, color='lime', linestyle=':', lw=1, label=f'Start({first_content_col_overall})')\n",
    "                 if last_content_col != -1: ax.axvline(x=last_content_col + 0.5, color='r', linestyle='--', lw=1, label=f'End({last_content_col})')\n",
    "                 ax.axvline(x=context_width - 0.5, color='cyan', linestyle='-', lw=0.5, label='Ctx End')\n",
    "                 ax.legend(fontsize=6, loc='upper right'); ax.axis('off'); debug_plot_idx += 1\n",
    "                 # Plot Next Context\n",
    "                 ax = debug_axes[debug_plot_idx]; display_context_np = np.ones((imgH, imgW_patch), dtype=np.uint8) * background_color\n",
    "                 if context_data_for_display is not None: context_display_np = (context_data_for_display.squeeze().numpy()*255.).astype(np.uint8); display_context_np[:, :actual_context_width_for_next] = context_display_np; ax.axvline(x=actual_context_width_for_next - 0.5, color='g', linestyle='--', lw=1)\n",
    "                 ax.imshow(display_context_np, cmap='gray', vmin=0, vmax=255); ax.set_title(f\"Step {i+1} Next Ctx (W: {actual_context_width_for_next})\", fontsize=9); ax.axis('off'); debug_plot_idx += 1\n",
    "\n",
    "        # --- Show Debug Plot ---\n",
    "        if debug and debug_fig is not None: plt.tight_layout(rect=[0, 0.03, 1, 0.96]); plt.show(); plt.close(debug_fig)\n",
    "\n",
    "        # 4. Final Processing (Remove initial context, Stretch)\n",
    "        if final_canvas is None or final_canvas.shape[3] <= context_width:\n",
    "             if(debug): print(f\"W: Final canvas empty or too small for '{word_to_generate}'. Skip.\");\n",
    "             continue\n",
    "\n",
    "        # Remove the initial blank context added at the start\n",
    "        final_word_only_tensor = final_canvas[:, :, :, context_width:]\n",
    "\n",
    "        if stretch_factor < 1.0:\n",
    "            W_gen = final_word_only_tensor.shape[3]; W_final = max(W_gen, math.ceil(W_gen / stretch_factor))\n",
    "            try:\n",
    "                stretched_image_tensor = F.interpolate(final_word_only_tensor, size=(imgH, W_final), mode='bilinear', align_corners=False)\n",
    "                stretched_image_tensor = torch.clamp(stretched_image_tensor, 0.0, 1.0)\n",
    "            except Exception as e:\n",
    "                stretched_image_tensor = final_word_only_tensor\n",
    "                if(debug): print(f\"W: Stretching failed '{word_to_generate}': {e}.\");\n",
    "                else: pass\n",
    "        else: stretched_image_tensor = final_word_only_tensor\n",
    "\n",
    "        target_width = 704\n",
    "        current_stretched_width = stretched_image_tensor.shape[3]\n",
    "\n",
    "        if current_stretched_width < target_width:\n",
    "            # Pad on the right side\n",
    "            pad_width = target_width - current_stretched_width\n",
    "            # Pad with normalized background value\n",
    "            final_output_tensor = F.pad(stretched_image_tensor, (0, pad_width), mode='constant', value=normalized_background)\n",
    "            # print(f\"Padding '{word_to_generate}' from {current_stretched_width} to {target_width}\")\n",
    "        elif current_stretched_width > target_width:\n",
    "            # Crop from the right side\n",
    "            final_output_tensor = stretched_image_tensor[:, :, :, :target_width]\n",
    "            # print(f\"Cropping '{word_to_generate}' from {current_stretched_width} to {target_width}\")\n",
    "        else:\n",
    "            # Width is already correct\n",
    "            final_output_tensor = stretched_image_tensor\n",
    "        \n",
    "        output_word_image_tensors.append(final_output_tensor.cpu()) # Store final CPU tensor\n",
    "        output_words.append(word_to_generate)\n",
    "\n",
    "    # print(\"Finished generating words.\")\n",
    "    return output_word_image_tensors, output_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def find_content_bounds(patch_np_uint8, background_color, threshold, smooth_window, rel_thresh, min_thresh, imgH):\n",
    "#     content_mask_np = (patch_np_uint8 > threshold)\n",
    "#     column_sums = np.sum(content_mask_np, axis=0).astype(float)\n",
    "#     if smooth_window > 1:\n",
    "#         kernel = np.ones(smooth_window) / smooth_window\n",
    "#         smoothed_sums = np.convolve(column_sums, kernel, mode='same')\n",
    "#     else: smoothed_sums = column_sums\n",
    "#     dynamic_threshold = imgH * rel_thresh; final_threshold = max(min_thresh, dynamic_threshold)\n",
    "#     indices_above_threshold = np.where(smoothed_sums > final_threshold)[0]\n",
    "#     first_col, last_col = -1, -1\n",
    "#     if indices_above_threshold.size > 0:\n",
    "#         first_col = indices_above_threshold[0]\n",
    "#         last_col = indices_above_threshold[-1]\n",
    "#         last_col = min(last_col + 2, patch_np_uint8.shape[1] - 1)\n",
    "#     return first_col, last_col\n",
    "\n",
    "# # --- OPTIMIZED Generation Function ---\n",
    "# def generate_words_autoregressive_optimized(\n",
    "#     words_list,\n",
    "#     generator,\n",
    "#     encoder,\n",
    "#     # ft_wv,\n",
    "#     noise_dim,\n",
    "#     imgH,\n",
    "#     imgW_patch,\n",
    "#     context_width,\n",
    "#     background_color,\n",
    "#     device,\n",
    "#     fixed_noise=None,\n",
    "#     stretch_factor=1.0,\n",
    "#     target_width=704, # Added target width for final padding/cropping\n",
    "#     debug=False,\n",
    "#     show_progress=True\n",
    "# ):\n",
    "#     generator.eval()\n",
    "#     output_word_image_tensors = []\n",
    "#     output_words = [] # Keep track of successfully generated words\n",
    "#     normalized_background = float(background_color / 255.0)\n",
    "\n",
    "#     num_words = len(words_list)\n",
    "#     if fixed_noise is not None and fixed_noise.shape[0] != num_words:\n",
    "#         print(\"W: fixed_noise mismatch. Using random.\"); fixed_noise = None\n",
    "\n",
    "#     word_iterator = tqdm(words_list, desc=\"Generating Words\", disable=not show_progress) if show_progress else words_list\n",
    "\n",
    "#     for word_idx, word_to_generate in enumerate(word_iterator):\n",
    "#         # 1. Tokenize & Embed\n",
    "#         try:\n",
    "#              char_unicodes, _ = getLetterTokens(word_to_generate); assert char_unicodes\n",
    "#              with torch.no_grad(): embeddings_np = encoder(char_unicodes); embeddings = torch.from_numpy(embeddings_np).to(device).float(); assert embeddings.shape[0] == len(char_unicodes)\n",
    "#         except Exception as e: print(f\"E: Inputs '{word_to_generate}': {e}. Skip.\"); continue\n",
    "\n",
    "#         # 2. Get Noise\n",
    "#         noise_vector = fixed_noise[word_idx:word_idx+1].to(device) if fixed_noise is not None else torch.randn(1, noise_dim).to(device)\n",
    "\n",
    "#         # 3. Autoregressive Generation Loop - Store FULL patches\n",
    "#         generated_patches_list_cpu = [] # Store FULL patches on CPU\n",
    "#         current_context_patch = torch.full((1, 1, imgH, imgW_patch), normalized_background, dtype=torch.float32).to(device)\n",
    "\n",
    "#         for i in range(len(embeddings)):\n",
    "#             char_embedding = embeddings[i:i+1]\n",
    "#             noise_step = noise_vector # Use same noise\n",
    "\n",
    "#             with torch.no_grad(): # Inference context\n",
    "#                 generated_patch = generator(current_context_patch, char_embedding, noise_step) # [1,1,H,W_patch], GPU\n",
    "\n",
    "#             # --- Store Full Patch ---\n",
    "#             generated_patches_list_cpu.append(generated_patch.cpu())\n",
    "\n",
    "#             # --- Prepare Context for Next Step (Minimal CPU/Numpy) ---\n",
    "#             # Only calculate last_col needed for next context\n",
    "#             generated_patch_np_uint8 = (generated_patch.squeeze().cpu().numpy() * 255.0).astype(np.uint8)\n",
    "#             # Run only the necessary part of find_content_bounds logic\n",
    "#             content_mask_np = (generated_patch_np_uint8 > FOREGROUND_THRESHOLD)\n",
    "#             column_sums = np.sum(content_mask_np, axis=0).astype(float)\n",
    "#             if SMOOTHING_WINDOW_SIZE > 1:\n",
    "#                 kernel = np.ones(SMOOTHING_WINDOW_SIZE)/SMOOTHING_WINDOW_SIZE; smoothed_sums = np.convolve(column_sums, kernel, mode='same')\n",
    "#             else: smoothed_sums = column_sums\n",
    "#             dynamic_threshold = imgH * RELATIVE_DENSITY_THRESHOLD; final_threshold = max(MIN_DENSITY_THRESHOLD, dynamic_threshold)\n",
    "#             indices_above_threshold = np.where(smoothed_sums > final_threshold)[0]\n",
    "#             last_content_col = indices_above_threshold[-1] if indices_above_threshold.size > 0 else -1\n",
    "#             if last_content_col != -1: last_content_col = min(last_content_col + 2, imgW_patch - 1)\n",
    "\n",
    "#             # Calculate next context based on last_col\n",
    "#             context_end_x_for_next = last_content_col + 1\n",
    "#             context_start_x_for_next = max(0, context_end_x_for_next - context_width)\n",
    "#             actual_context_width_for_next = context_end_x_for_next - context_start_x_for_next\n",
    "\n",
    "#             next_context_patch = torch.full_like(current_context_patch, normalized_background).to(device)\n",
    "#             if last_content_col >= 0 and actual_context_width_for_next > 0:\n",
    "#                  context_data = generated_patch[:, :, :, context_start_x_for_next:context_end_x_for_next] # Slice GPU tensor\n",
    "#                  next_context_patch[:, :, :, :actual_context_width_for_next] = context_data # Place on GPU tensor\n",
    "#             current_context_patch = next_context_patch\n",
    "#             # --- End Context Prep ---\n",
    "\n",
    "#         # 4. Post-Generation Stitching (No Overlap)\n",
    "#         if not generated_patches_list_cpu: print(f\"W: No patches for '{word_to_generate}'. Skip.\"); continue\n",
    "\n",
    "#         stitched_slices = []\n",
    "#         for k, patch_tensor_cpu in enumerate(generated_patches_list_cpu):\n",
    "#              patch_np_uint8 = (patch_tensor_cpu.squeeze().numpy() * 255.0).astype(np.uint8)\n",
    "#              # Find bounds for THIS patch now\n",
    "#              first_col, last_col = find_content_bounds(\n",
    "#                  patch_np_uint8, background_color, FOREGROUND_THRESHOLD,\n",
    "#                  SMOOTHING_WINDOW_SIZE, RELATIVE_DENSITY_THRESHOLD, MIN_DENSITY_THRESHOLD, imgH\n",
    "#              )\n",
    "#              # Extract content slice from the STORED tensor\n",
    "#              if first_col != -1 and last_col != -1 and last_col >= first_col:\n",
    "#                   content_slice = patch_tensor_cpu[:, :, :, first_col : last_col + 1]\n",
    "#                   stitched_slices.append(content_slice)\n",
    "#              elif k==0: # If first patch is blank, maybe add small space?\n",
    "#                  stitched_slices.append(torch.full((1, 1, imgH, 1), normalized_background, dtype=torch.float32))\n",
    "\n",
    "\n",
    "#         if not stitched_slices: print(f\"W: No content slices found for '{word_to_generate}'. Skip.\"); continue\n",
    "\n",
    "#         # Concatenate the extracted content slices\n",
    "#         final_word_only_tensor = torch.cat(stitched_slices, dim=3) # CPU tensor\n",
    "\n",
    "#         # 5. Stretching\n",
    "#         stretched_image_tensor = final_word_only_tensor\n",
    "#         if stretch_factor < 1.0:\n",
    "#             W_gen = final_word_only_tensor.shape[3]; W_final = max(1, max(W_gen, math.ceil(W_gen / stretch_factor))) # Ensure width >= 1\n",
    "#             try:\n",
    "#                 stretched_image_tensor = F.interpolate(final_word_only_tensor, size=(imgH, W_final), mode='bilinear', align_corners=False)\n",
    "#                 stretched_image_tensor = torch.clamp(stretched_image_tensor, 0.0, 1.0)\n",
    "#             except Exception as e: print(f\"W: Stretching failed '{word_to_generate}': {e}.\")\n",
    "\n",
    "#         # 6. Padding/Cropping to Target Width\n",
    "#         current_stretched_width = stretched_image_tensor.shape[3]\n",
    "#         if current_stretched_width < target_width:\n",
    "#             pad_width = target_width - current_stretched_width\n",
    "#             final_output_tensor = F.pad(stretched_image_tensor, (0, pad_width), mode='constant', value=normalized_background)\n",
    "#         elif current_stretched_width > target_width:\n",
    "#             final_output_tensor = stretched_image_tensor[:, :, :, :target_width]\n",
    "#         else:\n",
    "#             final_output_tensor = stretched_image_tensor\n",
    "\n",
    "#         output_word_image_tensors.append(final_output_tensor) # Keep on CPU\n",
    "#         output_words.append(word_to_generate) # Track successful words\n",
    "\n",
    "#     if show_progress: print(\"Finished generating words.\")\n",
    "#     return output_word_image_tensors, output_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOREGROUND_THRESHOLD = 240     # Pixels brighter than this considered foreground (adjust if background is white)\n",
    "SMOOTHING_WINDOW_SIZE = 5      # Size of moving average window for column sums\n",
    "RELATIVE_DENSITY_THRESHOLD = 0.05 # Column 'content' if smoothed sum > 5% of max possible\n",
    "MIN_DENSITY_THRESHOLD =0        # Or require at least X pixels absolute minimum\n",
    "def generateWordImage(words, generator, encoder, device, debug=False):\n",
    "    noise_dim_gen = 128            # Must match training\n",
    "    imgH_gen = 64\n",
    "    imgW_patch_gen = 128          # Width of the generator's OUTPUT patch\n",
    "    context_width_gen = 26        # Approx 20% of 128, adjust if needed\n",
    "    background_color_gen = 0      # The pixel value for blank background [0..255]\n",
    "\n",
    "    num_words = len(words)\n",
    "    example_fixed_noise = torch.randn(num_words, noise_dim).to(device)\n",
    "\n",
    "    # print(\"\\n--- Generating with Debug & No Overlap Stitching ---\")\n",
    "    generated_stitched, output_words = generate_words_autoregressive_stitched_no_overlap( # Call new function\n",
    "        words_list=words, generator=generator, encoder=encoder,\n",
    "        noise_dim=noise_dim, imgH=imgH_gen, imgW_patch=imgW_patch_gen, context_width=context_width_gen,\n",
    "        background_color=background_color_gen, device=device, fixed_noise=example_fixed_noise,\n",
    "        stretch_factor=0.70, debug=debug\n",
    "    )\n",
    "    generated_stitched = [x for x in generated_stitched if x != [] or x != None]\n",
    "    if(generated_stitched == None or generated_stitched == []): return generated_stitched, output_words\n",
    "    else:\n",
    "        generated_stitched = torch.stack(generated_stitched, dim=0).squeeze(1).squeeze(1)\n",
    "        return generated_stitched, output_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPADE Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPADE(nn.Module):\n",
    "    # ... (Include the SPADE class definition from previous examples) ...\n",
    "    def __init__(self, norm_nc, label_nc, ks=3):\n",
    "        super().__init__()\n",
    "        self.param_free_norm = nn.InstanceNorm2d(norm_nc, affine=False)\n",
    "        pw = ks // 2\n",
    "        nhidden = 128\n",
    "        self.mlp_shared = nn.Sequential(\n",
    "            nn.Conv2d(label_nc, nhidden, kernel_size=ks, padding=pw),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.mlp_gamma = nn.Conv2d(nhidden, norm_nc, kernel_size=ks, padding=pw)\n",
    "        self.mlp_beta = nn.Conv2d(nhidden, norm_nc, kernel_size=ks, padding=pw)\n",
    "    def forward(self, x, cond_map):\n",
    "        normalized = self.param_free_norm(x)\n",
    "        if cond_map.size()[2:] != x.size()[2:]:\n",
    "            cond_map_interp = F.interpolate(cond_map, size=x.size()[2:], mode='nearest')\n",
    "        else:\n",
    "            cond_map_interp = cond_map\n",
    "        actv = self.mlp_shared(cond_map_interp)\n",
    "        gamma = self.mlp_gamma(actv)\n",
    "        beta = self.mlp_beta(actv)\n",
    "        out = normalized * (1 + gamma) + beta\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigGANResBlockSPADEUp(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, label_nc, upsample=True):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest') if upsample else None\n",
    "        fmiddle = min(in_channels, out_channels)\n",
    "\n",
    "        # Define layers\n",
    "        self.norm1 = SPADE(in_channels, label_nc)\n",
    "        self.conv1 = nn.Conv2d(in_channels, fmiddle, kernel_size=3, padding=1, bias=False)\n",
    "        self.norm2 = SPADE(fmiddle, label_nc)\n",
    "        self.conv2 = nn.Conv2d(fmiddle, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.activation = nn.ReLU(True)\n",
    "\n",
    "        # Shortcut connection\n",
    "        self.learn_shortcut = (in_channels != out_channels) or upsample\n",
    "        if self.learn_shortcut:\n",
    "            self.conv_s = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "            # No SPADE on shortcut path usually, but could be added\n",
    "\n",
    "    def forward(self, x, cond_map):\n",
    "        # Shortcut path\n",
    "        x_s = x\n",
    "        if self.upsample:\n",
    "            x_s = self.upsample(x_s)\n",
    "        if self.learn_shortcut:\n",
    "            x_s = self.conv_s(x_s)\n",
    "\n",
    "        # Main path\n",
    "        dx = self.norm1(x, cond_map)\n",
    "        dx = self.activation(dx)\n",
    "        if self.upsample:\n",
    "            dx = self.upsample(dx) # Upsample after first norm/act\n",
    "        dx = self.conv1(dx)\n",
    "        dx = self.norm2(dx, cond_map)\n",
    "        dx = self.activation(dx)\n",
    "        dx = self.conv2(dx)\n",
    "\n",
    "        out = x_s + dx\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetStyleGenerator(nn.Module): # Renamed for clarity\n",
    "    def __init__(self, fasttext_dim=300, noise_dim=100, input_channels=1, output_channels=1,\n",
    "                 ngf=96, label_nc=128, add_attention=True):\n",
    "        super().__init__()\n",
    "        self.fasttext_dim = fasttext_dim\n",
    "        self.noise_dim = noise_dim\n",
    "        self.combined_cond_dim = fasttext_dim + noise_dim\n",
    "        self.ngf = ngf\n",
    "        self.label_nc = label_nc # Channel dim for SPADE condition map\n",
    "        self.add_attention = add_attention\n",
    "\n",
    "        # Conditioning projection\n",
    "        self.init_h, self.init_w = 4, 8 # Smallest feature map size (matches enc4 output)\n",
    "        self.fc_cond = nn.Linear(self.combined_cond_dim, self.label_nc * self.init_h * self.init_w)\n",
    "\n",
    "        # --- Encoder ---\n",
    "        # Input: (B, 1, 64, 128)\n",
    "        self.enc1 = nn.Conv2d(input_channels, ngf, kernel_size=4, stride=2, padding=1) # Out: (B, ngf, 32, 64)\n",
    "        self.enc2 = nn.Sequential(nn.LeakyReLU(0.2, True), nn.Conv2d(ngf    , ngf * 2, 4, 2, 1, bias=False), nn.InstanceNorm2d(ngf * 2)) # Out: (B, ngf*2, 16, 32)\n",
    "        self.enc3 = nn.Sequential(nn.LeakyReLU(0.2, True), nn.Conv2d(ngf * 2, ngf * 4, 4, 2, 1, bias=False), nn.InstanceNorm2d(ngf * 4)) # Out: (B, ngf*4, 8, 16)\n",
    "        self.enc4 = nn.Sequential(nn.LeakyReLU(0.2, True), nn.Conv2d(ngf * 4, ngf * 8, 4, 2, 1, bias=False), nn.InstanceNorm2d(ngf * 8)) # Out: (B, ngf*8, 4, 8)\n",
    "\n",
    "        # --- Bottleneck --- (Processes features at the smallest spatial dim)\n",
    "        self.bottleneck_conv = nn.Sequential(\n",
    "             nn.LeakyReLU(0.2, True),\n",
    "             nn.Conv2d(ngf * 8, ngf * 8, kernel_size=3, padding=1, bias=False), # Process at 4x8\n",
    "             nn.InstanceNorm2d(ngf * 8),\n",
    "             nn.ReLU(True) # Added ReLU often helps here\n",
    "        )\n",
    "\n",
    "        # --- Decoder using BigGAN-Style ResBlocks with Skip Connections ---\n",
    "        # Decoder input channels = channels from previous layer + channels from corresponding encoder layer\n",
    "\n",
    "        # Block 4: 4x8 -> 8x16\n",
    "        # Input: bottleneck output (ngf*8). No skip connection here. Output: ngf*4\n",
    "        self.dec4 = BigGANResBlockSPADEUp(in_channels=ngf * 8,         out_channels=ngf * 4, label_nc=self.label_nc, upsample=True)\n",
    "        # Optional Attention after Block 4 (operates on ngf*4 channels)\n",
    "        if self.add_attention: self.attn4 = SelfAttention(ngf * 4)\n",
    "\n",
    "        # Block 3: 8x16 -> 16x32\n",
    "        # Input: dec4 output (ngf*4) + enc3 output (ngf*4) = ngf*8. Output: ngf*2\n",
    "        self.dec3 = BigGANResBlockSPADEUp(in_channels=ngf * 8,         out_channels=ngf * 2, label_nc=self.label_nc, upsample=True)\n",
    "        # Optional Attention after Block 3 (operates on ngf*2 channels)\n",
    "        if self.add_attention: self.attn3 = SelfAttention(ngf * 2)\n",
    "\n",
    "        # Block 2: 16x32 -> 32x64\n",
    "        # Input: dec3 output (ngf*2) + enc2 output (ngf*2) = ngf*4. Output: ngf\n",
    "        self.dec2 = BigGANResBlockSPADEUp(in_channels=ngf * 4,         out_channels=ngf,     label_nc=self.label_nc, upsample=True)\n",
    "        # Optional Attention after Block 2 (operates on ngf channels)\n",
    "        # if self.add_attention: self.attn2 = SelfAttention(ngf) # Can add if needed\n",
    "\n",
    "        # Block 1: 32x64 -> 64x128\n",
    "        # Input: dec2 output (ngf) + enc1 output (ngf) = ngf*2. Output: ngf\n",
    "        self.dec1 = BigGANResBlockSPADEUp(in_channels=ngf * 2,         out_channels=ngf,     label_nc=self.label_nc, upsample=True)\n",
    "\n",
    "        # --- Final Output Layers ---\n",
    "        self.final_norm = nn.InstanceNorm2d(ngf, affine=True) # Final norm before output conv\n",
    "        self.final_activ = nn.ReLU(True)\n",
    "        self.final_conv = nn.Conv2d(ngf, output_channels, kernel_size=3, padding=1) # Keep kernel 3x3\n",
    "        self.final_output_act = nn.Sigmoid() # Use Sigmoid for [0, 1] output\n",
    "\n",
    "    def forward(self, input_patch, fasttext_vector, noise):\n",
    "        # --- Condition Processing ---\n",
    "        cond_combined = torch.cat([fasttext_vector, noise], dim=1)\n",
    "        cond_projected = self.fc_cond(cond_combined)\n",
    "        # Reshape condition to smallest spatial size for SPADE blocks\n",
    "        cond_map_base = cond_projected.view(-1, self.label_nc, self.init_h, self.init_w)\n",
    "\n",
    "        # --- Encoder Path & Save Skip Connections ---\n",
    "        e1 = self.enc1(input_patch) # (B, ngf,   32, 64)\n",
    "        e2 = self.enc2(e1)          # (B, ngf*2, 16, 32)\n",
    "        e3 = self.enc3(e2)          # (B, ngf*4, 8, 16)\n",
    "        e4 = self.enc4(e3)          # (B, ngf*8, 4, 8)\n",
    "\n",
    "        # --- Bottleneck ---\n",
    "        bottleneck = self.bottleneck_conv(e4) # (B, ngf*8, 4, 8)\n",
    "\n",
    "        # --- Decoder Path with Skip Connections ---\n",
    "        # Dec4: No skip connection from encoder needed here\n",
    "        d4_out = self.dec4(bottleneck, cond_map_base) # (B, ngf*4, 8, 16)\n",
    "        if self.add_attention and hasattr(self, 'attn4'):\n",
    "            d4_out = self.attn4(d4_out)\n",
    "\n",
    "        # Dec3: Concatenate d4_out with e3\n",
    "        d3_in = torch.cat([d4_out, e3], dim=1) # (B, ngf*4 + ngf*4 = ngf*8, 8, 16)\n",
    "        d3_out = self.dec3(d3_in, cond_map_base) # (B, ngf*2, 16, 32)\n",
    "        if self.add_attention and hasattr(self, 'attn3'):\n",
    "            d3_out = self.attn3(d3_out)\n",
    "\n",
    "        # Dec2: Concatenate d3_out with e2\n",
    "        d2_in = torch.cat([d3_out, e2], dim=1) # (B, ngf*2 + ngf*2 = ngf*4, 16, 32)\n",
    "        d2_out = self.dec2(d2_in, cond_map_base) # (B, ngf, 32, 64)\n",
    "        # if self.add_attention and hasattr(self, 'attn2'):\n",
    "        #     d2_out = self.attn2(d2_out)\n",
    "\n",
    "        # Dec1: Concatenate d2_out with e1\n",
    "        d1_in = torch.cat([d2_out, e1], dim=1) # (B, ngf + ngf = ngf*2, 32, 64)\n",
    "        d1_out = self.dec1(d1_in, cond_map_base) # (B, ngf, 64, 128)\n",
    "\n",
    "        # --- Final Layers ---\n",
    "        x = self.final_norm(d1_out)\n",
    "        x = self.final_activ(x)\n",
    "        x = self.final_conv(x)\n",
    "        output_patch = self.final_output_act(x) # (B, 1, 64, 128)\n",
    "\n",
    "        return output_patch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PatchGAN Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# class SelfAttention(nn.Module):\n",
    "#     def __init__(self, in_dim):\n",
    "#         super().__init__()\n",
    "#         self.chanel_in = in_dim\n",
    "#         sn = nn.utils.spectral_norm # Use spectral norm here too\n",
    "\n",
    "#         self.query_conv = sn(nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1))\n",
    "#         self.key_conv = sn(nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1))\n",
    "#         self.value_conv = sn(nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1))\n",
    "#         self.gamma = nn.Parameter(torch.zeros(1))\n",
    "#         self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         m_batchsize, C, height, width = x.size()\n",
    "#         proj_query = self.query_conv(x).view(m_batchsize, -1, width*height).permute(0, 2, 1)\n",
    "#         proj_key = self.key_conv(x).view(m_batchsize, -1, width*height)\n",
    "#         energy = torch.bmm(proj_query, proj_key)\n",
    "#         attention = self.softmax(energy)\n",
    "#         proj_value = self.value_conv(x).view(m_batchsize, -1, width*height)\n",
    "#         out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "#         out = out.view(m_batchsize, C, height, width)\n",
    "#         out = self.gamma * out + x\n",
    "#         return out\n",
    "\n",
    "\n",
    "# # --- Enhanced Conditional Discriminator ---\n",
    "# class ConditionalDiscriminator(nn.Module):\n",
    "#     def __init__(self, fasttext_dim=300, input_channels=1, target_channels=1, ndf=64, n_layers=4, add_attention=True):\n",
    "#         super().__init__()\n",
    "#         self.fasttext_dim = fasttext_dim\n",
    "#         self.ndf = ndf\n",
    "#         self.n_layers = n_layers\n",
    "#         self.add_attention = add_attention\n",
    "#         sn = nn.utils.spectral_norm # Spectral norm function\n",
    "\n",
    "#         # Conditioning projection (remains the same)\n",
    "#         self.cond_channels = ndf // 2\n",
    "#         self.cond_projection_map_size = (16, 32)\n",
    "#         self.cond_projector = nn.Sequential(\n",
    "#             # Apply SN to Linear layers as well? Optional but can help.\n",
    "#             sn(nn.Linear(fasttext_dim, self.ndf * 4)),\n",
    "#             nn.ReLU(True),\n",
    "#             sn(nn.Linear(self.ndf * 4, self.cond_channels * self.cond_projection_map_size[0] * self.cond_projection_map_size[1]))\n",
    "#         )\n",
    "\n",
    "#         kw = 4\n",
    "#         padw = 1\n",
    "#         input_c = input_channels + target_channels\n",
    "\n",
    "#         # Use ModuleList to build layers incrementally\n",
    "#         self.blocks = nn.ModuleList()\n",
    "\n",
    "#         # Initial Block (No Norm, apply SN to Conv)\n",
    "#         self.blocks.append(nn.Sequential(\n",
    "#             sn(nn.Conv2d(input_c, ndf, kernel_size=kw, stride=2, padding=padw)), # SN applied\n",
    "#             nn.LeakyReLU(0.2, True)\n",
    "#         )) # Out: ndf x 32x64\n",
    "\n",
    "#         # Intermediate Blocks\n",
    "#         nf_mult = 1\n",
    "#         nf_mult_prev = 1\n",
    "#         attention_inserted = False\n",
    "#         for n in range(1, n_layers):\n",
    "#             nf_mult_prev = nf_mult\n",
    "#             nf_mult = min(2**n, 8)\n",
    "#             in_ch = ndf * nf_mult_prev\n",
    "#             out_ch = ndf * nf_mult\n",
    "\n",
    "#             # Decide where to inject condition (e.g., before block n=2)\n",
    "#             if n == 2:\n",
    "#                 in_ch += self.cond_channels\n",
    "\n",
    "#             block = nn.Sequential(\n",
    "#                 sn(nn.Conv2d(in_ch, out_ch, kernel_size=kw, stride=2, padding=padw, bias=False)), # SN applied\n",
    "#                 nn.InstanceNorm2d(out_ch, affine=True), # Keep InstanceNorm for now\n",
    "#                 nn.LeakyReLU(0.2, True)\n",
    "#             )\n",
    "#             self.blocks.append(block)\n",
    "\n",
    "#             # Add attention after a specific block, e.g., after the block outputting 8x16 (n=2)\n",
    "#             if self.add_attention and n == 2 and not attention_inserted:\n",
    "#                 print(f\"Adding Attention Layer in Discriminator after block {n+1} (output channels: {out_ch})\")\n",
    "#                 self.attn = SelfAttention(in_dim=out_ch)\n",
    "#                 self.blocks.append(self.attn) # Add attention module to sequence\n",
    "#                 attention_inserted = True\n",
    "\n",
    "\n",
    "#         # Final Blocks (stride 1)\n",
    "#         nf_mult_prev = nf_mult\n",
    "#         nf_mult = min(2**n_layers, 8)\n",
    "#         self.blocks.append(nn.Sequential(\n",
    "#              sn(nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=False)), # SN applied\n",
    "#              nn.InstanceNorm2d(ndf * nf_mult, affine=True),\n",
    "#              nn.LeakyReLU(0.2, True)\n",
    "#         ))\n",
    "#         # Output Layer (apply SN)\n",
    "#         self.blocks.append(nn.Sequential(\n",
    "#             sn(nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)) # SN applied\n",
    "#         ))\n",
    "\n",
    "#     def forward(self, input_patch, target_or_fake_patch, fasttext_vector):\n",
    "#         combined_img = torch.cat([input_patch, target_or_fake_patch], dim=1)\n",
    "\n",
    "#         projected_cond = self.cond_projector(fasttext_vector)\n",
    "#         cond_map = projected_cond.view(-1, self.cond_channels, *self.cond_projection_map_size)\n",
    "\n",
    "#         x = combined_img\n",
    "#         conv_block_idx = 0 # Track completed CONV blocks (Conv + Act) for condition injection\n",
    "#         injected = False\n",
    "\n",
    "#         for i, block in enumerate(self.blocks):\n",
    "#              # Logic to find injection point - before block index 2 (which is the 3rd conv block overall)\n",
    "#              # Check *before* applying the block\n",
    "#             if not injected and conv_block_idx == 2 and isinstance(block[0], nn.Conv2d) :\n",
    "#                  if x.shape[2:] != cond_map.shape[2:]:\n",
    "#                       cond_map_resized = F.interpolate(cond_map, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
    "#                  else:\n",
    "#                       cond_map_resized = cond_map\n",
    "#                  x = torch.cat([x, cond_map_resized], dim=1) # Concatenate before the conv\n",
    "#                  injected = True\n",
    "\n",
    "#             x = block(x) # Apply the block (Conv+Act or Attn or Norm+Act)\n",
    "\n",
    "#             # Increment block index logic needs care if attention is inserted\n",
    "#             # Let's count based on Conv2d layers specifically\n",
    "#             if isinstance(block, nn.Sequential) and isinstance(block[0], nn.Conv2d):\n",
    "#                  # Check if it's one of the main downsampling/processing conv layers\n",
    "#                  if block[0].stride[0] > 1 or block[0].kernel_size[0] > 1: # Avoid counting final 1x1 if added later\n",
    "#                       conv_block_idx +=1\n",
    "\n",
    "\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# # Make sure SelfAttention is defined if using add_attention=True\n",
    "\n",
    "# # --- Self Attention Layer (SAGAN style - include this definition) ---\n",
    "# class SelfAttention(nn.Module):\n",
    "#     def __init__(self, in_dim):\n",
    "#         super().__init__()\n",
    "#         self.chanel_in = in_dim\n",
    "#         # Use spectral norm for stability\n",
    "#         sn = nn.utils.spectral_norm\n",
    "\n",
    "#         # Define query, key, value convolutions with SN\n",
    "#         # Using bias=False is common practice with SN, especially if followed by Norm\n",
    "#         self.query_conv = sn(nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1, bias=False))\n",
    "#         self.key_conv = sn(nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1, bias=False))\n",
    "#         self.value_conv = sn(nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1, bias=False))\n",
    "#         self.gamma = nn.Parameter(torch.zeros(1)) # Learnable scale parameter\n",
    "#         self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         m_batchsize, C, height, width = x.size()\n",
    "#         proj_query = self.query_conv(x).view(m_batchsize, -1, width*height).permute(0, 2, 1)\n",
    "#         proj_key = self.key_conv(x).view(m_batchsize, -1, width*height)\n",
    "#         energy = torch.bmm(proj_query, proj_key)\n",
    "#         # Optional scaling for stability, common in Transformers\n",
    "#         attention = self.softmax(energy / (self.chanel_in // 8)**0.5)\n",
    "#         proj_value = self.value_conv(x).view(m_batchsize, -1, width*height)\n",
    "#         out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "#         out = out.view(m_batchsize, C, height, width)\n",
    "#         out = self.gamma * out + x # Residual connection\n",
    "#         return out\n",
    "\n",
    "\n",
    "# # --- Final Discriminator: ProjectionDiscriminator with SN/Attention ---\n",
    "# class ProjectionDiscriminator(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Single-Scale PatchGAN Discriminator using Spectral Normalization,\n",
    "#     Projection Conditioning, and optional Self-Attention.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, fasttext_dim=300, input_channels=1, target_channels=1, ndf=64, n_layers=4, add_attention=True, embed_dim=128):\n",
    "#         super().__init__()\n",
    "#         self.fasttext_dim = fasttext_dim\n",
    "#         self.ndf = ndf\n",
    "#         self.n_layers = n_layers\n",
    "#         self.add_attention = add_attention\n",
    "#         self.embed_dim = embed_dim # Dim for projected class & image features\n",
    "#         sn = nn.utils.spectral_norm\n",
    "\n",
    "#         kw = 4 # Kernel size\n",
    "#         padw = 1 # Padding\n",
    "#         input_c = input_channels + target_channels # Combined input/target channels\n",
    "\n",
    "#         # --- Backbone Feature Extractor (using ModuleList) ---\n",
    "#         self.backbone = nn.ModuleList()\n",
    "#         # Initial Block: Conv + LeakyReLU (SN applied)\n",
    "#         self.backbone.append(nn.Sequential(\n",
    "#             sn(nn.Conv2d(input_c, ndf, kernel_size=kw, stride=2, padding=padw)),\n",
    "#             nn.LeakyReLU(0.2, True)\n",
    "#         )) # Out spatial: H/2, W/2 (e.g., 32x64)\n",
    "\n",
    "#         # Intermediate Blocks: Conv + Norm + LeakyReLU (SN applied to Conv)\n",
    "#         nf_mult = 1\n",
    "#         attention_inserted = False\n",
    "#         current_out_ch = ndf\n",
    "#         for n in range(1, n_layers):\n",
    "#             nf_mult_prev = nf_mult\n",
    "#             nf_mult = min(2**n, 8)\n",
    "#             in_ch = ndf * nf_mult_prev\n",
    "#             out_ch = ndf * nf_mult\n",
    "#             current_out_ch = out_ch\n",
    "\n",
    "#             block = nn.Sequential(\n",
    "#                 sn(nn.Conv2d(in_ch, out_ch, kernel_size=kw, stride=2, padding=padw, bias=False)),\n",
    "#                 # Note: InstanceNorm might slightly conflict with SN theory but often kept.\n",
    "#                 # Could be replaced by LayerNorm/GroupNorm or removed if causing issues.\n",
    "#                 nn.InstanceNorm2d(out_ch, affine=True),\n",
    "#                 nn.LeakyReLU(0.2, True)\n",
    "#             )\n",
    "#             self.backbone.append(block)\n",
    "\n",
    "#             # Add attention layer after the block where n=2 (output is ndf*4 channels)\n",
    "#             if self.add_attention and n == 2 and not attention_inserted:\n",
    "#                 print(f\"Adding Attention Layer in Discriminator (Input channels: {out_ch})\")\n",
    "#                 self.attn = SelfAttention(in_dim=out_ch)\n",
    "#                 self.backbone.append(self.attn)\n",
    "#                 attention_inserted = True\n",
    "\n",
    "#         # Penultimate Block (Stride 1 Conv + Norm + LeakyReLU)\n",
    "#         nf_mult_prev = nf_mult\n",
    "#         nf_mult = min(2**n_layers, 8)\n",
    "#         in_ch_penultimate = current_out_ch\n",
    "#         out_ch_penultimate = ndf * nf_mult\n",
    "#         current_out_ch = out_ch_penultimate\n",
    "\n",
    "#         self.backbone.append(nn.Sequential(\n",
    "#              sn(nn.Conv2d(in_ch_penultimate, out_ch_penultimate, kernel_size=kw, stride=1, padding=padw, bias=False)),\n",
    "#              nn.InstanceNorm2d(out_ch_penultimate, affine=True),\n",
    "#              nn.LeakyReLU(0.2, True)\n",
    "#         ))\n",
    "#         self.feature_out_channels = current_out_ch # Store final feature map channels\n",
    "\n",
    "#         # --- Projection Components (Defined in __init__) ---\n",
    "#         # 1. Scaler Convolution (Outputs base real/fake score map)\n",
    "#         self.final_conv_scaler = sn(nn.Conv2d(self.feature_out_channels, 1, kernel_size=kw, stride=1, padding=padw))\n",
    "\n",
    "#         # 2. Class Embedding Projection (Processes FastText vector)\n",
    "#         self.embedding_layer = sn(nn.Linear(fasttext_dim, self.embed_dim, bias=False))\n",
    "\n",
    "#         # 3. Image Feature Projection (Projects pooled features to match embed_dim)\n",
    "#         self.gap_proj = sn(nn.Linear(self.feature_out_channels, self.embed_dim, bias=False))\n",
    "\n",
    "#     def forward(self, input_patch, target_or_fake_patch, fasttext_vector):\n",
    "#         # Concatenate input/target images\n",
    "#         combined_img = torch.cat([input_patch, target_or_fake_patch], dim=1)\n",
    "\n",
    "#         # Pass through backbone (extracts features)\n",
    "#         features = combined_img\n",
    "#         for block in self.backbone:\n",
    "#             features = block(features)\n",
    "#         # `features` shape: (B, feature_out_channels, H', W')\n",
    "\n",
    "#         # --- Scaler Output ---\n",
    "#         # Base real/fake prediction map from image features only\n",
    "#         scaler_output = self.final_conv_scaler(features)\n",
    "#         # Shape: (B, 1, H', W')\n",
    "\n",
    "#         # --- Projection Term ---\n",
    "#         # 1. Get class embedding\n",
    "#         class_embedding = self.embedding_layer(fasttext_vector) # (B, embed_dim)\n",
    "\n",
    "#         # 2. Project globally pooled image features\n",
    "#         gap = F.adaptive_avg_pool2d(features, 1).view(features.size(0), -1) # (B, feature_out_channels)\n",
    "#         projected_features = self.gap_proj(gap) # (B, embed_dim)\n",
    "\n",
    "#         # 3. Calculate inner product (alignment score)\n",
    "#         inner_product_scalar = torch.sum(projected_features * class_embedding, dim=1, keepdim=True) # (B, 1)\n",
    "\n",
    "#         # 4. Expand spatially to match scaler_output map\n",
    "#         inner_product_term = inner_product_scalar.view(-1, 1, 1, 1).expand_as(scaler_output)\n",
    "\n",
    "#         # --- Final Output ---\n",
    "#         # Combine base realism score with class alignment score\n",
    "#         output = scaler_output + inner_product_term\n",
    "#         # Shape: (B, 1, H', W') - Output logits, use Hinge or BCEWithLogitsLoss\n",
    "\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# Make sure SelfAttention is defined\n",
    "\n",
    "# --- Self Attention Layer (SAGAN style - same as before) ---\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.chanel_in = in_dim\n",
    "        sn = nn.utils.spectral_norm\n",
    "        self.query_conv = sn(nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1, bias=False))\n",
    "        self.key_conv = sn(nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1, bias=False))\n",
    "        self.value_conv = sn(nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1, bias=False))\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        m_batchsize, C, height, width = x.size()\n",
    "        proj_query = self.query_conv(x).view(m_batchsize, -1, width*height).permute(0, 2, 1)\n",
    "        proj_key = self.key_conv(x).view(m_batchsize, -1, width*height)\n",
    "        energy = torch.bmm(proj_query, proj_key)\n",
    "        attention = self.softmax(energy / (self.chanel_in // 8)**0.5)\n",
    "        proj_value = self.value_conv(x).view(m_batchsize, -1, width*height)\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "        out = out.view(m_batchsize, C, height, width)\n",
    "        out = self.gamma * out + x\n",
    "        return out\n",
    "\n",
    "\n",
    "# --- Projection Discriminator with SN and MORE Attention ---\n",
    "class ProjectionDiscriminator(nn.Module): # Renamed class\n",
    "    \"\"\"\n",
    "    Single-Scale PatchGAN Discriminator using Spectral Normalization,\n",
    "    Projection Conditioning, and MULTIPLE optional Self-Attention layers.\n",
    "    \"\"\"\n",
    "    # Add flags to control where attention is added\n",
    "    def __init__(self, fasttext_dim=300, input_channels=1, target_channels=1,\n",
    "                 ndf=64, n_layers=4, embed_dim=128,\n",
    "                 attn_after_layer_1=True, # Add attn after 32x64 features\n",
    "                 attn_after_layer_2=True): # Add attn after 8x16 features\n",
    "        super().__init__()\n",
    "        self.fasttext_dim = fasttext_dim\n",
    "        self.ndf = ndf\n",
    "        self.n_layers = n_layers\n",
    "        self.embed_dim = embed_dim\n",
    "        self.attn_after_layer_1 = attn_after_layer_1\n",
    "        self.attn_after_layer_2 = attn_after_layer_2\n",
    "        sn = nn.utils.spectral_norm\n",
    "\n",
    "        kw = 4\n",
    "        padw = 1\n",
    "        input_c = input_channels + target_channels\n",
    "\n",
    "        # --- Backbone Feature Extractor (using ModuleList) ---\n",
    "        self.backbone = nn.ModuleList()\n",
    "        # Initial Block\n",
    "        self.backbone.append(nn.Sequential(\n",
    "            sn(nn.Conv2d(input_c, ndf, kernel_size=kw, stride=2, padding=padw)),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        )) # Out: ndf x 32x64\n",
    "\n",
    "        # Intermediate Blocks\n",
    "        nf_mult = 1\n",
    "        current_out_ch = ndf\n",
    "        for n in range(1, n_layers):\n",
    "            nf_mult_prev = nf_mult\n",
    "            nf_mult = min(2**n, 8)\n",
    "            in_ch = ndf * nf_mult_prev\n",
    "            out_ch = ndf * nf_mult\n",
    "            current_out_ch = out_ch\n",
    "\n",
    "            block = nn.Sequential(\n",
    "                sn(nn.Conv2d(in_ch, out_ch, kernel_size=kw, stride=2, padding=padw, bias=False)),\n",
    "                nn.InstanceNorm2d(out_ch, affine=True),\n",
    "                nn.LeakyReLU(0.2, True)\n",
    "            )\n",
    "            self.backbone.append(block)\n",
    "\n",
    "            # Add attention layer after the block where n=1 (output is ndf*2 channels, 16x32 spatial)\n",
    "            if self.attn_after_layer_1 and n == 1:\n",
    "                attn_ch = out_ch # Channels going into attention block\n",
    "                print(f\"Adding Attention Layer 1 in Discriminator (Input channels: {attn_ch})\")\n",
    "                # Use unique attribute names for multiple attention layers\n",
    "                self.attn1 = SelfAttention(in_dim=attn_ch)\n",
    "                self.backbone.append(self.attn1)\n",
    "                # Note: output channels remain attn_ch\n",
    "\n",
    "            # Add attention layer after the block where n=2 (output is ndf*4 channels, 8x16 spatial)\n",
    "            if self.attn_after_layer_2 and n == 2:\n",
    "                attn_ch = out_ch # Channels going into attention block\n",
    "                print(f\"Adding Attention Layer 2 in Discriminator (Input channels: {attn_ch})\")\n",
    "                self.attn2 = SelfAttention(in_dim=attn_ch)\n",
    "                self.backbone.append(self.attn2)\n",
    "                # Note: output channels remain attn_ch\n",
    "\n",
    "\n",
    "        # Penultimate Block (Stride 1 Conv + Norm + LeakyReLU)\n",
    "        nf_mult_prev = nf_mult\n",
    "        nf_mult = min(2**n_layers, 8)\n",
    "        # Input channels depend on whether attention was added after the previous block\n",
    "        in_ch_penultimate = current_out_ch # Output channels from last loop iteration\n",
    "        out_ch_penultimate = ndf * nf_mult\n",
    "        current_out_ch = out_ch_penultimate\n",
    "\n",
    "        self.backbone.append(nn.Sequential(\n",
    "             sn(nn.Conv2d(in_ch_penultimate, out_ch_penultimate, kernel_size=kw, stride=1, padding=padw, bias=False)),\n",
    "             nn.InstanceNorm2d(out_ch_penultimate, affine=True),\n",
    "             nn.LeakyReLU(0.2, True)\n",
    "        ))\n",
    "        self.feature_out_channels = current_out_ch # Store final feature map channels\n",
    "\n",
    "        # --- Projection Components (Defined in __init__) ---\n",
    "        self.final_conv_scaler = sn(nn.Conv2d(self.feature_out_channels, 1, kernel_size=kw, stride=1, padding=padw))\n",
    "        self.embedding_layer = sn(nn.Linear(fasttext_dim, self.embed_dim, bias=False))\n",
    "        self.gap_proj = sn(nn.Linear(self.feature_out_channels, self.embed_dim, bias=False))\n",
    "\n",
    "    def forward(self, input_patch, target_or_fake_patch, fasttext_vector):\n",
    "        combined_img = torch.cat([input_patch, target_or_fake_patch], dim=1)\n",
    "\n",
    "        # Pass through backbone (extracts features)\n",
    "        # Attention layers are now part of self.backbone and applied automatically\n",
    "        features = combined_img\n",
    "        for block in self.backbone:\n",
    "            features = block(features)\n",
    "        # `features` shape: (B, feature_out_channels, H', W')\n",
    "\n",
    "        # --- Scaler Output ---\n",
    "        scaler_output = self.final_conv_scaler(features)\n",
    "        # Shape: (B, 1, H', W')\n",
    "\n",
    "        # --- Projection Term ---\n",
    "        class_embedding = self.embedding_layer(fasttext_vector)\n",
    "        gap = F.adaptive_avg_pool2d(features, 1).view(features.size(0), -1)\n",
    "        projected_features = self.gap_proj(gap)\n",
    "        inner_product_scalar = torch.sum(projected_features * class_embedding, dim=1, keepdim=True)\n",
    "        inner_product_term = inner_product_scalar.view(-1, 1, 1, 1).expand_as(scaler_output)\n",
    "\n",
    "        # --- Final Output ---\n",
    "        output = scaler_output + inner_product_term\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters & Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --- Hyperparameter Tuning Intuition ---\n",
    "- **lr_g, lr_d:** (0.0001 - 0.0004) Lower if unstable/exploding loss, higher if slow convergence. D often slightly lower than G if unbalanced.\n",
    "- **lambda_l1:** (50 - 200) Higher values force more precise pixel matching (good for structure), lower allows more GAN \"creativity\". Start high for text.\n",
    "- **lambda_perceptual:** (1 - 20) Higher values focus on feature similarity (less blurry), lower rely more on L1/Adv.\n",
    "- **noise_dim:** (64 - 128) Less critical, influences output variation. 100 is standard.\n",
    "- **ngf, ndf:** (32 - 128) Increase for more model capacity if underfitting (and memory allows), decrease if overfitting or memory issues.\n",
    "- **batch_size:** Maximize based on GPU memory. Larger batches often stabilize training.\n",
    "- **n_layers_d:** (3 - 5) Controls PatchGAN receptive field. 3-4 is typical.\n",
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training ---\n",
    "num_epochs = 20000\n",
    "lr_g = 0.0002 * 0.5\n",
    "lr_d = 0.0002 * 0.25 * 0.5 * 0.8\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "# lambda_l1 = 3.0 * 0         # Weight for L1 reconstruction loss\n",
    "# lambda_perceptual = 1.0 * 0.0   # Weight for VGG perceptual loss\n",
    "\n",
    "use_gradient_balancing = True\n",
    "lambda_ocr = 0.8 # Base weight for OCR loss (used with alpha)\n",
    "grad_balance_alpha = 1.0 # Alpha for gradient balancing\n",
    "\n",
    "# --- Model ---\n",
    "fasttext_dim = 300\n",
    "noise_dim = 128            # Dimension of the noise vector\n",
    "ngf = 256                   # Base number of generator filters\n",
    "ndf = 96                   # Base number of discriminator filters\n",
    "label_nc_g = 256            # Number of channels for SPADE conditioning map\n",
    "n_layers_d = 4             # Number of conv layers in Discriminator patch output\n",
    "embed_dim_d = 256\n",
    "add_attention_g = True # Use attention in G\n",
    "add_attention_d = True # Use attention in D\n",
    "\n",
    "# --- Data ---\n",
    "batch_size = 128            # Adjust based on GPU memory (try 16, 32, 64)\n",
    "npy_folder = \"LetterClassPairsCompressed\" # Folder with .npy files\n",
    "\n",
    "# --- Environment ---\n",
    "model_save_interval = 1         # Save models every N epochs\n",
    "sample_save_interval = 1         # Save samples every N epochs\n",
    "num_samples_to_save = 4\n",
    "output_dir = \"./training_Output/\" # Directory for saving models/samples\n",
    "sample_output_dir = \"./samples\" # Subdirectory for samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using 4 GPUs!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Initialize Models ---\n",
    "encoder = FastTextEncoder().to(device)\n",
    "generator = UNetStyleGenerator( # Use the new generator\n",
    "    fasttext_dim=fasttext_dim, noise_dim=noise_dim, ngf=ngf, label_nc=label_nc_g, add_attention=add_attention_g\n",
    ").to(device)\n",
    "discriminator = ProjectionDiscriminator( # Keep ProjectionD (non-AC)\n",
    "    fasttext_dim=fasttext_dim, ndf=ndf, n_layers=n_layers_d, embed_dim=embed_dim_d, attn_after_layer_1=False, attn_after_layer_2=False\n",
    ").to(device)\n",
    "\n",
    "# --- Multi-GPU Setup ---\n",
    "if torch.cuda.device_count() > 1:\n",
    "  print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "  generator = nn.DataParallel(generator)\n",
    "  discriminator = nn.DataParallel(discriminator)\n",
    "\n",
    "# --- Optimizers ---\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=lr_g, betas=(beta1, beta2))\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=lr_d, betas=(beta1, beta2))\n",
    "\n",
    "# --- Loss Functions ---\n",
    "# adversarial_loss_fn = nn.BCEWithLogitsLoss()\n",
    "# l1_loss_fn = nn.L1Loss()\n",
    "# Perceptual loss needs VGG model definition (assumed available)\n",
    "# perceptual_loss_fn = VGGPerceptualLoss().to(device)\n",
    "OCR_criterion = nn.CTCLoss(blank=converter.dict['-'], reduction='none', zero_infinity=True) # Use blank index from converter\n",
    "\n",
    "# --- Output Directory ---\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(sample_output_dir, exist_ok=True) # Create sample directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize network with kaiming\n",
      "CRNN model structure created.\n",
      "Model weights loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT_PATH = '../SavedModels/CRNN/CRNN_V1.pth' # !!! CHANGE THIS !!!\n",
    "recognizer = CRNN(opt, leakyRelu=opt.leakyRelu)\n",
    "print(\"CRNN model structure created.\")\n",
    "\n",
    "if not os.path.exists(CHECKPOINT_PATH):\n",
    "    print(f\"Error: Checkpoint not found at {CHECKPOINT_PATH}\");\n",
    "try:\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH, map_location='cpu') # Load to CPU first\n",
    "    state_dict = checkpoint.get('model_state_dict', checkpoint) # Handle different save formats\n",
    "\n",
    "    # Handle potential 'module.' prefix from DataParallel saving\n",
    "    if list(state_dict.keys())[0].startswith('module.'):\n",
    "         new_state_dict = OrderedDict()\n",
    "         for k, v in state_dict.items():\n",
    "             name = k[7:] # remove `module.`\n",
    "             new_state_dict[name] = v\n",
    "         state_dict = new_state_dict\n",
    "         print(\"Removed 'module.' prefix from state_dict keys.\")\n",
    "\n",
    "    recognizer.load_state_dict(state_dict)\n",
    "    recognizer.to(device)\n",
    "    recognizer.eval()\n",
    "    print(\"Model weights loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading weights: {e}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(155384835, 15923425)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in generator.parameters() if p.requires_grad), sum(p.numel() for p in discriminator.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11496523"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in recognizer.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # ---------------------------\n",
    "# # Training Loop\n",
    "# # ---------------------------\n",
    "# print(\"Starting Training...\")\n",
    "# fixed_noise_for_samples = torch.randn(batch_size, noise_dim).to(device) # Use fixed noise for consistent sample generation across epochs\n",
    "# fixed_sample_batch = None # Will store one batch for sample generation\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     generator.train()\n",
    "#     discriminator.train()\n",
    "#     total_g_loss_epoch = 0.0\n",
    "#     total_d_loss_epoch = 0.0\n",
    "\n",
    "#     # --- Use tqdm for batch progress within each epoch ---\n",
    "#     pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=True)\n",
    "\n",
    "#     for i, batch in enumerate(pbar):\n",
    "#         input_patch, target_patch, class_name_batch = batch # Dataloader yields these\n",
    "#         current_batch_size = input_patch.size(0)\n",
    "\n",
    "#         # --- Store a fixed batch for sample generation ---\n",
    "#         if i == 0 and fixed_sample_batch is None:\n",
    "#             fixed_sample_batch = (target_patch.clone(), class_name_batch) # Store input and class names\n",
    "\n",
    "#         # --- Prepare FastText Embeddings ---\n",
    "#         with torch.no_grad(): # No need to track gradients for the pre-trained encoder\n",
    "#             fasttext_vector_batch = torch.from_numpy(\n",
    "#                 encoder(class_name_batch) # Call your encoder module\n",
    "#             ).to(device).float()\n",
    "\n",
    "\n",
    "#         # Move data to device\n",
    "#         input_patch = input_patch.to(device)\n",
    "#         target_patch = target_patch.to(device)\n",
    "#         # fasttext_vector_batch is already moved to device above\n",
    "\n",
    "#         # --- Train Discriminator ---\n",
    "#         d_optimizer.zero_grad()\n",
    "\n",
    "#         # Real samples\n",
    "#         # Discriminator output shape depends on n_layers and input size (it's a patch output)\n",
    "#         # We need labels matching the output patch shape\n",
    "#         d_output_real = discriminator(input_patch, target_patch, fasttext_vector_batch)\n",
    "#         # Use smoothed labels or just ones/zeros\n",
    "#         # real_labels = torch.ones_like(d_output_real).to(device) # Target is 1 for real\n",
    "#         # real_labels = torch.full_like(d_output_real, 0.9).to(device) # Target is 0.9 for real\n",
    "#         # d_loss_real = adversarial_loss_fn(d_output_real, real_labels)\n",
    "#         d_loss_real = torch.mean(F.relu(1.0 - d_output_real))\n",
    "\n",
    "#         # Fake samples\n",
    "#         noise = torch.randn(current_batch_size, noise_dim).to(device)\n",
    "#         fake_images = generator(input_patch, fasttext_vector_batch, noise)\n",
    "#         # fake_labels = torch.zeros_like(d_output_real).to(device) # Target is 0 for fake\n",
    "#         # fake_labels = torch.full_like(d_output_real, 0.1).to(device) # Target is 0.1 for fake\n",
    "#         # Detach fake_images to avoid gradients flowing back to generator during D update\n",
    "#         d_output_fake = discriminator(input_patch, fake_images.detach(), fasttext_vector_batch)\n",
    "#         # d_loss_fake = adversarial_loss_fn(d_output_fake, fake_labels)\n",
    "#         d_loss_fake = torch.mean(F.relu(1.0 + d_output_fake))\n",
    "\n",
    "#         # Combined Discriminator Loss\n",
    "#         d_loss = (d_loss_real + d_loss_fake) * 0.5\n",
    "#         d_loss.backward()\n",
    "#         d_optimizer.step()\n",
    "\n",
    "#         # --- Train Generator ---\n",
    "#         g_optimizer.zero_grad()\n",
    "\n",
    "#         # Adversarial Loss (Generator wants Discriminator to output 1 for fake images)\n",
    "#         # Re-run discriminator on *non-detached* fake images\n",
    "#         d_output_g = discriminator(input_patch, fake_images, fasttext_vector_batch)\n",
    "#         # Generator wants these classified as real (label=1)\n",
    "#         # g_loss_adv = adversarial_loss_fn(d_output_g, real_labels) # Use real_labels (target=1)\n",
    "#         g_loss_adv = -torch.mean(d_output_g)\n",
    "\n",
    "#         # Perceptual Loss\n",
    "#         # g_loss_perceptual = perceptual_loss_fn(fake_images, target_patch)\n",
    "\n",
    "#         # Combined Generator Loss\n",
    "#         g_loss = g_loss_adv\n",
    "#         g_loss.backward()\n",
    "#         g_optimizer.step()\n",
    "\n",
    "#         # --- Logging & Progress Bar Update ---\n",
    "#         total_g_loss_epoch += g_loss.item()\n",
    "#         total_d_loss_epoch += d_loss.item()\n",
    "#         g_loss_adv_item = g_loss_adv.item()\n",
    "#         # g_loss_perceptual_item = g_loss_perceptual.item() * lambda_perceptual # Show weighted loss\n",
    "#         pbar.set_postfix({\n",
    "#             \"G_Loss\": f\"{g_loss.item():.4f}\",\n",
    "#             \"D_Loss\": f\"{d_loss.item():.4f}\",\n",
    "#             \"G_Adv\": f\"{g_loss_adv_item:.4f}\",      # Log individual component\n",
    "#         })\n",
    "\n",
    "#         # --- Save Models Periodically ---\n",
    "#         if ((epoch + 1) % model_save_interval == 0 or epoch == num_epochs - 1) and i == 0:\n",
    "#             print(f\"Saving models for epoch {epoch+1}...\")\n",
    "#             # Handle DataParallel state dict saving correctly\n",
    "#             if isinstance(generator, nn.DataParallel):\n",
    "#                 torch.save(generator.module.state_dict(), os.path.join(output_dir, f\"generator_epoch_{epoch+1}.pth\"))\n",
    "#                 torch.save(discriminator.module.state_dict(), os.path.join(output_dir, f\"discriminator_epoch_{epoch+1}.pth\"))\n",
    "#             else:\n",
    "#                 torch.save(generator.state_dict(), os.path.join(output_dir, f\"generator_epoch_{epoch+1}.pth\"))\n",
    "#                 torch.save(discriminator.state_dict(), os.path.join(output_dir, f\"discriminator_epoch_{epoch+1}.pth\"))\n",
    "#             print(\"Models saved.\")\n",
    "            \n",
    "#         # --- Generate and Display Samples Periodically ---\n",
    "#         if (i + 1) % 90 == 0 and fixed_sample_batch is not None:\n",
    "#             generator.eval()\n",
    "#             with torch.no_grad():\n",
    "#                 sample_input_patch, sample_class_names = fixed_sample_batch\n",
    "#                 sample_input_patch_cpu = sample_input_patch.clone().cpu() # Keep a CPU copy for display later\n",
    "#                 sample_input_patch = sample_input_patch.to(device) # Move input to device for generation\n",
    "#                 current_sample_batch_size = sample_input_patch.size(0)\n",
    "    \n",
    "#                 # Prepare FastText\n",
    "#                 with torch.no_grad():\n",
    "#                     sample_fasttext_batch = torch.from_numpy(encoder(sample_class_names)).to(device).float()\n",
    "    \n",
    "#                 noise_for_samples = fixed_noise_for_samples[:current_sample_batch_size]\n",
    "#                 generated_samples = generator(sample_input_patch, sample_fasttext_batch, noise_for_samples).cpu() # Move generated samples to CPU\n",
    "    \n",
    "#                 timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#                 num_to_save = min(num_samples_to_save, current_sample_batch_size)\n",
    "#                 indices_to_save = random.sample(range(current_sample_batch_size), num_to_save)\n",
    "    \n",
    "#                 print(f\"\\n--- Generating Samples Epoch {epoch+1} ---\")\n",
    "#                 # Create figure with 2 columns per sample (Input, Output)\n",
    "#                 plt.figure(figsize=(6 * num_to_save, 4)) # Adjust width (e.g., 6 inches per pair)\n",
    "    \n",
    "#                 for i, idx in enumerate(indices_to_save):\n",
    "#                     inp_img_tensor = sample_input_patch_cpu[idx] # Use CPU version for display (1, H, W)\n",
    "#                     gen_img_tensor = generated_samples[idx]     # Use generated output from CPU (1, H, W)\n",
    "    \n",
    "#                     # Denormalize tensors to numpy arrays for display/saving\n",
    "#                     inp_img_np = (inp_img_tensor.squeeze(0).numpy() * 255.0).astype(np.uint8)\n",
    "#                     gen_img_np = (gen_img_tensor.squeeze(0).numpy() * 255.0).astype(np.uint8)\n",
    "    \n",
    "#                     class_name = sample_class_names[idx]\n",
    "#                     safe_class_name = \"\".join(c for c in class_name if c.isalnum() or c in ('_'))\n",
    "    \n",
    "#                     # --- Save the COMBINED image (optional, kept from previous logic) ---\n",
    "#                     combined_img_np = np.concatenate((inp_img_np, gen_img_np), axis=1)\n",
    "#                     filename = f\"{safe_class_name}_epoch{epoch+1}_{timestamp}_sample{idx}_combined.png\"\n",
    "#                     filepath = os.path.join(sample_output_dir, filename)\n",
    "#                     try:\n",
    "#                         plt.imsave(filepath, combined_img_np, cmap='gray')\n",
    "#                     except Exception as e:\n",
    "#                         print(f\"W: Save failed {filepath}. E: {e}\")\n",
    "#                     # --- End Save Combined ---\n",
    "    \n",
    "#                     # --- Display Input Patch ---\n",
    "#                     plt.subplot(1, num_to_save * 2, (2 * i) + 1) # Position for Input\n",
    "#                     plt.imshow(inp_img_np, cmap='gray')\n",
    "#                     plt.title(f\"Input (Class: {safe_class_name})\\nEpoch {epoch+1}\", fontsize=8)\n",
    "#                     plt.axis('off')\n",
    "    \n",
    "#                     # --- Display Generated Output Patch ---\n",
    "#                     plt.subplot(1, num_to_save * 2, (2 * i) + 2) # Position for Output\n",
    "#                     plt.imshow(gen_img_np, cmap='gray')\n",
    "#                     plt.title(f\"Generated Output\\n(Sample Index {idx})\", fontsize=8)\n",
    "#                     plt.axis('off')\n",
    "#                     # --------------------------\n",
    "    \n",
    "#                 plt.tight_layout(pad=0.5) # Add a little padding\n",
    "#                 plt.show() # Show the figure with all subplots\n",
    "#                 plt.close() # Close the figure window\n",
    "    \n",
    "#             generator.train()\n",
    "#             print(f\"Displayed {num_to_save} input/output pairs. Combined images saved to {sample_output_dir}\")\n",
    "#     # --- End of Epoch ---\n",
    "#     avg_g_loss = total_g_loss_epoch / len(dataloader)\n",
    "#     avg_d_loss = total_d_loss_epoch / len(dataloader)\n",
    "#     print(f\"Epoch {epoch+1} Complete. Avg G_Loss: {avg_g_loss:.4f}, Avg D_Loss: {avg_d_loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Training Finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toggle_grad(model, on_or_off):\n",
    "  for param in model.parameters():\n",
    "    param.requires_grad = on_or_off\n",
    "\n",
    "def display_word_samples(generated_tensors, word_labels, num_to_display, epoch, sample_output_dir, batch_idx=None):\n",
    "\n",
    "    actual_num_to_display = min(num_to_display, len(generated_tensors))\n",
    "    if actual_num_to_display == 0:\n",
    "        return\n",
    "\n",
    "    # Setup 2xN subplot grid (Max 2 rows) - Adjust as needed\n",
    "    ncols = min(actual_num_to_display, 2)\n",
    "    if actual_num_to_display == 1: ncols = 1\n",
    "    elif 2 < actual_num_to_display <= 4: ncols = 2\n",
    "    else: ncols = 2 # Default for > 4 samples\n",
    "\n",
    "    nrows = math.ceil(actual_num_to_display / ncols)\n",
    "    fig_words, axes_words = plt.subplots(nrows, ncols, figsize=((704 / 33) * ncols, (64/33) * nrows), squeeze=False)\n",
    "    axes_flat = axes_words.flatten()\n",
    "\n",
    "    batch_info_title = f\", B {batch_idx+1}\" if batch_idx is not None else \"\"\n",
    "    batch_info_fname = f\"_B{batch_idx+1}\" if batch_idx is not None else \"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    # Create a filename based on the displayed words\n",
    "    displayed_words_str = \"_\".join(\n",
    "        [\"\".join(c for c in word_labels[k] if c.isalnum() or c in ('_'))\n",
    "         for k in range(actual_num_to_display)]\n",
    "    )\n",
    "    max_len = 90\n",
    "    safe_filename_part = (displayed_words_str[:max_len] + '..') if len(displayed_words_str) > max_len else displayed_words_str\n",
    "    fig_filename = f\"samples_E{epoch+1}{batch_info_fname}_{safe_filename_part}_{timestamp}.png\"\n",
    "    fig_filepath = os.path.join(sample_output_dir, fig_filename)\n",
    "\n",
    "    fig_words.suptitle(f\"Generated Samples Epoch {epoch+1}{batch_info_title}\", fontsize=12, y=1.02)\n",
    "\n",
    "    for display_idx in range(actual_num_to_display):\n",
    "        img_tensor = generated_tensors[display_idx]\n",
    "        word_label = word_labels[display_idx] if display_idx < len(word_labels) else f\"Sample {display_idx}\"\n",
    "        img_np = (img_tensor.squeeze().cpu().numpy() * 255.0).astype(np.uint8)\n",
    "\n",
    "        ax = axes_flat[display_idx]\n",
    "        ax.imshow(img_np, cmap='gray', aspect='auto')\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Hide unused axes\n",
    "    for ax_idx in range(actual_num_to_display, len(axes_flat)):\n",
    "         axes_flat[ax_idx].axis('off')\n",
    "\n",
    "    plt.tight_layout(pad=0.5)\n",
    "    try:\n",
    "        plt.savefig(fig_filepath, bbox_inches='tight') # Save the figure\n",
    "        print(f\"Saved sample grid to: {fig_filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving sample figure: {e}\")\n",
    "    plt.show()\n",
    "    plt.close(fig_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---------------------------\n",
    "# # Training Loop\n",
    "# # ---------------------------\n",
    "# print(\"Starting Training...\")\n",
    "# fixed_noise_for_samples = torch.randn(batch_size, noise_dim).to(device) # Use fixed noise for consistent sample generation across epochs\n",
    "# fixed_sample_batch = None # Will store one batch for sample generation\n",
    "\n",
    "# word_batch_for_samples = next(iter(word_dataloader)) # Get one batch of words\n",
    "# fixed_words_to_generate = word_batch_for_samples # Take first N\n",
    "# fixed_noise_for_patch_samples = torch.randn(batch_size, noise_dim).to(device)\n",
    "# fixed_patch_sample_batch = None\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     generator.train()\n",
    "#     discriminator.train()\n",
    "#     # recognizer stays in eval mode\n",
    "#     total_g_loss_epoch, total_d_loss_epoch, total_ocr_fake_epoch = 0.0, 0.0, 0.0\n",
    "#     g_loss, d_loss = torch.tensor(0.0), torch.tensor(0.0)\n",
    "#     g_loss_adv, loss_OCR_fake = torch.tensor(0.0), torch.tensor(0.0)\n",
    "#     bal_a_item = 0.0\n",
    "#     log_dict = {}\n",
    "\n",
    "#     pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=True)\n",
    "\n",
    "#     for i, batch in enumerate(pbar):\n",
    "#         input_patch, target_patch, class_name_batch = batch\n",
    "#         current_batch_size = input_patch.size(0)\n",
    "\n",
    "#         # Store fixed batch for patch sampling\n",
    "#         if i == 0 and fixed_patch_sample_batch is None:\n",
    "#              num_fixed = min(current_batch_size, fixed_noise_for_samples.size(0))\n",
    "#              fixed_patch_sample_batch = (input_patch[:num_fixed].clone().cpu(), class_name_batch[:num_fixed])\n",
    "\n",
    "#         # Prepare FastText & Move data\n",
    "#         with torch.no_grad(): # No need to track gradients for the pre-trained encoder\n",
    "#             fasttext_vector_batch = torch.from_numpy(\n",
    "#                 encoder(class_name_batch) # Call your encoder module\n",
    "#             ).to(device).float()\n",
    "\n",
    "#         # --- Train Discriminator ---\n",
    "#         # Recognizer is frozen, no need to toggle its grad or step its optimizer\n",
    "#         toggle_grad(generator, False); toggle_grad(discriminator, True)\n",
    "#         d_optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "#         # Real Pass\n",
    "#         d_output_real = discriminator(input_patch, target_patch, fasttext_vector_batch)\n",
    "#         d_loss_real = torch.mean(F.relu(1.0 - d_output_real)) # Hinge Loss\n",
    "\n",
    "#         # Fake Pass\n",
    "#         noise = torch.randn(current_batch_size, noise_dim).to(device)\n",
    "#         with torch.no_grad():\n",
    "#              fake_images = generator(input_patch, fasttext_vector_batch, noise).detach()\n",
    "#         d_output_fake = discriminator(input_patch, fake_images, fasttext_vector_batch)\n",
    "#         d_loss_fake = torch.mean(F.relu(1.0 + d_output_fake)) # Hinge Loss\n",
    "\n",
    "#         d_loss = (d_loss_real + d_loss_fake) * 0.5 # No OCR loss component for D here\n",
    "#         d_loss.backward()\n",
    "#         d_optimizer.step()\n",
    "#         total_d_loss_epoch += d_loss.item()\n",
    "\n",
    "#         # --- Train Generator ---\n",
    "#         toggle_grad(generator, True); toggle_grad(discriminator, False); toggle_grad(recognizer, False)\n",
    "#         g_optimizer.zero_grad(set_to_none=True) # Zero grads before ANY G computation\n",
    "\n",
    "#         # Generate fake images with grad enabled for G\n",
    "#         noise_g = torch.randn(current_batch_size, noise_dim).to(device)\n",
    "#         fake_images_g = generator(input_patch, fasttext_vector_batch, noise_g)\n",
    "#         if not fake_images_g.requires_grad: fake_images_g.requires_grad_(True)\n",
    "\n",
    "#         # --- Calculate Individual Losses ---\n",
    "#         # Adversarial Loss\n",
    "#         d_output_g = discriminator(input_patch, fake_images_g, fasttext_vector_batch)\n",
    "#         loss_G_adv = -torch.mean(d_output_g)\n",
    "\n",
    "#         # OCR Loss on Fakes\n",
    "#         loss_OCR_fake = torch.tensor(0.0).to(device) # Initialize\n",
    "#         with torch.no_grad(): # Forward pass for OCR loss VALUE doesn't need grads tracked here\n",
    "#             pred_fake_OCR = recognizer(fake_images_g)\n",
    "#         log_probs_fake = F.log_softmax(pred_fake_OCR, dim=2)\n",
    "#         preds_size_fake = torch.IntTensor([pred_fake_OCR.size(0)] * current_batch_size).to(device)\n",
    "#         text_encode_fake, len_text_fake = converter.encode(class_name_batch)\n",
    "#         text_encode_fake = text_encode_fake.to(device); len_text_fake = len_text_fake.to(device)\n",
    "#         loss_OCR_fake_batch = OCR_criterion(log_probs_fake, text_encode_fake.detach(), preds_size_fake, len_text_fake.detach())\n",
    "#         valid_losses_fake = loss_OCR_fake_batch[~torch.isnan(loss_OCR_fake_batch) & ~torch.isinf(loss_OCR_fake_batch)]\n",
    "#         if valid_losses_fake.numel() > 0:\n",
    "#             loss_OCR_fake = torch.mean(valid_losses_fake)\n",
    "#         total_ocr_fake_epoch += loss_OCR_fake.item() # Track raw loss\n",
    "\n",
    "#         # --- Gradient Balancing (if enabled) ---\n",
    "#         bal_a_item = lambda_ocr # Default if not balancing\n",
    "\n",
    "#         if use_gradient_balancing and loss_OCR_fake > 0 and loss_G_adv.requires_grad:\n",
    "#             # 1. Calculate grads for Adv + Recon losses\n",
    "#             g_optimizer.zero_grad() # Zero grads specifically for this backward pass\n",
    "#             loss_adv_recon = loss_G_adv\n",
    "#             # Ensure the graph from generator to fake_images_g is available\n",
    "#             loss_adv_recon.backward(retain_graph=True) # Keep graph for OCR grad calc\n",
    "#             grads_adv_recon = [p.grad.data.clone() if p.grad is not None else None for p in generator.parameters()]\n",
    "#             std_adv_recon = torch.std(torch.cat([g.view(-1) for g in grads_adv_recon if g is not None])).item() if grads_adv_recon else 1e-6\n",
    "#             std_adv_recon = np.nan_to_num(std_adv_recon, nan=1e-6)\n",
    "\n",
    "#             # 2. Calculate grads for OCR loss (needs temp train mode for R)\n",
    "#             g_optimizer.zero_grad() # Zero grads again\n",
    "#             recognizer.train(); toggle_grad(recognizer, True) # Temp train mode for backward\n",
    "#             # Re-run OCR forward pass within this grad scope\n",
    "#             pred_fake_OCR_grad = recognizer(fake_images_g)\n",
    "#             log_probs_fake_grad = F.log_softmax(pred_fake_OCR_grad, dim=2)\n",
    "#             preds_size_fake_grad = torch.IntTensor([pred_fake_OCR_grad.size(0)] * current_batch_size).to(device)\n",
    "#             loss_OCR_fake_for_grad_batch = OCR_criterion(log_probs_fake_grad, text_encode_fake.detach(), preds_size_fake_grad, len_text_fake.detach())\n",
    "#             valid_losses_fake_grad = loss_OCR_fake_for_grad_batch[~torch.isnan(loss_OCR_fake_for_grad_batch) & ~torch.isinf(loss_OCR_fake_for_grad_batch)]\n",
    "#             mean_loss_ocr_fake_grad = torch.mean(valid_losses_fake_grad) if valid_losses_fake_grad.numel() > 0 else torch.tensor(0.0).to(device)\n",
    "\n",
    "#             if mean_loss_ocr_fake_grad > 0:\n",
    "#                 mean_loss_ocr_fake_grad.backward(retain_graph=False) # Calculate OCR grads (no need to retain graph after this)\n",
    "#                 grads_ocr = [p.grad.data.clone() if p.grad is not None else None for p in generator.parameters()]\n",
    "#                 std_ocr = torch.std(torch.cat([g.view(-1) for g in grads_ocr if g is not None])).item() if grads_ocr else 1e-6\n",
    "#                 std_ocr = np.nan_to_num(std_ocr, nan=1e-6)\n",
    "\n",
    "#                 # Reset recognizer mode/grad state\n",
    "#                 recognizer.eval(); toggle_grad(recognizer, False)\n",
    "\n",
    "#                 # 3. Combine gradients manually\n",
    "#                 if std_ocr > 1e-7:\n",
    "#                     a = grad_balance_alpha * (std_adv_recon / std_ocr)\n",
    "#                     bal_a_item = a\n",
    "#                     g_optimizer.zero_grad() # Zero grads before applying combined\n",
    "#                     for p_idx, p in enumerate(generator.parameters()):\n",
    "#                          if grads_adv_recon[p_idx] is not None and grads_ocr[p_idx] is not None:\n",
    "#                              p.grad = grads_adv_recon[p_idx] + a * grads_ocr[p_idx] # Apply combined grad\n",
    "#                          elif grads_adv_recon[p_idx] is not None:\n",
    "#                               p.grad = grads_adv_recon[p_idx] # Only Adv+Recon grad exists\n",
    "#                          # If only OCR grad exists (unlikely), it's ignored here unless explicitly handled\n",
    "\n",
    "#                     # Log the *conceptual* combined loss AFTER balancing (for reporting)\n",
    "#                     g_loss = loss_adv_recon + (a * loss_OCR_fake)\n",
    "\n",
    "#                 else: # Fallback if std_ocr is zero\n",
    "#                      print(\"W: OCR gradient std dev near zero, using non-balanced sum.\")\n",
    "#                      g_loss = loss_adv_recon + (lambda_ocr * loss_OCR_fake) # Fallback sum\n",
    "#                      bal_a_item = lambda_ocr\n",
    "#                      # Need grads from loss_adv_recon only\n",
    "#                      g_optimizer.zero_grad()\n",
    "#                      loss_adv_recon.backward() # Calculate grads for fallback sum\n",
    "\n",
    "#             else: # Fallback if OCR loss was zero/invalid for gradient calculation\n",
    "#                  print(\"W: OCR loss zero/invalid for grad calc. Using Adv+Recon loss only.\")\n",
    "#                  g_loss = loss_adv_recon\n",
    "#                  bal_a_item = 0.0\n",
    "#                  g_optimizer.zero_grad()\n",
    "#                  loss_adv_recon.backward() # Calculate grads for fallback sum\n",
    "\n",
    "#             # --- !!! NO g_loss.backward() needed here !!! ---\n",
    "\n",
    "#         else: # --- If NOT using gradient balancing ---\n",
    "#             # Simple weighted sum of all component losses\n",
    "#             g_loss = loss_G_adv + (lambda_ocr * loss_OCR_fake)\n",
    "#             bal_a_item = lambda_ocr\n",
    "#             # Calculate gradients for the simple sum\n",
    "#             g_loss.backward()\n",
    "#             # ------------------------------------------\n",
    "\n",
    "#         # --- Optimizer Step (Uses gradients stored in p.grad) ---\n",
    "#         g_optimizer.step()\n",
    "#         total_g_loss_epoch += g_loss.item() # Log the final calculated g_loss value\n",
    "\n",
    "#         # --- Logging ---\n",
    "#         log_dict = {\n",
    "#             \"G_Loss\": f\"{g_loss.item():.4f}\", \"D_Loss\": f\"{d_loss.item():.4f}\",\n",
    "#             \"G_Adv\": f\"{loss_G_adv.item():.4f}\", \"G_OCR\": f\"{loss_OCR_fake.item():.4f}\",\n",
    "#             \"Bal_a\": f\"{bal_a_item:.2f}\"\n",
    "#         }\n",
    "#         pbar.set_postfix(log_dict)\n",
    "\n",
    "#         # --- Save Models Periodically ---\n",
    "#         if ((epoch + 1) % model_save_interval == 0 or epoch == num_epochs - 1) and i == 0:\n",
    "#             print(f\"Saving models for epoch {epoch+1}...\")\n",
    "#             # Handle DataParallel state dict saving correctly\n",
    "#             if isinstance(generator, nn.DataParallel):\n",
    "#                 torch.save(generator.module.state_dict(), os.path.join(output_dir, f\"generator_epoch_{epoch+1}.pth\"))\n",
    "#                 torch.save(discriminator.module.state_dict(), os.path.join(output_dir, f\"discriminator_epoch_{epoch+1}.pth\"))\n",
    "#             else:\n",
    "#                 torch.save(generator.state_dict(), os.path.join(output_dir, f\"generator_epoch_{epoch+1}.pth\"))\n",
    "#                 torch.save(discriminator.state_dict(), os.path.join(output_dir, f\"discriminator_epoch_{epoch+1}.pth\"))\n",
    "#             print(\"Models saved.\")\n",
    "            \n",
    "#         # --- Generate and Display Samples Periodically ---\n",
    "#         if (i + 1) % 33 == 0 and fixed_words_to_generate is not None:\n",
    "#             generator.eval()\n",
    "#             with torch.no_grad():\n",
    "#                 # print(f\"\\n--- Generating Word Samples Epoch {epoch+1} ---\")\n",
    "#                 generator_to_sample = generator.module if isinstance(generator, nn.DataParallel) else generator\n",
    "#                 generated_word_tensors, output_words = generateWordImage(\n",
    "#                     words=fixed_words_to_generate,\n",
    "#                     generator=generator_to_sample, # Pass unwrapped model\n",
    "#                     encoder=encoder,\n",
    "#                     device=device\n",
    "#                 )\n",
    "#                 if(len(generated_word_tensors) != len(output_words)):\n",
    "#                     print(f\"WARNING!!!!\\nNumber of Images:{len(generated_word_tensors)} != Number of Words:{len(output_words)}\")\n",
    "#                 display_word_samples(\n",
    "#                         generated_tensors=generated_word_tensors,\n",
    "#                         word_labels=fixed_words_to_generate,\n",
    "#                         num_to_display=num_samples_to_save, # Use config variable\n",
    "#                         epoch=epoch, sample_output_dir=sample_output_dir,\n",
    "#                         batch_idx=i\n",
    "#                     )\n",
    "    \n",
    "#             generator.train()\n",
    "#     # --- End of Epoch ---\n",
    "#     avg_g_loss = total_g_loss_epoch / len(dataloader)\n",
    "#     avg_d_loss = total_d_loss_epoch / len(dataloader)\n",
    "#     avg_ocr_fake = total_ocr_fake_epoch / len(dataloader) if len(dataloader) > 0 else 0\n",
    "#     print(f\"Epoch {epoch+1} Complete. Avg G: {avg_g_loss:.4f}, Avg D: {avg_d_loss:.4f}, Avg G_OCR: {avg_ocr_fake:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Training Finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20000:   0%|                                                                                | 0/157 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Training Loop\n",
    "# ---------------------------\n",
    "print(\"Starting Training...\")\n",
    "fixed_noise_for_samples = torch.randn(batch_size, noise_dim).to(device) # Use fixed noise for consistent sample generation across epochs\n",
    "fixed_sample_batch = None # Will store one batch for sample generation\n",
    "\n",
    "\n",
    "fixed_noise_for_patch_samples = torch.randn(batch_size, noise_dim).to(device)\n",
    "fixed_patch_sample_batch = None\n",
    "optimizer_needs_step = False\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    # recognizer stays in eval mode\n",
    "    total_g_loss_epoch, total_d_loss_epoch, total_ocr_fake_epoch = 0.0, 0.0, 0.0\n",
    "    g_loss, d_loss = torch.tensor(0.0), torch.tensor(0.0)\n",
    "    g_loss_adv, loss_OCR_fake = torch.tensor(0.0), torch.tensor(0.0)\n",
    "    bal_a_item = 0.0\n",
    "    log_dict = {}\n",
    "\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=True)\n",
    "\n",
    "    for i, batch in enumerate(pbar):\n",
    "        input_patch, target_patch, class_name_batch = batch\n",
    "        current_batch_size = input_patch.size(0)\n",
    "\n",
    "        word_batch_for_samples = next(iter(word_dataloader)) # Get one batch of words\n",
    "        fixed_words_to_generate = word_batch_for_samples # Take first N\n",
    "\n",
    "        # Store fixed batch for patch sampling\n",
    "        if i == 0 and fixed_patch_sample_batch is None:\n",
    "             num_fixed = min(current_batch_size, fixed_noise_for_samples.size(0))\n",
    "             fixed_patch_sample_batch = (input_patch[:num_fixed].clone().cpu(), class_name_batch[:num_fixed])\n",
    "\n",
    "        # Prepare FastText & Move data\n",
    "        with torch.no_grad(): # No need to track gradients for the pre-trained encoder\n",
    "            fasttext_vector_batch = torch.from_numpy(\n",
    "                encoder(class_name_batch) # Call your encoder module\n",
    "            ).to(device).float()\n",
    "\n",
    "        # --- Train Discriminator (on PATCHES) ---\n",
    "        toggle_grad(generator, False); toggle_grad(discriminator, True); toggle_grad(recognizer, False)\n",
    "        d_optimizer.zero_grad(set_to_none=True)\n",
    "        # Real Pass\n",
    "        d_output_real = discriminator(input_patch, target_patch, fasttext_vector_batch)\n",
    "        d_loss_real = torch.mean(F.relu(1.0 - d_output_real))\n",
    "        # Fake Pass\n",
    "        noise = torch.randn(current_batch_size, noise_dim).to(device)\n",
    "        with torch.no_grad(): fake_patch_images = generator(input_patch, fasttext_vector_batch, noise).detach()\n",
    "        d_output_fake = discriminator(input_patch, fake_patch_images, fasttext_vector_batch)\n",
    "        d_loss_fake = torch.mean(F.relu(1.0 + d_output_fake))\n",
    "        # Combined D Loss\n",
    "        d_loss = (d_loss_real + d_loss_fake) * 0.5\n",
    "        d_loss.backward(); d_optimizer.step()\n",
    "        total_d_loss_epoch += d_loss.item()\n",
    "        \n",
    "        # --- Train Generator ---\n",
    "        toggle_grad(generator, True); toggle_grad(discriminator, False); toggle_grad(recognizer, False)\n",
    "        g_optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # --- Generate FAKE PATCHES for Adv/L1/Perc ---\n",
    "        # We need gradients flowing back from these patches for Adv/L1 losses\n",
    "        noise_g = torch.randn(current_batch_size, noise_dim).to(device)\n",
    "        fake_patches_g = generator(input_patch, fasttext_vector_batch, noise_g)\n",
    "        if not fake_patches_g.requires_grad: fake_patches_g.requires_grad_(True)\n",
    "\n",
    "        # --- Calculate Patch-Based Losses ---\n",
    "        # Adversarial Loss\n",
    "        d_output_g = discriminator(input_patch, fake_patches_g, fasttext_vector_batch)\n",
    "        loss_G_adv = -torch.mean(d_output_g)\n",
    "        # Combine Adv and Reconstruction losses\n",
    "\n",
    "        # --- Generate FULL WORDS for OCR Loss ---\n",
    "        loss_OCR_fake = torch.tensor(0.0).to(device) # Default OCR loss\n",
    "        generated_words_batch, gt_word_labels_for_ocr = None, None\n",
    "        if lambda_ocr > 0:\n",
    "            generator_to_render = generator.module if isinstance(generator, nn.DataParallel) else generator\n",
    "            current_word_list = word_batch_for_samples\n",
    "            generated_words_padded = None # Initialize\n",
    "            \n",
    "            try:\n",
    "                generated_words_batch, gt_word_labels_for_ocr = generateWordImage( # Your wrapper function\n",
    "                    words=current_word_list,\n",
    "                    generator=generator_to_render,\n",
    "                    encoder=encoder,\n",
    "                    device=device,\n",
    "                    # Pass any other necessary args required by generateWordImage\n",
    "                )\n",
    "\n",
    "                # if generated_words_batch is not None and generated_words_batch.requires_grad and gt_word_labels_for_ocr:\n",
    "                if generated_words_batch is not None and gt_word_labels_for_ocr != []:\n",
    "                    # --- Calculate OCR Loss on Generated Words ---\n",
    "                    # Encode the GROUND TRUTH labels returned by generateWordImage\n",
    "                    generated_words_batch = generated_words_batch.unsqueeze(1)\n",
    "                    generated_words_batch = generated_words_batch.to(device)\n",
    "                    text_encoded_ocr, len_text_ocr = converter.encode(gt_word_labels_for_ocr) # Encode GT labels\n",
    "                    text_encoded_ocr = text_encoded_ocr.to(device)\n",
    "                    len_text_ocr = len_text_ocr.to(device)\n",
    "\n",
    "                    # Ensure batch sizes match after potential failures in generation\n",
    "                    ocr_batch_size = generated_words_batch.size(0)\n",
    "                    if ocr_batch_size != len(gt_word_labels_for_ocr) or ocr_batch_size != text_encoded_ocr.size(0) or ocr_batch_size != len_text_ocr.size(0):\n",
    "                         raise ValueError(f\"Batch size mismatch in OCR inputs after generation ({ocr_batch_size} vs {len(gt_word_labels_for_ocr)})\")\n",
    "\n",
    "                    # Run Recognizer\n",
    "                    pred_fake_OCR = recognizer(generated_words_batch) # R is eval\n",
    "                    log_probs_fake = F.log_softmax(pred_fake_OCR, dim=2)\n",
    "                    preds_size_fake = torch.IntTensor([pred_fake_OCR.size(0)] * ocr_batch_size).to(device)\n",
    "\n",
    "                    # Calculate CTC Loss\n",
    "                    loss_OCR_fake_batch = OCR_criterion(log_probs_fake, text_encoded_ocr.detach(), preds_size_fake, len_text_ocr.detach())\n",
    "                    valid_losses_fake = loss_OCR_fake_batch[~torch.isnan(loss_OCR_fake_batch) & ~torch.isinf(loss_OCR_fake_batch)]\n",
    "                    if valid_losses_fake.numel() > 0:\n",
    "                        loss_OCR_fake = torch.mean(valid_losses_fake) # Grad exists via generated_words_batch\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error during approx OCR loss calculation: {e}\")\n",
    "                loss_OCR_fake = torch.tensor(0.0).to(device)\n",
    "        total_ocr_fake_epoch += loss_OCR_fake.item() # Track approx OCR loss\n",
    "\n",
    "        # --- Combine ALL Generator Losses (Simple Weighted Sum) ---\n",
    "        g_loss_final = loss_G_adv + (loss_OCR_fake * lambda_ocr) # Add weighted OCR loss\n",
    "\n",
    "        # --- Backward and Optimize ---\n",
    "        if g_loss_final.requires_grad:\n",
    "             g_loss_final.backward()\n",
    "             optimizer_needs_step = True\n",
    "        else:\n",
    "             print(f\"W: Final G loss (Batch {i}) does not require grad. Skipping G step.\")\n",
    "             optimizer_needs_step = False\n",
    "\n",
    "        if optimizer_needs_step: g_optimizer.step()\n",
    "        total_g_loss_epoch += g_loss_final.item()\n",
    "\n",
    "        # --- Logging ---\n",
    "        log_dict = {\n",
    "            \"G_Loss\": f\"{g_loss_final.item():.4f}\", \"D_Loss\": f\"{d_loss.item():.4f}\",\n",
    "            \"G_Adv\": f\"{loss_G_adv.item():.4f}\", \"G_OCR\": f\"{loss_OCR_fake.item():.4f}\",\n",
    "        }\n",
    "        # Add G_Perc if used\n",
    "        pbar.set_postfix(log_dict)\n",
    "\n",
    "        # --- Save Models Periodically ---\n",
    "        if ((epoch + 1) % model_save_interval == 0 or epoch == num_epochs - 1) and i == 0:\n",
    "            print(f\"Saving models for epoch {epoch+1}...\")\n",
    "            # Handle DataParallel state dict saving correctly\n",
    "            if isinstance(generator, nn.DataParallel):\n",
    "                torch.save(generator.module.state_dict(), os.path.join(output_dir, f\"generator_epoch_{epoch+1}.pth\"))\n",
    "                torch.save(discriminator.module.state_dict(), os.path.join(output_dir, f\"discriminator_epoch_{epoch+1}.pth\"))\n",
    "            else:\n",
    "                torch.save(generator.state_dict(), os.path.join(output_dir, f\"generator_epoch_{epoch+1}.pth\"))\n",
    "                torch.save(discriminator.state_dict(), os.path.join(output_dir, f\"discriminator_epoch_{epoch+1}.pth\"))\n",
    "            print(\"Models saved.\")\n",
    "            \n",
    "        # --- Generate and Display Samples Periodically ---\n",
    "        if (i + 1) % 15 == 0 and fixed_words_to_generate is not None:\n",
    "            generator.eval()\n",
    "            with torch.no_grad():\n",
    "                # print(f\"\\n--- Generating Word Samples Epoch {epoch+1} ---\")\n",
    "                if(generated_words_batch != None and gt_word_labels_for_ocr != []):\n",
    "                    display_word_samples(\n",
    "                        generated_tensors=generated_words_batch,\n",
    "                        word_labels=gt_word_labels_for_ocr,\n",
    "                        num_to_display=num_samples_to_save, # Use config variable\n",
    "                        epoch=epoch, sample_output_dir=sample_output_dir,\n",
    "                        batch_idx=i\n",
    "                    )\n",
    "    \n",
    "            generator.train()\n",
    "    # --- End of Epoch ---\n",
    "    avg_g_loss = total_g_loss_epoch / len(dataloader)\n",
    "    avg_d_loss = total_d_loss_epoch / len(dataloader)\n",
    "    avg_ocr_fake = total_ocr_fake_epoch / len(dataloader) if len(dataloader) > 0 else 0\n",
    "    print(f\"Epoch {epoch+1} Complete. Avg G: {avg_g_loss:.4f}, Avg D: {avg_d_loss:.4f}, Avg G_OCR: {avg_ocr_fake:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Training Finished.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7063308,
     "sourceId": 11295851,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
