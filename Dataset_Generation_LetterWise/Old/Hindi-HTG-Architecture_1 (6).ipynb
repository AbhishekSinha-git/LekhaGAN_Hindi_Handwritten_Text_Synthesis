{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (4.67.1)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.11/site-packages (3.10.1)\n",
      "Collecting editdistance\n",
      "  Downloading editdistance-0.8.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting pytorch_msssim\n",
      "  Downloading pytorch_msssim-1.0.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.11/site-packages (from opencv-python) (2.2.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.2.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.6.3)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Downloading protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-3.0.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.71.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Downloading keras-3.9.1-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting numpy>=1.21.2 (from opencv-python)\n",
      "  Downloading numpy-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.13.0)\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Collecting numpy>=1.21.2 (from opencv-python)\n",
      "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (from pytorch_msssim) (2.6.0+cu118)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow)\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch->pytorch_msssim) (3.17.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch->pytorch_msssim) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch->pytorch_msssim) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch->pytorch_msssim) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /opt/conda/lib/python3.11/site-packages (from torch->pytorch_msssim) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /opt/conda/lib/python3.11/site-packages (from torch->pytorch_msssim) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /opt/conda/lib/python3.11/site-packages (from torch->pytorch_msssim) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch->pytorch_msssim) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /opt/conda/lib/python3.11/site-packages (from torch->pytorch_msssim) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.11/site-packages (from torch->pytorch_msssim) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /opt/conda/lib/python3.11/site-packages (from torch->pytorch_msssim) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /opt/conda/lib/python3.11/site-packages (from torch->pytorch_msssim) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /opt/conda/lib/python3.11/site-packages (from torch->pytorch_msssim) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch->pytorch_msssim) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /opt/conda/lib/python3.11/site-packages (from torch->pytorch_msssim) (11.8.86)\n",
      "Requirement already satisfied: triton==3.2.0 in /opt/conda/lib/python3.11/site-packages (from torch->pytorch_msssim) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch->pytorch_msssim) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch->pytorch_msssim) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading editdistance-0.8.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (412 kB)\n",
      "Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytorch_msssim-1.0.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading absl_py-2.2.1-py3-none-any.whl (277 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.71.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.9.1-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Downloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-3.0.0-py3-none-any.whl (6.3 kB)\n",
      "Downloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
      "Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, opt-einsum, numpy, mdurl, markdown, grpcio, google-pasta, gast, editdistance, absl-py, tensorboard, smart-open, scipy, opencv-python, ml-dtypes, markdown-it-py, rich, pytorch_msssim, gensim, keras, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.2\n",
      "    Uninstalling numpy-2.2.2:\n",
      "      Successfully uninstalled numpy-2.2.2\n",
      "Successfully installed absl-py-2.2.1 editdistance-0.8.1 flatbuffers-25.2.10 gast-0.6.0 gensim-4.3.3 google-pasta-0.2.0 grpcio-1.71.0 keras-3.9.1 libclang-18.1.1 markdown-3.7 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.5.1 namex-0.0.8 numpy-1.26.4 opencv-python-4.11.0.86 opt-einsum-3.4.0 protobuf-5.29.4 pytorch_msssim-1.0.0 rich-14.0.0 scipy-1.13.1 smart-open-7.1.0 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.37.1 termcolor-3.0.0 werkzeug-3.1.3 wrapt-1.17.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python tqdm matplotlib editdistance tensorflow gensim pytorch_msssim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "dd2ce8d2-2b01-4322-b3f8-02beacf815f2",
    "_kg_hide-input": false,
    "_uuid": "fd463609-9d0b-4b5d-a279-aa1936a888cf",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "conjunct = ['091C_094D', '0915_094D', '0924_094D']\n",
    "charFolders = ['0905', '0905_0902', '0905_0903', '0906', '0907', '0908', '0909', '090A', '090F', '0910', '0913', '0914', '0915', '0915_093E', '0915_093F', '0915_0940', '0915_0941', '0915_0942', '0915_0947', '0915_0948', '0915_094B', '0915_094C', '0915_094D', '0915_094D_0937', '0915_094D_0937_0903', '0915_094D_0937_093E', '0915_094D_0937_0940', '0915_094D_0937_0941', '0915_094D_0937_0942', '0915_094D_0937_0947', '0915_094D_0937_0948', '0915_094D_0937_094B', '0915_094D_0937_094C', '0916', '0916_093E', '0916_093F', '0916_0941', '0916_0942', '0916_0948', '0916_094B', '0916_094C', '0916_094D', '0917', '0917_093E', '0917_093F', '0917_0940', '0917_0941', '0917_0942', '0917_0947', '0917_0948', '0917_094B', '0917_094C', '0917_094D', '0918', '0918_093E', '0918_093F', '0918_0940', '0918_0941', '0918_0942', '0918_0947', '0918_0948', '0918_094B', '0918_094C', '0918_094D', '0919', '0919_0902', '0919_0903', '0919_093E', '0919_093F', '0919_0940', '0919_0941', '0919_0942', '0919_0947', '0919_0948', '0919_094B', '0919_094C', '091A', '091A_0902', '091A_0903', '091A_093E', '091A_093F', '091A_0940', '091A_0941', '091A_0942', '091A_0947', '091A_0948', '091A_094B', '091A_094C', '091B', '091B_0902', '091B_0903', '091B_093E', '091B_093F', '091B_0940', '091B_0941', '091B_0942', '091B_0947', '091B_0948', '091B_094B', '091B_094C', '091C', '091C_0902', '091C_0903', '091C_093E', '091C_093F', '091C_0940', '091C_0941', '091C_0942', '091C_0947', '091C_0948', '091C_094B', '091C_094C', '091C_094D_091E', '091C_094D_091E_0902', '091C_094D_091E_0903', '091C_094D_091E_093E', '091C_094D_091E_093F', '091C_094D_091E_0940', '091C_094D_091E_0941', '091C_094D_091E_0942', '091C_094D_091E_0947', '091C_094D_091E_0948', '091C_094D_091E_094B', '091C_094D_091E_094C', '091D', '091D_0902', '091D_0903', '091D_093E', '091D_093F', '091D_0940', '091D_0941', '091D_0942', '091D_0947', '091D_0948', '091D_094B', '091D_094C', '091E', '091E_0902', '091E_0903', '091E_093E', '091E_093F', '091E_0940', '091E_0941', '091E_0942', '091E_0947', '091E_0948', '091E_094B', '091E_094C', '091F', '091F_0903', '091F_093E', '091F_093F', '091F_0940', '091F_0941', '091F_0942', '091F_0947', '091F_0948', '091F_094B', '091F_094C', '0920', '0920_0903', '0920_093E', '0920_093F', '0920_0940', '0920_0941', '0920_0942', '0920_0947', '0920_0948', '0920_094B', '0920_094C', '0921', '0921_0902', '0921_0903', '0921_093E', '0921_093F', '0921_0940', '0921_0941', '0921_0942', '0921_0947', '0921_0948', '0921_094B', '0921_094C', '0922', '0922_0902', '0922_0903', '0922_093E', '0922_093F', '0922_0940', '0922_0941', '0922_0942', '0922_0947', '0922_0948', '0922_094B', '0922_094C', '0923', '0923_0902', '0923_0903', '0923_093E', '0923_093F', '0923_0940', '0923_0941', '0923_0942', '0923_0947', '0923_0948', '0923_094B', '0923_094C', '0924', '0924_0902', '0924_0903', '0924_093E', '0924_093F', '0924_0940', '0924_0941', '0924_0942', '0924_0947', '0924_0948', '0924_094B', '0924_094C', '0924_094D_0930', '0924_094D_0930_0902', '0924_094D_0930_0903', '0924_094D_0930_093E', '0924_094D_0930_093F', '0924_094D_0930_0940', '0924_094D_0930_0941', '0924_094D_0930_0942', '0924_094D_0930_0947', '0924_094D_0930_0948', '0924_094D_0930_094B', '0924_094D_0930_094C', '0925', '0925_0902', '0925_093E', '0925_093F', '0925_0940', '0925_0941', '0925_0942', '0925_0947', '0925_0948', '0925_094B', '0925_094C', '0926', '0926_0902', '0926_0902_0903', '0926_0903', '0926_093E', '0926_093F', '0926_0940', '0926_0941', '0926_0942', '0926_0947', '0926_0948', '0926_094B', '0926_094C', '0927', '0927_0902', '0927_0903', '0927_093E', '0927_093F', '0927_0940', '0927_0941', '0927_0942', '0927_0947', '0927_0948', '0927_094B', '0927_094C', '0928', '0928_0903', '0928_093E', '0928_093F', '0928_0940', '0928_0941', '0928_0942', '0928_0947', '0928_0948', '0928_094B', '0928_094C', '092A', '092A_0902', '092A_0903', '092A_093E', '092A_093F', '092A_0940', '092A_0941', '092A_0942', '092A_0947', '092A_0948', '092A_094B', '092A_094C', '092B', '092B_0902', '092B_0903', '092B_093E', '092B_093F', '092B_0940', '092B_0941', '092B_0942', '092B_0947', '092B_0948', '092B_094B', '092B_094C', '092C', '092C_0902', '092C_0903', '092C_093E', '092C_093F', '092C_0940', '092C_0941', '092C_0942', '092C_0947', '092C_0948', '092C_094B', '092C_094C', '092D', '092D_0902', '092D_0903', '092D_093E', '092D_093F', '092D_0940', '092D_0941', '092D_0942', '092D_0947', '092D_0948', '092D_094B', '092D_094C', '092E', '092E_0902', '092E_0903', '092E_093E', '092E_093F', '092E_0940', '092E_0941', '092E_0942', '092E_0947', '092E_0948', '092E_0948_0902', '092E_094B', '092E_094C', '092F', '092F_0902', '092F_0903', '092F_093E', '092F_093F', '092F_0940', '092F_0941', '092F_0942', '092F_0947', '092F_0948', '092F_094B', '092F_094C', '0930', '0930_0902', '0930_0903', '0930_093E', '0930_093F', '0930_0940', '0930_0941', '0930_0942', '0930_0947', '0930_0948', '0930_094B', '0930_094C', '0932', '0932_0902', '0932_0903', '0932_093E', '0932_093F', '0932_0940', '0932_0941', '0932_0942', '0932_0947', '0932_0948', '0932_094B', '0932_094C', '0935', '0935_0902', '0935_0903', '0935_093E', '0935_093F', '0935_0940', '0935_0941', '0935_0942', '0935_0947', '0935_0948', '0935_094B', '0935_094C', '0936', '0936_0902', '0936_0903', '0936_093E', '0936_093F', '0936_0940', '0936_0941', '0936_0942', '0936_0947', '0936_0948', '0936_094B', '0936_094C', '0937', '0937_0902', '0937_0903', '0937_093E', '0937_093F', '0937_0940', '0937_0941', '0937_0942', '0937_0947', '0937_0948', '0937_094B', '0937_094C', '0938', '0938_0902', '0938_0903', '0938_093E', '0938_093F', '0938_0940', '0938_0941', '0938_0942', '0938_0947', '0938_0948', '0938_094B', '0938_094C', '0939', '0939_0902', '0939_0903', '0939_093E', '0939_093F', '0939_0940', '0939_0941', '0939_0942', '0939_0947', '0939_0948', '0939_094B', '0939_094C', '0966', '0967', '0968', '0969', '096A', '096B', '096C', '096D', '096E', '096F']\n",
    "\n",
    "def getLetterTokens(word: str):\n",
    "\n",
    "    wordComb = []\n",
    "    approved_words = []\n",
    "\n",
    "    # for each word in dict\n",
    "\n",
    "    hindi_word = word\n",
    "    charList = []\n",
    "\n",
    "    if(len(word) <= 31): # set word length limit to 31 characters (including matras)\n",
    "        word = word.replace('\\n', '')\n",
    "        characters = []\n",
    "        for ch in word: # convert letters to unicode representations and store\n",
    "            characters.append(('0' + hex(ord(ch))[2:]).upper())\n",
    "        \n",
    "        check = True\n",
    "        i = 0\n",
    "        # for each unicode character representation of current word\n",
    "        while check and i < len(characters):\n",
    "            check = False\n",
    "            word = ''\n",
    "\n",
    "            # add join current and next char\n",
    "            if i < len(characters) - 1:\n",
    "                word = characters[i] + '_' + characters[i+1]\n",
    "            \n",
    "            # if half (halant) letter exists in combination to next character\n",
    "            if word in conjunct and i < len(characters) - 2:\n",
    "                word2 = word + '_' + characters[i+2] # join with next character to check for more possibilities\n",
    "                if word2 in charFolders: # if the current handwritten character combination exists\n",
    "                    # if still not reached end and concatenation of next character exists in folder\n",
    "                    if i < len(characters) - 3 and word2 + '_' + characters[i+3] in charFolders:\n",
    "                        charList.append(word2 + '_' + characters[i+3]) # add to charlist\n",
    "                        check = True\n",
    "                        i += 4\n",
    "                    else: # add the next character as a seperate sequence element\n",
    "                        charList.append(word2)\n",
    "                        check = True\n",
    "                        i += 3\n",
    "            \n",
    "            # above if condition only adds char to charlist if that subsequence is found in folder names, and so check is set to true\n",
    "            # below, if check is false, only then we fall back to the word (character[i] + character[i+1])\n",
    "            # or character (character[i]) combination addition to charlist\n",
    "            \n",
    "            # check if word (character[i] + character[i+1]) combination exists and adds to charlist\n",
    "            # and sets flag to true\n",
    "            if check == False and word in charFolders:\n",
    "                check = True\n",
    "                charList.append(word)\n",
    "                i += 2\n",
    "            \n",
    "            # if word also does not exist, then only the character is added to charlist given that it exists in the folder\n",
    "            if check == False and characters[i] in charFolders:\n",
    "                check = True\n",
    "                charList.append(characters[i])\n",
    "                i += 1\n",
    "        \n",
    "        # appends all information for that word as well as the annotated word\n",
    "        if check == True:\n",
    "            wordComb.append((charList, word))\n",
    "            approved_words.append(hindi_word)\n",
    "    \n",
    "    # if(wordComb != []):\n",
    "    return wordComb[0][0], approved_words\n",
    "    # else:\n",
    "    #     return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-02 09:26:07.802467: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-02 09:26:07.812450: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743585967.830513     260 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743585967.836228     260 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743585967.850802     260 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743585967.850818     260 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743585967.850820     260 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743585967.850822     260 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-02 09:26:07.855751: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, IterableDataset, DataLoader\n",
    "from torch.utils.data._utils.collate import default_convert\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pickle, h5py\n",
    "import time\n",
    "from gensim.models.fasttext import load_facebook_model\n",
    "import random, os, re\n",
    "import pytorch_msssim\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "K.clear_session()\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tokens = ['0905', '0905_0902', '0905_0903', '0906', '0907', '0908', '0909', '090A', '090F', '0910', '0913', '0914', '0915', '0915_093E', '0915_093F', '0915_0940', '0915_0941', '0915_0942', '0915_0947', '0915_0948', '0915_094B', '0915_094C', '0915_094D', '0915_094D_0937', '0915_094D_0937_0903', '0915_094D_0937_093E', '0915_094D_0937_0940', '0915_094D_0937_0941', '0915_094D_0937_0942', '0915_094D_0937_0947', '0915_094D_0937_0948', '0915_094D_0937_094B', '0915_094D_0937_094C', '0916', '0916_093E', '0916_093F', '0916_0941', '0916_0942', '0916_0948', '0916_094B', '0916_094C', '0916_094D', '0917', '0917_093E', '0917_093F', '0917_0940', '0917_0941', '0917_0942', '0917_0947', '0917_0948', '0917_094B', '0917_094C', '0917_094D', '0918', '0918_093E', '0918_093F', '0918_0940', '0918_0941', '0918_0942', '0918_0947', '0918_0948', '0918_094B', '0918_094C', '0918_094D', '0919', '0919_0902', '0919_0903', '0919_093E', '0919_093F', '0919_0940', '0919_0941', '0919_0942', '0919_0947', '0919_0948', '0919_094B', '0919_094C', '091A', '091A_0902', '091A_0903', '091A_093E', '091A_093F', '091A_0940', '091A_0941', '091A_0942', '091A_0947', '091A_0948', '091A_094B', '091A_094C', '091B', '091B_0902', '091B_0903', '091B_093E', '091B_093F', '091B_0940', '091B_0941', '091B_0942', '091B_0947', '091B_0948', '091B_094B', '091B_094C', '091C', '091C_0902', '091C_0903', '091C_093E', '091C_093F', '091C_0940', '091C_0941', '091C_0942', '091C_0947', '091C_0948', '091C_094B', '091C_094C', '091C_094D_091E', '091C_094D_091E_0902', '091C_094D_091E_0903', '091C_094D_091E_093E', '091C_094D_091E_093F', '091C_094D_091E_0940', '091C_094D_091E_0941', '091C_094D_091E_0942', '091C_094D_091E_0947', '091C_094D_091E_0948', '091C_094D_091E_094B', '091C_094D_091E_094C', '091D', '091D_0902', '091D_0903', '091D_093E', '091D_093F', '091D_0940', '091D_0941', '091D_0942', '091D_0947', '091D_0948', '091D_094B', '091D_094C', '091E', '091E_0902', '091E_0903', '091E_093E', '091E_093F', '091E_0940', '091E_0941', '091E_0942', '091E_0947', '091E_0948', '091E_094B', '091E_094C', '091F', '091F_0903', '091F_093E', '091F_093F', '091F_0940', '091F_0941', '091F_0942', '091F_0947', '091F_0948', '091F_094B', '091F_094C', '0920', '0920_0903', '0920_093E', '0920_093F', '0920_0940', '0920_0941', '0920_0942', '0920_0947', '0920_0948', '0920_094B', '0920_094C', '0921', '0921_0902', '0921_0903', '0921_093E', '0921_093F', '0921_0940', '0921_0941', '0921_0942', '0921_0947', '0921_0948', '0921_094B', '0921_094C', '0922', '0922_0902', '0922_0903', '0922_093E', '0922_093F', '0922_0940', '0922_0941', '0922_0942', '0922_0947', '0922_0948', '0922_094B', '0922_094C', '0923', '0923_0902', '0923_0903', '0923_093E', '0923_093F', '0923_0940', '0923_0941', '0923_0942', '0923_0947', '0923_0948', '0923_094B', '0923_094C', '0924', '0924_0902', '0924_0903', '0924_093E', '0924_093F', '0924_0940', '0924_0941', '0924_0942', '0924_0947', '0924_0948', '0924_094B', '0924_094C', '0924_094D_0930', '0924_094D_0930_0902', '0924_094D_0930_0903', '0924_094D_0930_093E', '0924_094D_0930_093F', '0924_094D_0930_0940', '0924_094D_0930_0941', '0924_094D_0930_0942', '0924_094D_0930_0947', '0924_094D_0930_0948', '0924_094D_0930_094B', '0924_094D_0930_094C', '0925', '0925_0902', '0925_093E', '0925_093F', '0925_0940', '0925_0941', '0925_0942', '0925_0947', '0925_0948', '0925_094B', '0925_094C', '0926', '0926_0902', '0926_0902_0903', '0926_0903', '0926_093E', '0926_093F', '0926_0940', '0926_0941', '0926_0942', '0926_0947', '0926_0948', '0926_094B', '0926_094C', '0927', '0927_0902', '0927_0903', '0927_093E', '0927_093F', '0927_0940', '0927_0941', '0927_0942', '0927_0947', '0927_0948', '0927_094B', '0927_094C', '0928', '0928_0903', '0928_093E', '0928_093F', '0928_0940', '0928_0941', '0928_0942', '0928_0947', '0928_0948', '0928_094B', '0928_094C', '092A', '092A_0902', '092A_0903', '092A_093E', '092A_093F', '092A_0940', '092A_0941', '092A_0942', '092A_0947', '092A_0948', '092A_094B', '092A_094C', '092B', '092B_0902', '092B_0903', '092B_093E', '092B_093F', '092B_0940', '092B_0941', '092B_0942', '092B_0947', '092B_0948', '092B_094B', '092B_094C', '092C', '092C_0902', '092C_0903', '092C_093E', '092C_093F', '092C_0940', '092C_0941', '092C_0942', '092C_0947', '092C_0948', '092C_094B', '092C_094C', '092D', '092D_0902', '092D_0903', '092D_093E', '092D_093F', '092D_0940', '092D_0941', '092D_0942', '092D_0947', '092D_0948', '092D_094B', '092D_094C', '092E', '092E_0902', '092E_0903', '092E_093E', '092E_093F', '092E_0940', '092E_0941', '092E_0942', '092E_0947', '092E_0948', '092E_0948_0902', '092E_094B', '092E_094C', '092F', '092F_0902', '092F_0903', '092F_093E', '092F_093F', '092F_0940', '092F_0941', '092F_0942', '092F_0947', '092F_0948', '092F_094B', '092F_094C', '0930', '0930_0902', '0930_0903', '0930_093E', '0930_093F', '0930_0940', '0930_0941', '0930_0942', '0930_0947', '0930_0948', '0930_094B', '0930_094C', '0932', '0932_0902', '0932_0903', '0932_093E', '0932_093F', '0932_0940', '0932_0941', '0932_0942', '0932_0947', '0932_0948', '0932_094B', '0932_094C', '0935', '0935_0902', '0935_0903', '0935_093E', '0935_093F', '0935_0940', '0935_0941', '0935_0942', '0935_0947', '0935_0948', '0935_094B', '0935_094C', '0936', '0936_0902', '0936_0903', '0936_093E', '0936_093F', '0936_0940', '0936_0941', '0936_0942', '0936_0947', '0936_0948', '0936_094B', '0936_094C', '0937', '0937_0902', '0937_0903', '0937_093E', '0937_093F', '0937_0940', '0937_0941', '0937_0942', '0937_0947', '0937_0948', '0937_094B', '0937_094C', '0938', '0938_0902', '0938_0903', '0938_093E', '0938_093F', '0938_0940', '0938_0941', '0938_0942', '0938_0947', '0938_0948', '0938_094B', '0938_094C', '0939', '0939_0902', '0939_0903', '0939_093E', '0939_093F', '0939_0940', '0939_0941', '0939_0942', '0939_0947', '0939_0948', '0939_094B', '0939_094C', '0966', '0967', '0968', '0969', '096A', '096B', '096C', '096D', '096E', '096F']\n",
    "unique_tokens = ['<PAD>', '<UNK>'] + unique_tokens\n",
    "token_id_map = {token: i for i, token in enumerate(unique_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 1\n",
      "GPU 0: NVIDIA A100-SXM4-80GB\n",
      "TensorFlow version: 2.19.0\n",
      "GPU Available: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1743585969.706448     260 gpu_device.cc:2341] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of available GPUs: {num_gpus}\")\n",
    "\n",
    "    # Print the name of each GPU\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"No CUDA devices available. Using CPU.\")\n",
    "\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import h5py\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomPickleDataset(Dataset):\n",
    "    def __init__(self, tokens_file, sizes_file, folder_path, token_id_map, pad_length=12):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tokens_file (str): Path to the pickle file containing tokens.\n",
    "            sizes_file (str): Path to the pickle file containing sizes.\n",
    "            folder_path (str): Path to the folder containing HDF5 files.\n",
    "            token_id_map (dict): Mapping of tokens to IDs.\n",
    "            pad_length (int): Maximum length for padding tokens.\n",
    "        \"\"\"\n",
    "        # Load tokens and sizes\n",
    "        with open(tokens_file, 'rb') as f:\n",
    "            self.tokens = pickle.load(f)\n",
    "        \n",
    "        with open(sizes_file, 'rb') as f:\n",
    "            self.sizes = pickle.load(f)\n",
    "\n",
    "        # Preload all HDF5 data into memory\n",
    "        self.data = []\n",
    "        self.token_id_map = token_id_map\n",
    "        self.pad_length = pad_length\n",
    "\n",
    "        def extract_number(file_path):\n",
    "            match = re.search(r'(\\d+)', file_path)\n",
    "            return int(match.group(1)) if match else float('inf')\n",
    "\n",
    "        self.h5_files = sorted(\n",
    "            [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.h5')],\n",
    "            key=extract_number\n",
    "        )[:1000]\n",
    "\n",
    "        # Load all chunks from all files into memory\n",
    "        for file_path in self.h5_files:\n",
    "            with h5py.File(file_path, 'r') as h5_file:\n",
    "                # print(f\"extracted data from file: {file_path}\")\n",
    "                file_data = h5_file['data']\n",
    "                self.data.extend(file_data[:])  # Load all data from the file\n",
    "\n",
    "        \n",
    "        # change length of tokens and sizes to keep lengths of variables consistent\n",
    "        self.tokens = self.tokens[:len(self.data)]\n",
    "        self.sizes = self.sizes[:len(self.data)]\n",
    "        \n",
    "        # Validate data consistency\n",
    "        assert len(self.tokens) == len(self.sizes) == len(self.data), \\\n",
    "            f\"Mismatch in data lengths: tokens({len(self.tokens)}), sizes({len(self.sizes)}), data({len(self.data)}).\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get tokenized and padded tokens\n",
    "        # print(f\"getting sample {idx}\")\n",
    "        tokens = self.tokens[idx][0]\n",
    "        tokens = [self.token_id_map[x] for x in tokens]\n",
    "        tokens = tokens + [self.token_id_map[\"<PAD>\"]] * (self.pad_length - len(tokens))\n",
    "        \n",
    "        word = self.tokens[idx][1]  # Get the word\n",
    "        size = self.sizes[idx]      # Get the size\n",
    "        chunk = self.data[idx]      # Get the corresponding chunk\n",
    "        return torch.tensor(tokens), word, torch.tensor(size), torch.tensor(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import os\n",
    "# import h5py\n",
    "# import re\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# class CustomPickleDataset(Dataset):\n",
    "#     def __init__(self, tokens_file, sizes_file, folder_path, token_id_map, pad_length=12):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             tokens_file (str): Path to the pickle file containing tokens.\n",
    "#             sizes_file (str): Path to the pickle file containing sizes.\n",
    "#             folder_path (str): Path to the folder containing HDF5 files.\n",
    "#             token_id_map (dict): Mapping of tokens to IDs.\n",
    "#             pad_length (int): Maximum length for padding tokens.\n",
    "#         \"\"\"\n",
    "#         # Load tokens and sizes\n",
    "#         with open(tokens_file, 'rb') as f:\n",
    "#             self.tokens = pickle.load(f)\n",
    "        \n",
    "#         with open(sizes_file, 'rb') as f:\n",
    "#             self.sizes = pickle.load(f)\n",
    "\n",
    "#         self.token_id_map = token_id_map\n",
    "#         self.pad_length = pad_length\n",
    "\n",
    "#         # Store paths to HDF5 files\n",
    "#         self.h5_files = sorted(\n",
    "#             [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.h5')],\n",
    "#             key=lambda x: int(re.search(r'(\\d+)', x).group(1)) if re.search(r'(\\d+)', x) else float('inf')\n",
    "#         )\n",
    "\n",
    "#         # Validate data consistency\n",
    "#         assert len(self.tokens) == len(self.sizes), \\\n",
    "#             f\"Mismatch in data lengths: tokens({len(self.tokens)}), sizes({len(self.sizes)}).\"\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.tokens)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         # Get tokenized and padded tokens\n",
    "#         tokens = self.tokens[idx][0]\n",
    "#         tokens = [self.token_id_map.get(x, self.token_id_map[\"<UNK>\"]) for x in tokens]  # Use <UNK> for unknown tokens\n",
    "#         tokens = tokens + [self.token_id_map[\"<PAD>\"]] * (self.pad_length - len(tokens))\n",
    "        \n",
    "#         word = self.tokens[idx][1]  # Get the word\n",
    "#         size = self.sizes[idx]      # Get the size\n",
    "\n",
    "#         # Load the corresponding chunk from the HDF5 file\n",
    "#         file_idx = idx // 200  # Assuming 1000 chunks per file (adjust based on your data)\n",
    "#         chunk_idx = idx % 200\n",
    "#         print(file_idx)\n",
    "#         with h5py.File(self.h5_files[file_idx], 'r') as h5_file:\n",
    "#             chunk = h5_file['data'][chunk_idx]\n",
    "\n",
    "#         return torch.tensor(tokens), word, torch.tensor(size), torch.tensor(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del dataset\n",
    "    del data_loader\n",
    "except: pass\n",
    "\n",
    "tokens_file = './dataset/tokens.pkl'\n",
    "sizes_file = './dataset/chunks_size_count.pkl'\n",
    "folder_path = './dataset/ImageChunksSmallCollated/ImageChunksSmallCollated'\n",
    "\n",
    "dataset = CustomPickleDataset(tokens_file, sizes_file, folder_path, token_id_map, pad_length=12)\n",
    "dataloader = DataLoader(dataset, batch_size=256, shuffle=True, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: torch.Size([256, 12])\n",
      "Words: 256\n",
      "Sizes: torch.Size([256])\n",
      "Chunks: torch.Size([256, 11, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    # print(batch)\n",
    "    tokens, words, sizes, chunks = batch\n",
    "    print(\"Tokens:\", tokens.shape)\n",
    "    print(\"Words:\", len(words[0]))\n",
    "    print(\"Sizes:\", sizes.shape)\n",
    "    print(\"Chunks:\", chunks.shape)  # Expect (batch_size, 11, 64, 64)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 11, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "print(chunks.shape)\n",
    "torch.save(chunks, \"sample_chunks.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sample_labels.pkl\", \"wb\") as f:\n",
    "    pickle.dump(words, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "लंपारह\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f767142e610>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "idxx = 1\n",
    "print(words[0][idxx])\n",
    "plt.imshow(np.hstack(chunks[idxx]), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.hi.300.bin.gz\n",
    "# !gunzip cc.hi.300.bin.gz\n",
    "# !ls -lh cc.hi.300.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText Encoder (averaged embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try: del model\n",
    "# except: pass\n",
    "# Load FastText Hindi model\n",
    "if \"model\" not in globals():\n",
    "    model = load_facebook_model(\"cc.hi.300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_hindi(uni_list):\n",
    "    return [\"\".join(map(lambda x: chr(int(x, 16)), u.split(\"_\"))) for u in uni_list]\n",
    "\n",
    "def getMeanEmbedding(text: str):\n",
    "    unicodes, word = getLetterTokens(text)\n",
    "    return np.mean([model.wv[x] for x in unicode_to_hindi(unicodes)], axis=0)\n",
    "\n",
    "class FastTextEncoder(nn.Module):\n",
    "    def __init__(self): super().__init__()\n",
    "    def forward(self, words):\n",
    "        \n",
    "        def unicode_to_hindi(uni_list):\n",
    "            return [\"\".join(map(lambda x: chr(int(x, 16)), u.split(\"_\"))) for u in uni_list]\n",
    "        \n",
    "        def getMeanEmbedding(text: str):\n",
    "            unicodes, word = getLetterTokens(text)\n",
    "            return np.mean([model.wv[x] for x in unicode_to_hindi(unicodes)], axis=0)\n",
    "\n",
    "        return np.array([getMeanEmbedding(word) for word in words[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(device)\n",
    "# encoderr = FastTextEncoder().to(device)\n",
    "# word_embeddings = encoderr(words).to(device)\n",
    "# # encoderr.forward(words).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.get_device_name(word_embeddings.get_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR Recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closeFit(img, padding=5):\n",
    "    # Check if image is empty or too small\n",
    "    if img is None or img.size == 0 or img.shape[0] < 5 or img.shape[1] < 5:\n",
    "        return img\n",
    "    \n",
    "    # Get image dimensions\n",
    "    height, width = img.shape[:2]\n",
    "    is_dark_background = np.mean(img) < 127\n",
    "    \n",
    "    # if is_dark_background:\n",
    "        # White text on black background - use directly\n",
    "    col_projection = np.sum(img, axis=0) / height\n",
    "    row_projection = np.sum(img, axis=1) / width\n",
    "    threshold_col = np.max(col_projection) * 0.000001  # 5% of max projection\n",
    "    threshold_row = np.max(row_projection) * 0.000001  # 5% of max projection\n",
    "    \n",
    "    # Find left boundary (w1)\n",
    "    w1 = 0\n",
    "    while w1 < width - 1 and col_projection[w1] <= threshold_col:\n",
    "        w1 += 1\n",
    "    w1 = max(0, w1 - padding)  # Add padding\n",
    "    \n",
    "    # Find right boundary (w2)\n",
    "    w2 = width - 1\n",
    "    while w2 > 0 and col_projection[w2] <= threshold_col:\n",
    "        w2 -= 1\n",
    "    w2 = min(width - 1, w2 + padding)  # Add padding\n",
    "    \n",
    "    # Ensure we have valid crop dimensions\n",
    "    # if w1 >= w2 or h1 >= h2 or w2 - w1 < 5 or h2 - h1 < 5:\n",
    "    if w1 >= w2 or w2 - w1 < 5:\n",
    "        # If crop would be invalid, return original image\n",
    "        # print(\"Warning: Cropping would result in invalid dimensions. Returning original image.\", w1, w2)\n",
    "        return img\n",
    "    \n",
    "    # Crop the image\n",
    "    final = img[:, w1:w2]\n",
    "    return final\n",
    "\n",
    "def preprocess(img, dataAugmentation=False, bg_color=255):\n",
    "    \"\"\"Preprocess image for the CRNN model\"\"\"\n",
    "    (wt, ht) = (128, 32)\n",
    "    if img is None:\n",
    "        img = (np.zeros((wt, ht, 1))).astype('uint8')\n",
    "    if img.dtype == np.float32 or img.dtype == np.float64:\n",
    "        img = (img * 255).astype(np.uint8)\n",
    "    img = cv2.threshold(img, 0, 1, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1] * 255\n",
    "    \n",
    "    # if dataAugmentation:\n",
    "    #     stretch = (np.random.random() - 0.5)                      # -0.5 .. +0.5\n",
    "    #     wStretched = max(int(img.shape[1] * (1 + stretch)), 1)    # random width, but at least 1\n",
    "    #     img = cv2.resize(img, (wStretched, img.shape[0]))         # stretch horizontally by factor 0.5 .. 1.5\n",
    "    img = closeFit(img)                                           # to avoid lot of white space around text\n",
    "    \n",
    "    h = img.shape[0]\n",
    "    w = img.shape[1]\n",
    "    fx = w / wt\n",
    "    fy = h / ht\n",
    "    f = max(fx, fy)\n",
    "    newSize = (max(min(wt, int(w / f)), 1), max(min(ht, int(h / f)), 1))  # scale according to f\n",
    "    img = cv2.resize(img, newSize, interpolation=cv2.INTER_AREA)          # INTER_AREA important\n",
    "    img = cv2.threshold(img, 0, 1, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1] * 255\n",
    "    \n",
    "    target = np.ones([ht, wt]) * bg_color\n",
    "    target[0:newSize[1], 0:newSize[0]] = img\n",
    "    img = cv2.transpose(target)\n",
    "    (m, s) = cv2.meanStdDev(img)\n",
    "    m = m[0][0]\n",
    "    s = s[0][0]\n",
    "    img = img - m\n",
    "    img = img / s if s > 1e-3 else img\n",
    "    return np.reshape(img, (img.shape[0], img.shape[1], 1))\n",
    "\n",
    "def labelsToText(labels, unicodes):\n",
    "    \"\"\"Convert numerical labels to text using unicode mapping\"\"\"\n",
    "    ret = []\n",
    "    for c in labels:\n",
    "        if c == len(unicodes):\n",
    "            ret.append(\"\")\n",
    "        else:\n",
    "            ret.append(unicodes[c])\n",
    "    return \"\".join(ret)\n",
    "\n",
    "def decode(y_pred, unicodes):\n",
    "    \"\"\"Best Path Decoder for CTC output\"\"\"\n",
    "    texts = []\n",
    "    for y in y_pred:\n",
    "        # CTC decoding - take argmax and remove duplicates\n",
    "        label = list(np.argmax(y[2:], 1))\n",
    "        label = [k for k, g in itertools.groupby(label)]\n",
    "        # Convert label to text\n",
    "        text = labelsToText(label, unicodes)\n",
    "        text = text.replace(\"-\", \"\")\n",
    "        texts.append(text)\n",
    "    return texts\n",
    "\n",
    "\n",
    "def ctcLambdaFunc(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    y_pred = y_pred[:, 2:, :]\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    "\n",
    "# Function to calculate Word Error Rate (WER) and Character Error Rate (CER)\n",
    "def calculate_error_rates(pred_texts, true_texts):\n",
    "    \"\"\"\n",
    "    Calculate Word Error Rate and Character Error Rate\n",
    "    \n",
    "    Args:\n",
    "        pred_texts (list): List of predicted texts\n",
    "        true_texts (list): List of ground truth texts\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (WER, CER) - Word Error Rate and Character Error Rate\n",
    "    \"\"\"\n",
    "    total_wer = 0\n",
    "    total_cer = 0\n",
    "    \n",
    "    for pred, true in zip(pred_texts, true_texts):\n",
    "        # Calculate CER\n",
    "        cer = editdistance.eval(pred, true) / max(len(true), 1)\n",
    "        total_cer += cer\n",
    "        \n",
    "        # Calculate WER - split texts into words\n",
    "        pred_words = pred.split()\n",
    "        true_words = true.split()\n",
    "        wer = editdistance.eval(pred_words, true_words) / max(len(true_words), 1)\n",
    "        total_wer += wer\n",
    "    \n",
    "    # Average over all samples\n",
    "    avg_wer = total_wer / len(pred_texts) if pred_texts else 0\n",
    "    avg_cer = total_cer / len(pred_texts) if pred_texts else 0\n",
    "    \n",
    "    return avg_wer, avg_cer\n",
    "\n",
    "\n",
    "# Alternative approach - modify the CRNN function to handle inference with loss properly\n",
    "def CRNN_fixed(outClasses, training=True):\n",
    "    \"\"\"\n",
    "    Create a CRNN model architecture with proper separation of training and inference\n",
    "    \n",
    "    Args:\n",
    "        outClasses (int): Number of output classes\n",
    "        training (bool): Whether the model is for training (True) or inference (False)\n",
    "        \n",
    "    Returns:\n",
    "        keras.Model: CRNN model\n",
    "    \"\"\"\n",
    "    inputShape = (128, 32, 1)\n",
    "    kernels = [5, 5, 3, 3, 3]\n",
    "    filters = [32, 64, 128, 128, 256]\n",
    "    strides = [(2,2), (2,2), (1,2), (1,2), (1,2)]\n",
    "    rnnUnits = 256\n",
    "    maxStringLen = 32\n",
    "    \n",
    "    # Main input for image data\n",
    "    inputs = Input(name='inputX', shape=inputShape, dtype='float32')\n",
    "    \n",
    "    # CNN layers\n",
    "    inner = inputs\n",
    "    for i in range(len(kernels)):\n",
    "        inner = Conv2D(filters[i], (kernels[i], kernels[i]), padding='same',\n",
    "                      name='conv' + str(i+1), kernel_initializer='glorot_normal')(inner)\n",
    "        inner = BatchNormalization()(inner)\n",
    "        inner = Activation('elu')(inner)\n",
    "        inner = MaxPooling2D(pool_size=strides[i], name='max' + str(i+1))(inner)\n",
    "    \n",
    "    # Reshape for RNN\n",
    "    inner = Reshape(target_shape=(maxStringLen, filters[-1]), name='reshape')(inner)\n",
    "    \n",
    "    # Bidirectional LSTM layers\n",
    "    # First LSTM layer\n",
    "    LSF = LSTM(rnnUnits, return_sequences=True, kernel_initializer='glorot_normal', name='LSTM1F')(inner)\n",
    "    LSB = LSTM(rnnUnits, return_sequences=True, go_backwards=True, kernel_initializer='glorot_normal', name='LSTM1B')(inner)\n",
    "    LSB = Lambda(lambda inputTensor: tf.keras.backend.reverse(inputTensor, axes=[1]), output_shape=(maxStringLen, rnnUnits)) (LSB)\n",
    "    LS1 = Average()([LSF, LSB])\n",
    "    LS1 = BatchNormalization()(LS1)\n",
    "    \n",
    "    # Second LSTM layer\n",
    "    LSF = LSTM(rnnUnits, return_sequences=True, kernel_initializer='glorot_normal', name='LSTM2F')(LS1)\n",
    "    LSB = LSTM(rnnUnits, return_sequences=True, go_backwards=True, kernel_initializer='glorot_normal', name='LSTM2B')(LS1)\n",
    "    LSB = Lambda(lambda inputTensor: tf.keras.backend.reverse(inputTensor, axes=[1]), output_shape=(maxStringLen, rnnUnits)) (LSB)\n",
    "    LS2 = Concatenate()([LSF, LSB])\n",
    "    LS2 = BatchNormalization()(LS2)\n",
    "    \n",
    "    # Output layer\n",
    "    yPred = Dense(outClasses, kernel_initializer='glorot_normal', name='dense2')(LS2)\n",
    "    yPred = Activation('softmax', name='softmax')(yPred)\n",
    "    \n",
    "    # Create an inference model that only takes image input and outputs predictions\n",
    "    inference_model = Model(inputs=inputs, outputs=yPred)\n",
    "    \n",
    "    if not training:\n",
    "        # For inference, just return the prediction model\n",
    "        return inference_model\n",
    "    \n",
    "    # For training, create a separate model that includes the CTC loss\n",
    "    labels = Input(name='label', shape=[maxStringLen], dtype='float32')\n",
    "    inputLength = Input(name='inputLen', shape=[1], dtype='int64')\n",
    "    labelLength = Input(name='labelLen', shape=[1], dtype='int64')\n",
    "    \n",
    "    # Calculate CTC loss\n",
    "    lossOut = Lambda(ctcLambdaFunc, output_shape=(1,), name='ctc')([yPred, labels, inputLength, labelLength])\n",
    "    \n",
    "    # Return the training model\n",
    "    return Model(inputs=[inputs, labels, inputLength, labelLength], outputs=[lossOut])\n",
    "\n",
    "# To use the modified evaluator with the original model:\n",
    "def evaluate_with_ctc_loss(batch_tensor, true_labels, model, unicodes_path):\n",
    "    \"\"\"\n",
    "    Evaluate a batch with the model and calculate CTC loss separately\n",
    "    \n",
    "    Args:\n",
    "        batch_tensor (torch.Tensor): PyTorch tensor of batch images\n",
    "        true_labels (list): List of true text labels\n",
    "        model (tf.keras.Model): CRNN model for inference\n",
    "        unicodes_path (str): Path to unicodes.npy file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (WER, CER, CTC_loss, predictions)\n",
    "    \"\"\"\n",
    "    # Load unicode characters\n",
    "    unicodes = list(np.load(unicodes_path, allow_pickle=True))\n",
    "    \n",
    "    # Convert PyTorch tensor to numpy and preprocess\n",
    "    batch_np = batch_tensor.detach().cpu().numpy()\n",
    "    batch_size = batch_np.shape[0]\n",
    "    processed_batch = np.zeros((batch_size, 128, 32, 1), dtype=np.float32)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        img = batch_np[i]\n",
    "        # Adjust preprocessing as needed for your data\n",
    "        img = 255 - img\n",
    "        processed_img = preprocess(255 - img, dataAugmentation=False, bg_color=0)\n",
    "        processed_batch[i] = 0 - processed_img\n",
    "    \n",
    "    # Get predictions from the inference model\n",
    "    predictions = model.predict(processed_batch, verbose=0)\n",
    "    \n",
    "    # Decode predictions to text\n",
    "    predicted_texts = decode(predictions, unicodes)\n",
    "    \n",
    "    # Calculate WER and CER\n",
    "    wer, cer = calculate_error_rates(predicted_texts, true_labels)\n",
    "    \n",
    "    # Calculate CTC loss manually using TensorFlow\n",
    "    maxStringLen = 32\n",
    "    max_seq_len = 30  # Based on model architecture\n",
    "    \n",
    "    # Prepare label data\n",
    "    label_data = np.zeros((batch_size, maxStringLen))\n",
    "    label_lengths = np.zeros((batch_size, 1), dtype=np.int64)\n",
    "    input_lengths = np.ones((batch_size, 1), dtype=np.int64) * max_seq_len\n",
    "    \n",
    "    # Convert true labels to numerical indices\n",
    "    for i, label in enumerate(true_labels):\n",
    "        for j, char in enumerate(label):\n",
    "            if j < maxStringLen:\n",
    "                if char in unicodes:\n",
    "                    label_data[i, j] = unicodes.index(char)\n",
    "                else:\n",
    "                    label_data[i, j] = len(unicodes)\n",
    "        label_lengths[i, 0] = min(len(label), max_seq_len)\n",
    "    \n",
    "    # Calculate CTC loss using TensorFlow directly\n",
    "    try:\n",
    "        # CTC needs predictions starting from index 2 (skipping blank and unknown)\n",
    "        y_pred_for_ctc = predictions[:, 2:, :]\n",
    "        \n",
    "        # Convert to TensorFlow tensors\n",
    "        tf_y_pred = tf.convert_to_tensor(y_pred_for_ctc, dtype=tf.float32)\n",
    "        tf_labels = tf.convert_to_tensor(label_data, dtype=tf.float32)\n",
    "        tf_input_length = tf.convert_to_tensor(input_lengths, dtype=tf.int64)\n",
    "        tf_label_length = tf.convert_to_tensor(label_lengths, dtype=tf.int64)\n",
    "        \n",
    "        # Calculate CTC loss\n",
    "        ctc_loss = tf.keras.backend.ctc_batch_cost(\n",
    "            tf_labels, tf_y_pred, tf_input_length, tf_label_length\n",
    "        )\n",
    "        \n",
    "        # Get the mean loss value\n",
    "        avg_ctc_loss = float(tf.reduce_mean(ctc_loss))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating CTC loss: {e}\")\n",
    "        avg_ctc_loss = None\n",
    "    \n",
    "    return wer, cer, avg_ctc_loss, predicted_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiVector Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiVectorDecoder(nn.Module):\n",
    "    def __init__(self, encoder_embed_dim=300, max_seq_len=11, num_layers=3, num_heads=6, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder_embed_dim = encoder_embed_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        # Transformer Decoder Layer\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=encoder_embed_dim, \n",
    "            nhead=num_heads, \n",
    "            dim_feedforward=ff_dim, \n",
    "            dropout=dropout,\n",
    "            batch_first=True  # Ensures (batch, seq_len, d_encoder_embed_dimmodel) ordering\n",
    "        )            \n",
    "\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Learnable initial token (or can be random noise)\n",
    "        self.init_tgt_tokens = nn.Parameter(torch.randn(1, max_seq_len, encoder_embed_dim))  # Shape (1, max_seq_len, 300)\n",
    "    \n",
    "    \n",
    "    def forward(self, encoder_output):\n",
    "        batch_size = encoder_output.shape[0]\n",
    "\n",
    "        # Expand the learnable tokens for batch processing\n",
    "        tgt = self.init_tgt_tokens.expand(batch_size, -1, -1)  # Shape: (batch_size, max_seq_len, embed_dim)\n",
    "\n",
    "        # Create a causal mask for autoregressive generation\n",
    "        tgt_mask = torch.triu(torch.ones(self.max_seq_len, self.max_seq_len), diagonal=1).bool().to(encoder_output.device)\n",
    "\n",
    "        # Transformer Decoder\n",
    "        output = self.decoder(tgt, encoder_output, tgt_mask=tgt_mask)\n",
    "\n",
    "        return output  # Shape: (batch_size, max_seq_len, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoderr = MultiVectorDecoder()\n",
    "\n",
    "# encoder_output = torch.randn(30, 1, 300)  # Shape (batch, 1, 300)\n",
    "# tgt = torch.randn(30, 11, 300)            # Shape (batch, 15, 300)\n",
    "\n",
    "# tgt_seq_len = 11\n",
    "# tgt_mask = torch.triu(torch.ones(tgt_seq_len, tgt_seq_len), diagonal=1).bool()\n",
    "# output = decoderr(encoder_output, tgt, tgt_mask)\n",
    "# print(\"Decoder output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## GAN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.spectral_norm as spectral_norm\n",
    "\n",
    "\n",
    "# class ConditionalGANGenerator(nn.Module):\n",
    "#     def __init__(self, latent_dim=300, noise_dim=100, img_size=64, channels=1):\n",
    "#         super().__init__()\n",
    "#         self.input_dim = latent_dim + noise_dim + (channels * img_size * img_size)  # 300 + 100 + 64*64\n",
    "        \n",
    "#         self.fc = nn.Linear(self.input_dim, 1024 * 4 * 4)  # Project to small spatial size\n",
    "        \n",
    "#         self.deconv_layers = nn.Sequential(\n",
    "#             nn.ConvTranspose2d(1024, 512, 4, stride=2, padding=1),  # First transposed convolution\n",
    "#             nn.BatchNorm2d(512),\n",
    "#             nn.LeakyReLU(0.01),\n",
    "        \n",
    "#             nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1),  # Second transposed convolution\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.LeakyReLU(0.01),\n",
    "        \n",
    "#             nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),  # Third transposed convolution\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.LeakyReLU(0.01),\n",
    "        \n",
    "#             nn.ConvTranspose2d(128, channels, 4, stride=2, padding=1),  # New layer for added complexity\n",
    "#             # nn.BatchNorm2d(64),\n",
    "#             # nn.LeakyReLU(0.2),\n",
    "        \n",
    "#             # nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),   # Additional layer\n",
    "#             # nn.BatchNorm2d(32),\n",
    "#             # nn.LeakyReLU(0.2),\n",
    "        \n",
    "#             # nn.ConvTranspose2d(32, channels, 4, stride=2, padding=1),  # Final layer, output image in range [0, 1]\n",
    "#             nn.Sigmoid()  \n",
    "#         )\n",
    "\n",
    "\n",
    "#     def forward(self, latent_vectors, prev_image, noise):\n",
    "#         batch_size, seq_len, _ = latent_vectors.shape  # (batch_size, 11, 300)\n",
    "        \n",
    "#         prev_image = prev_image.view(batch_size, seq_len, -1)  # Flatten image (batch_size, 11, 64*64)\n",
    "#         noise = noise.view(batch_size, seq_len, -1)  # (batch_size, 11, 100)\n",
    "        \n",
    "#         input_tensor = torch.cat([latent_vectors, prev_image, noise], dim=-1)  # (batch_size, 11, 300+100+4096)\n",
    "        \n",
    "#         input_tensor = input_tensor.view(batch_size * seq_len, -1)  # Merge batch & sequence for processing\n",
    "#         x = self.fc(input_tensor)  # (batch_size * seq_len, 1024*4*4)\n",
    "#         x = x.view(batch_size * seq_len, 1024, 4, 4)\n",
    "#         x = self.deconv_layers(x)\n",
    "        \n",
    "#         return x.view(batch_size, seq_len, 1, 64, 64)  # Reshape back to (batch_size, 11, 1, 64, 64)\n",
    "\n",
    "class ConditionalUNetGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim=300, noise_dim=100, img_size=64, channels=1):\n",
    "        super(ConditionalUNetGenerator, self).__init__()\n",
    "        \n",
    "        # Latent vector + noise projection (for each time step)\n",
    "        self.latent_fc = nn.Linear(latent_dim + noise_dim, 64 * 64)\n",
    "        \n",
    "        # Encoder (Downsampling)\n",
    "        self.down1 = nn.Sequential(\n",
    "            nn.Conv2d(2, 64, kernel_size=4, stride=2, padding=1),  # (64 → 32)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.down2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # (32 → 16)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.down3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),  # (16 → 8)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.down4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),  # (8 → 4)\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Decoder (Upsampling)\n",
    "        self.up1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),  # (4 → 8)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.up2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 128, kernel_size=4, stride=2, padding=1),  # (8 → 16)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.up3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 64, kernel_size=4, stride=2, padding=1),  # (16 → 32)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.up4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 1, kernel_size=4, stride=2, padding=1),  # (32 → 64)\n",
    "            nn.Sigmoid()  # Output range [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, latent_vectors, prev_images, noise):\n",
    "        \"\"\"\n",
    "        latent_vectors: (batch, seq_len, 300) -> Projected to (batch, seq_len, 64, 64)\n",
    "        prev_images: (batch, seq_len, 64, 64) -> Used in the encoder\n",
    "        noise: (batch, seq_len, 100) -> Injects randomness\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _, _ = prev_images.squeeze(2).shape  # Expect (batch, 11, 64, 64)\n",
    "\n",
    "        # Combine latent vector and noise\n",
    "        latent_vectors = torch.cat((latent_vectors, noise), dim=-1)  # (batch, seq_len, 400)\n",
    "        latent_vectors = self.latent_fc(latent_vectors)  # (batch, seq_len, 64*64)\n",
    "        latent_vectors = latent_vectors.view(batch_size, seq_len, 64, 64)  # (batch, seq_len, 64, 64)\n",
    "\n",
    "        # Stack previous image with latent projection (as 2-channel input)\n",
    "        # print(latent_vectors.shape, prev_images.shape)\n",
    "        x = torch.cat((latent_vectors.unsqueeze(2), prev_images), dim=2)  # (batch, seq_len, 2, 64, 64)\n",
    "        x = x.view(batch_size * seq_len, 2, 64, 64)  # Flatten seq_len for CNN\n",
    "\n",
    "        # U-Net Encoder\n",
    "        enc1 = self.down1(x)  # (batch*seq_len, 64, 32, 32)\n",
    "        enc2 = self.down2(enc1)  # (batch*seq_len, 128, 16, 16)\n",
    "        enc3 = self.down3(enc2)  # (batch*seq_len, 256, 8, 8)\n",
    "        enc4 = self.down4(enc3)  # (batch*seq_len, 512, 4, 4)\n",
    "\n",
    "        # U-Net Decoder with skip connections\n",
    "        dec1 = self.up1(enc4)  # (batch*seq_len, 256, 8, 8)\n",
    "        dec2 = self.up2(torch.cat((dec1, enc3), dim=1))  # (batch*seq_len, 128, 16, 16)\n",
    "        dec3 = self.up3(torch.cat((dec2, enc2), dim=1))  # (batch*seq_len, 64, 32, 32)\n",
    "        output_images = self.up4(torch.cat((dec3, enc1), dim=1))  # (batch*seq_len, 1, 64, 64)\n",
    "\n",
    "        # Reshape output back to (batch, seq_len, 64, 64)\n",
    "        output_images = output_images.view(batch_size, seq_len, 64, 64)\n",
    "\n",
    "        return output_images\n",
    "        # print(output_images.shape, prev_images.shape)\n",
    "        # return output_images + prev_images.squeeze(2)\n",
    "\n",
    "# class PatchGANDiscriminator(nn.Module):\n",
    "#     def __init__(self, in_channels=1, ndf=64):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.model = nn.Sequential(\n",
    "#             spectral_norm(nn.Conv2d(in_channels, ndf, kernel_size=4, stride=2, padding=1)),\n",
    "#             nn.LeakyReLU(0.01, inplace=True),\n",
    "\n",
    "#             spectral_norm(nn.Conv2d(ndf, ndf * 2, kernel_size=4, stride=2, padding=1)),\n",
    "#             # nn.BatchNorm2d(ndf * 2),\n",
    "#             nn.LeakyReLU(0.01, inplace=True),\n",
    "\n",
    "#             spectral_norm(nn.Conv2d(ndf * 2, ndf * 4, kernel_size=4, stride=2, padding=1)),\n",
    "#             nn.BatchNorm2d(ndf * 4),\n",
    "#             nn.LeakyReLU(0.01, inplace=True),\n",
    "\n",
    "#             spectral_norm(nn.Conv2d(ndf * 4, ndf * 8, kernel_size=4, stride=2, padding=1)),\n",
    "#             nn.BatchNorm2d(ndf * 8),\n",
    "#             nn.LeakyReLU(0.01, inplace=True),\n",
    "#             nn.Dropout(0.3),\n",
    "\n",
    "#             spectral_norm(nn.Conv2d(ndf * 8, 1, kernel_size=3, stride=1, padding=1))\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         batch_size, seq_len, _, h, w = x.shape  # Already (B, 11, 1, 64, 64)\n",
    "#         x = x.view(batch_size * seq_len, 1, h, w)  # Flatten sequence\n",
    "#         x = self.model(x)  # (B * 11, 1, 4, 4)\n",
    "#         x = x.view(batch_size, seq_len, 4, 4)  # Reshape back\n",
    "#         return x\n",
    "\n",
    "\n",
    "\n",
    "class ConditionalGANDiscriminator(nn.Module):\n",
    "    def __init__(self, img_size=64, channels=1):\n",
    "        super().__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(channels, 64, 4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.15),\n",
    "            nn.Dropout(0.15),\n",
    "\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.15),\n",
    "            nn.Dropout(0.15),\n",
    "\n",
    "            nn.Conv2d(128, 256, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.15),\n",
    "            nn.Dropout(0.15),\n",
    "\n",
    "            nn.Conv2d(256, 512, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.15),\n",
    "            nn.Dropout(0.15),\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(512 * 4 * 4, 1)  # Fully connected layer for final output\n",
    "\n",
    "    def forward(self, image):\n",
    "        batch_size, seq_len, channels, height, width = image.shape\n",
    "        image = image.view(batch_size * seq_len, channels, height, width)  # Merge batch & sequence for processing\n",
    "        x = self.conv_layers(image)\n",
    "        x = x.view(batch_size * seq_len, -1)\n",
    "        x = self.fc(x)\n",
    "        return x.view(batch_size, seq_len)  # Output shape (batch_size, seq_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Embedding Hyperparameters\n",
    "unique_tokens = ['0905', '0905_0902', '0905_0903', '0906', '0907', '0908', '0909', '090A', '090F', '0910', '0913', '0914', '0915', '0915_093E', '0915_093F', '0915_0940', '0915_0941', '0915_0942', '0915_0947', '0915_0948', '0915_094B', '0915_094C', '0915_094D', '0915_094D_0937', '0915_094D_0937_0903', '0915_094D_0937_093E', '0915_094D_0937_0940', '0915_094D_0937_0941', '0915_094D_0937_0942', '0915_094D_0937_0947', '0915_094D_0937_0948', '0915_094D_0937_094B', '0915_094D_0937_094C', '0916', '0916_093E', '0916_093F', '0916_0941', '0916_0942', '0916_0948', '0916_094B', '0916_094C', '0916_094D', '0917', '0917_093E', '0917_093F', '0917_0940', '0917_0941', '0917_0942', '0917_0947', '0917_0948', '0917_094B', '0917_094C', '0917_094D', '0918', '0918_093E', '0918_093F', '0918_0940', '0918_0941', '0918_0942', '0918_0947', '0918_0948', '0918_094B', '0918_094C', '0918_094D', '0919', '0919_0902', '0919_0903', '0919_093E', '0919_093F', '0919_0940', '0919_0941', '0919_0942', '0919_0947', '0919_0948', '0919_094B', '0919_094C', '091A', '091A_0902', '091A_0903', '091A_093E', '091A_093F', '091A_0940', '091A_0941', '091A_0942', '091A_0947', '091A_0948', '091A_094B', '091A_094C', '091B', '091B_0902', '091B_0903', '091B_093E', '091B_093F', '091B_0940', '091B_0941', '091B_0942', '091B_0947', '091B_0948', '091B_094B', '091B_094C', '091C', '091C_0902', '091C_0903', '091C_093E', '091C_093F', '091C_0940', '091C_0941', '091C_0942', '091C_0947', '091C_0948', '091C_094B', '091C_094C', '091C_094D_091E', '091C_094D_091E_0902', '091C_094D_091E_0903', '091C_094D_091E_093E', '091C_094D_091E_093F', '091C_094D_091E_0940', '091C_094D_091E_0941', '091C_094D_091E_0942', '091C_094D_091E_0947', '091C_094D_091E_0948', '091C_094D_091E_094B', '091C_094D_091E_094C', '091D', '091D_0902', '091D_0903', '091D_093E', '091D_093F', '091D_0940', '091D_0941', '091D_0942', '091D_0947', '091D_0948', '091D_094B', '091D_094C', '091E', '091E_0902', '091E_0903', '091E_093E', '091E_093F', '091E_0940', '091E_0941', '091E_0942', '091E_0947', '091E_0948', '091E_094B', '091E_094C', '091F', '091F_0903', '091F_093E', '091F_093F', '091F_0940', '091F_0941', '091F_0942', '091F_0947', '091F_0948', '091F_094B', '091F_094C', '0920', '0920_0903', '0920_093E', '0920_093F', '0920_0940', '0920_0941', '0920_0942', '0920_0947', '0920_0948', '0920_094B', '0920_094C', '0921', '0921_0902', '0921_0903', '0921_093E', '0921_093F', '0921_0940', '0921_0941', '0921_0942', '0921_0947', '0921_0948', '0921_094B', '0921_094C', '0922', '0922_0902', '0922_0903', '0922_093E', '0922_093F', '0922_0940', '0922_0941', '0922_0942', '0922_0947', '0922_0948', '0922_094B', '0922_094C', '0923', '0923_0902', '0923_0903', '0923_093E', '0923_093F', '0923_0940', '0923_0941', '0923_0942', '0923_0947', '0923_0948', '0923_094B', '0923_094C', '0924', '0924_0902', '0924_0903', '0924_093E', '0924_093F', '0924_0940', '0924_0941', '0924_0942', '0924_0947', '0924_0948', '0924_094B', '0924_094C', '0924_094D_0930', '0924_094D_0930_0902', '0924_094D_0930_0903', '0924_094D_0930_093E', '0924_094D_0930_093F', '0924_094D_0930_0940', '0924_094D_0930_0941', '0924_094D_0930_0942', '0924_094D_0930_0947', '0924_094D_0930_0948', '0924_094D_0930_094B', '0924_094D_0930_094C', '0925', '0925_0902', '0925_093E', '0925_093F', '0925_0940', '0925_0941', '0925_0942', '0925_0947', '0925_0948', '0925_094B', '0925_094C', '0926', '0926_0902', '0926_0902_0903', '0926_0903', '0926_093E', '0926_093F', '0926_0940', '0926_0941', '0926_0942', '0926_0947', '0926_0948', '0926_094B', '0926_094C', '0927', '0927_0902', '0927_0903', '0927_093E', '0927_093F', '0927_0940', '0927_0941', '0927_0942', '0927_0947', '0927_0948', '0927_094B', '0927_094C', '0928', '0928_0903', '0928_093E', '0928_093F', '0928_0940', '0928_0941', '0928_0942', '0928_0947', '0928_0948', '0928_094B', '0928_094C', '092A', '092A_0902', '092A_0903', '092A_093E', '092A_093F', '092A_0940', '092A_0941', '092A_0942', '092A_0947', '092A_0948', '092A_094B', '092A_094C', '092B', '092B_0902', '092B_0903', '092B_093E', '092B_093F', '092B_0940', '092B_0941', '092B_0942', '092B_0947', '092B_0948', '092B_094B', '092B_094C', '092C', '092C_0902', '092C_0903', '092C_093E', '092C_093F', '092C_0940', '092C_0941', '092C_0942', '092C_0947', '092C_0948', '092C_094B', '092C_094C', '092D', '092D_0902', '092D_0903', '092D_093E', '092D_093F', '092D_0940', '092D_0941', '092D_0942', '092D_0947', '092D_0948', '092D_094B', '092D_094C', '092E', '092E_0902', '092E_0903', '092E_093E', '092E_093F', '092E_0940', '092E_0941', '092E_0942', '092E_0947', '092E_0948', '092E_0948_0902', '092E_094B', '092E_094C', '092F', '092F_0902', '092F_0903', '092F_093E', '092F_093F', '092F_0940', '092F_0941', '092F_0942', '092F_0947', '092F_0948', '092F_094B', '092F_094C', '0930', '0930_0902', '0930_0903', '0930_093E', '0930_093F', '0930_0940', '0930_0941', '0930_0942', '0930_0947', '0930_0948', '0930_094B', '0930_094C', '0932', '0932_0902', '0932_0903', '0932_093E', '0932_093F', '0932_0940', '0932_0941', '0932_0942', '0932_0947', '0932_0948', '0932_094B', '0932_094C', '0935', '0935_0902', '0935_0903', '0935_093E', '0935_093F', '0935_0940', '0935_0941', '0935_0942', '0935_0947', '0935_0948', '0935_094B', '0935_094C', '0936', '0936_0902', '0936_0903', '0936_093E', '0936_093F', '0936_0940', '0936_0941', '0936_0942', '0936_0947', '0936_0948', '0936_094B', '0936_094C', '0937', '0937_0902', '0937_0903', '0937_093E', '0937_093F', '0937_0940', '0937_0941', '0937_0942', '0937_0947', '0937_0948', '0937_094B', '0937_094C', '0938', '0938_0902', '0938_0903', '0938_093E', '0938_093F', '0938_0940', '0938_0941', '0938_0942', '0938_0947', '0938_0948', '0938_094B', '0938_094C', '0939', '0939_0902', '0939_0903', '0939_093E', '0939_093F', '0939_0940', '0939_0941', '0939_0942', '0939_0947', '0939_0948', '0939_094B', '0939_094C', '0966', '0967', '0968', '0969', '096A', '096B', '096C', '096D', '096E', '096F']\n",
    "unique_tokens = ['<PAD>', '<UNK>'] + unique_tokens\n",
    "token_id_map = {token: i for i, token in enumerate(unique_tokens)}\n",
    "\n",
    "vocab_size = len(unique_tokens)\n",
    "\n",
    "\n",
    "# Transformer Decoder Hyperparameters \n",
    "encoder_embed_dim=300\n",
    "num_layers=6\n",
    "num_heads=10\n",
    "ff_dim=256\n",
    "dropout=0.1\n",
    "\n",
    "latent_vector_dim=300\n",
    "max_seq_len=11\n",
    "\n",
    "# GANs Hyperparameters\n",
    "noise_dim=100\n",
    "img_size=64\n",
    "\n",
    "# Recognizer Parameters\n",
    "weights_path = \"crnn_weights_exp2.h5\"  # Replace with your weights file path\n",
    "unicodes_path = \"unicodes.npy\"  # Replace with your unicodes file path\n",
    "unicodes = list(np.load(unicodes_path, allow_pickle=True))\n",
    "\n",
    "# Model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "try:\n",
    "    # del encoder\n",
    "    del decoder\n",
    "    del generator\n",
    "    del discriminator\n",
    "    del recognizer\n",
    "except: pass\n",
    "encoder = FastTextEncoder().to(device)\n",
    "decoder = MultiVectorDecoder(encoder_embed_dim=encoder_embed_dim, max_seq_len=max_seq_len, num_layers=num_layers,\n",
    "                             num_heads=num_heads, ff_dim=ff_dim, dropout=dropout).to(device)\n",
    "generator = ConditionalUNetGenerator(latent_dim=latent_vector_dim, noise_dim=noise_dim, img_size=img_size).to(device)\n",
    "discriminator = ConditionalGANDiscriminator(img_size=img_size).to(device)\n",
    "# discriminator = PatchGANDiscriminator().to(device)\n",
    "recognizer = CRNN_fixed(len(unicodes) + 1, training=False)\n",
    "recognizer.load_weights(weights_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# encoder = nn.DataParallel(encoder)\n",
    "# decoder = nn.DataParallel(decoder)\n",
    "# generator = nn.DataParallel(generator)\n",
    "# discriminator = nn.DataParallel(discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Count Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Params:\n",
      "Total Parameters: 0\n",
      "Trainable Parameters: 0\n",
      "\n",
      "Decoder Params:\n",
      "Total Parameters: 5273436\n",
      "Trainable Parameters: 5273436\n",
      "\n",
      "Generator Params:\n",
      "Total Parameters: 7811201\n",
      "Trainable Parameters: 7811201\n",
      "\n",
      "Discriminator Params:\n",
      "Total Parameters: 2764481\n",
      "Trainable Parameters: 2764481\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total Parameters: {total_params}\")\n",
    "    print(f\"Trainable Parameters: {trainable_params}\")\n",
    "\n",
    "print(f\"Encoder Params:\")\n",
    "count_parameters(encoder)\n",
    "print(f\"\\nDecoder Params:\")\n",
    "count_parameters(decoder)\n",
    "print(f\"\\nGenerator Params:\")\n",
    "count_parameters(generator)\n",
    "print(f\"\\nDiscriminator Params:\")\n",
    "count_parameters(discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# Loss functions\n",
    "bce_loss = nn.BCEWithLogitsLoss()  # Binary Cross-Entropy Loss for image reconstruction\n",
    "mse_loss = nn.MSELoss()  # Mean Squared Error Loss for latent count prediction\n",
    "\n",
    "# Optimizer\n",
    "g_opt = optim.Adam(generator.parameters(), lr=1e-2, betas=(0.5, 0.999))\n",
    "d_opt = optim.Adam(discriminator.parameters(), lr=2e-5, betas=(0.5, 0.999))\n",
    "\n",
    "## Load model states\n",
    "# checkpoint = torch.load(\"c_disc_unet_gen_checkpoint.pth\")\n",
    "# decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "# generator.load_state_dict(checkpoint['gen_state_dict'])\n",
    "# discriminator.load_state_dict(checkpoint['disc_state_dict'])\n",
    "# g_opt.load_state_dict(checkpoint['gen_optimizer_state_dict'])\n",
    "# d_opt.load_state_dict(checkpoint['disc_optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG Perceptual Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGPerceptualLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGPerceptualLoss, self).__init__()\n",
    "        vgg = models.vgg16(pretrained=True).features\n",
    "        self.vgg_layers = nn.Sequential(*list(vgg.children())[:24])  # Use layers up to relu3_3\n",
    "        self.vgg_layers.to(device).eval()\n",
    "        for param in self.vgg_layers.parameters():\n",
    "            param.requires_grad = False  # Freeze VGG\n",
    "\n",
    "    def forward(self, real_images, fake_images):\n",
    "        # Expecting shape: (B, 11, 1, H, W)\n",
    "        batch_size, seq_len, _, h, w = real_images.shape\n",
    "\n",
    "        # Flatten sequence dimension: (B, 11, 1, H, W) → (B * 11, 1, H, W)\n",
    "        real_images = real_images.view(batch_size * seq_len, 1, h, w)\n",
    "        fake_images = fake_images.view(batch_size * seq_len, 1, h, w)\n",
    "\n",
    "        # Convert grayscale (1-channel) to RGB (3-channel)\n",
    "        real_images_rgb = real_images.repeat(1, 3, 1, 1)\n",
    "        fake_images_rgb = fake_images.repeat(1, 3, 1, 1)\n",
    "\n",
    "        # Normalize inputs (assumes values in range [0, 1])\n",
    "        real_images_rgb = (real_images_rgb - 0.5) * 2  # Scale to [-1, 1]\n",
    "        fake_images_rgb = (fake_images_rgb - 0.5) * 2\n",
    "\n",
    "        # Extract VGG features\n",
    "        real_features = self.vgg_layers(real_images_rgb)\n",
    "        fake_features = self.vgg_layers(fake_images_rgb)\n",
    "\n",
    "        # Compute L1 loss on feature maps\n",
    "        loss = F.l1_loss(real_features, fake_features)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 528M/528M [00:05<00:00, 99.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "perceptual_loss_fn = VGGPerceptualLoss()\n",
    "\n",
    "def ms_ssim_loss(real_images, fake_images):\n",
    "    return 1 - pytorch_msssim.ssim(real_images, fake_images, data_range=1.0)\n",
    "\n",
    "def gradient_balance_two_losses(generator, loss_1, loss_2, alpha=1.0):\n",
    "    # Compute gradients for first loss\n",
    "    loss_1.backward(retain_graph=True)\n",
    "    \n",
    "    # Store gradients from first loss\n",
    "    grads_1 = {name: param.grad.clone() for name, param in generator.named_parameters() if param.grad is not None}\n",
    "    \n",
    "    # Zero out gradients\n",
    "    g_opt.zero_grad()\n",
    "    \n",
    "    # Compute gradients for second loss\n",
    "    loss_2.backward()\n",
    "    \n",
    "    # Collect standard deviations of gradients\n",
    "    std_grads = {\n",
    "        '1': sum(g.std().item() for g in grads_1.values()) / len(grads_1),\n",
    "        '2': sum(param.grad.std().item() for name, param in generator.named_parameters() \n",
    "                if param.grad is not None and name in grads_1) / len(grads_1)\n",
    "    }\n",
    "    \n",
    "    # Apply gradient balancing: ∇IR ← α * (σ(∇ID)/σ(∇IR)) * ∇IR\n",
    "    for name, param in generator.named_parameters():\n",
    "        if param.grad is not None and name in grads_1 and std_grads['1'] > 0:\n",
    "            scaling_factor = alpha * (std_grads['2'] / std_grads['1'])\n",
    "            param.grad = param.grad + scaling_factor * grads_1[name]\n",
    "\n",
    "def train_step(chunks, labels, latent_vectors, context_images, noise, batch_idx):\n",
    "    chunks = chunks.to(device)\n",
    "    latent_vectors = latent_vectors.to(device)\n",
    "    context_images = context_images.to(device)\n",
    "    noise = noise.to(device)\n",
    "    \n",
    "    batch_size = chunks.shape[0]\n",
    "    # real_labels = torch.ones(batch_size, 11, 4, 4, device=device)\n",
    "    # fake_labels = torch.zeros(batch_size, 11, 4, 4, device=device)\n",
    "\n",
    "    \n",
    "    real_labels = torch.ones(batch_size, 11, 1, device=device)\n",
    "    fake_labels = torch.zeros(batch_size, 11, 1, device=device)\n",
    "\n",
    "    \n",
    "    # Train Discriminator\n",
    "    # print(\"training disc...\")\n",
    "    # print(\"training disc on real\")\n",
    "    d_opt.zero_grad()\n",
    "    real_preds = discriminator(chunks)\n",
    "    # print(real_preds.shape, real_labels.shape)\n",
    "    # real_loss = bce_loss(real_preds, real_labels)\n",
    "    real_loss = bce_loss(real_preds, real_labels.squeeze(-1))\n",
    "\n",
    "    # print(\"training disc on fake\")\n",
    "    fake_images = generator(latent_vectors, context_images, noise)\n",
    "    fake_preds = discriminator(fake_images.unsqueeze(2).detach())\n",
    "    # fake_loss = bce_loss(fake_preds, fake_labels)\n",
    "    fake_loss = bce_loss(fake_preds, fake_labels.squeeze(-1))\n",
    "    \n",
    "    d_loss = real_loss + fake_loss\n",
    "    d_loss.backward()\n",
    "    d_opt.step()\n",
    "\n",
    "    # Train Generator\n",
    "    # print(\"training gen...\")\n",
    "    g_opt.zero_grad()\n",
    "    fake_preds = discriminator(fake_images.unsqueeze(2))\n",
    "    # adv_loss = bce_loss(fake_preds, real_labels)\n",
    "    adv_loss = bce_loss(fake_preds, real_labels.squeeze(-1))\n",
    "    l1_loss = F.l1_loss(fake_images, chunks.squeeze(2))\n",
    "    perc_loss = perceptual_loss_fn(chunks, fake_images)\n",
    "    ms_ssim = ms_ssim_loss(chunks.squeeze(2), fake_images)\n",
    "\n",
    "    tensor_hstacked = torch.cat([fake_images[:, i] for i in range(11)], dim=2)\n",
    "    wer, cer, ctc_loss, predictions = evaluate_with_ctc_loss(tensor_hstacked, labels, recognizer, unicodes_path)\n",
    "    # print(ctc_loss)\n",
    "\n",
    "    # weighted sum of losses\n",
    "    lambda_adv, lambda_l1, lambda_perc, lambda_ssim = 1.0, 4.0, 6.0, 6.0\n",
    "    g_loss = (\n",
    "        lambda_adv * adv_loss\n",
    "        # + lambda_l1 * l1_loss\n",
    "        # + lambda_perc * perc_loss\n",
    "        # + lambda_ssim * ms_ssim\n",
    "        # + 0.3 * ctc_loss\n",
    "    )\n",
    "    # g_loss = l1_loss + ctc_loss\n",
    "    g_loss.backward()\n",
    "\n",
    "    # gradient_balance_two_losses(generator, l1_loss, ctc_loss, alpha=1.0)\n",
    "\n",
    "    # 🔍 Check if decoder is receiving gradients    \n",
    "    # for name, param in decoder.module.decoder.named_parameters():\n",
    "    #     if param.grad is not None:\n",
    "    #         print(f\"{name} has gradients: {param.grad.norm().item()}\")\n",
    "    #     else:\n",
    "    #         print(f\"{name} has NO gradients!\")\n",
    "    g_opt.step()\n",
    "\n",
    "\n",
    "    # 🔍 Plot images every 10 batches\n",
    "    if batch_idx % 50 == 0 and batch_idx > 0:\n",
    "        plot_generated_vs_real(chunks, fake_images, batch_idx)\n",
    "\n",
    "    del real_preds, fake_preds, fake_images, real_loss, fake_loss\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return d_loss.item(), g_loss.item()\n",
    "\n",
    "\n",
    "def plot_generated_vs_real(real_images, fake_images, batch_idx):\n",
    "    \"\"\"\n",
    "    Plots a random sample of real and generated images concatenated horizontally,\n",
    "    and then stacks these pairs vertically.\n",
    "    \"\"\"\n",
    "    real_images = real_images.squeeze(2).cpu().detach()  # Remove the extra channel dim\n",
    "    fake_images = fake_images.squeeze(2).cpu().detach()\n",
    "\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(10, 10))  # 5 pairs of images (vertical stack)\n",
    "\n",
    "    for i in range(1):  # Show 5 samples\n",
    "        idx = random.randint(0, real_images.shape[0] - 1)\n",
    "\n",
    "        real_concat = torch.cat([real_images[idx, j] for j in range(real_images.shape[1])], dim=1)\n",
    "        fake_concat = torch.cat([fake_images[idx, j] for j in range(fake_images.shape[1])], dim=1)\n",
    "\n",
    "        stacked = torch.cat([real_concat, fake_concat], dim=0)  # Stack vertically\n",
    "\n",
    "        # if(batch_idx%1000 == 0 and batch_idx > 0):\n",
    "        #     axes.imshow(stacked.numpy(), cmap=\"gray\")\n",
    "\n",
    "        # Save with timestamp\n",
    "        filename = f\"generated_{batch_idx}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png\"\n",
    "        plt.imsave(f\"./Results_Architecture_1/{filename}\", stacked.numpy(), cmap=\"gray\")\n",
    "        plt.close(fig)\n",
    "        # axes.axis(\"off\")\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# plt.imshow(torch.cat([img for img in context_images[0]], dim=1).numpy())\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# print(chunks.shape, context_images.shape) -> torch.Size([512, 11, 64, 64]) torch.Size([512, 11, 64, 64])\u001b[39;00m\n\u001b[1;32m     42\u001b[0m noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(chunks\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], max_seq_len, \u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m---> 43\u001b[0m d_loss, g_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwords\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_images\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m total_d_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m d_loss\n\u001b[1;32m     46\u001b[0m total_g_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m g_loss\n",
      "Cell \u001b[0;32mIn[28], line 77\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(chunks, labels, latent_vectors, context_images, noise, batch_idx)\u001b[0m\n\u001b[1;32m     74\u001b[0m ms_ssim \u001b[38;5;241m=\u001b[39m ms_ssim_loss(chunks\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m2\u001b[39m), fake_images)\n\u001b[1;32m     76\u001b[0m tensor_hstacked \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([fake_images[:, i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m11\u001b[39m)], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 77\u001b[0m wer, cer, ctc_loss, predictions \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_with_ctc_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_hstacked\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecognizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43municodes_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# print(ctc_loss)\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# weighted sum of losses\u001b[39;00m\n\u001b[1;32m     81\u001b[0m lambda_adv, lambda_l1, lambda_perc, lambda_ssim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m4.0\u001b[39m, \u001b[38;5;241m6.0\u001b[39m, \u001b[38;5;241m6.0\u001b[39m\n",
      "Cell \u001b[0;32mIn[20], line 237\u001b[0m, in \u001b[0;36mevaluate_with_ctc_loss\u001b[0;34m(batch_tensor, true_labels, model, unicodes_path)\u001b[0m\n\u001b[1;32m    234\u001b[0m     processed_batch[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m-\u001b[39m processed_img\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# Get predictions from the inference model\u001b[39;00m\n\u001b[0;32m--> 237\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# Decode predictions to text\u001b[39;00m\n\u001b[1;32m    240\u001b[0m predicted_texts \u001b[38;5;241m=\u001b[39m decode(predictions, unicodes)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:499\u001b[0m, in \u001b[0;36mTensorFlowTrainer.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;129m@traceback_utils\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_traceback\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;28mself\u001b[39m, x, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    497\u001b[0m ):\n\u001b[1;32m    498\u001b[0m     \u001b[38;5;66;03m# Create an iterator that yields batches of input data.\u001b[39;00m\n\u001b[0;32m--> 499\u001b[0m     epoch_iterator \u001b[38;5;241m=\u001b[39m \u001b[43mTFEpochIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdistribute_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;66;03m# Container that configures and calls callbacks.\u001b[39;00m\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:720\u001b[0m, in \u001b[0;36mTFEpochIterator.__init__\u001b[0;34m(self, distribute_strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribute_strategy \u001b[38;5;241m=\u001b[39m distribute_strategy\n\u001b[0;32m--> 720\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tf_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedDataset):\n\u001b[1;32m    722\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribute_strategy\u001b[38;5;241m.\u001b[39mexperimental_distribute_dataset(\n\u001b[1;32m    723\u001b[0m         dataset\n\u001b[1;32m    724\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/trainers/data_adapters/array_data_adapter.py:140\u001b[0m, in \u001b[0;36mArrayDataAdapter.get_tf_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m indices\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# We prefetch a single element. Computing large permutations can take\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# quite a while so we don't want to wait for prefetching over an epoch\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# boundary to trigger the next permutation. On the other hand, too many\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# simultaneous shuffles can contend on a hardware level and degrade all\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# performance.\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m indices_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mindices_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpermutation\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mprefetch(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mslice_batch_indices\u001b[39m(indices):\n\u001b[1;32m    143\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert a Tensor of indices into a dataset of batched indices.\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m    This step can be accomplished in several ways. The most natural is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m        A Dataset of batched indices.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/data/ops/dataset_ops.py:2341\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, synchronous, use_unbounded_threadpool, name)\u001b[0m\n\u001b[1;32m   2336\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops -> map_op ->\u001b[39;00m\n\u001b[1;32m   2337\u001b[0m \u001b[38;5;66;03m# dataset_ops).\u001b[39;00m\n\u001b[1;32m   2338\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[1;32m   2339\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m map_op\n\u001b[0;32m-> 2341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmap_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_v2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2342\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2346\u001b[0m \u001b[43m    \u001b[49m\u001b[43msynchronous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynchronous\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2347\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_unbounded_threadpool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_unbounded_threadpool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2349\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/data/ops/map_op.py:43\u001b[0m, in \u001b[0;36m_map_v2\u001b[0;34m(input_dataset, map_func, num_parallel_calls, deterministic, synchronous, use_unbounded_threadpool, name)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m deterministic \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m debug_mode\u001b[38;5;241m.\u001b[39mDEBUG_MODE:\n\u001b[1;32m     39\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `deterministic` argument has no effect unless the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`num_parallel_calls` argument is specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     42\u001b[0m     )\n\u001b[0;32m---> 43\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MapDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m      \u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpreserve_cardinality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m      \u001b[49m\u001b[43mforce_synchronous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msynchronous\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msynchronous\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m synchronous:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/data/ops/map_op.py:157\u001b[0m, in \u001b[0;36m_MapDataset.__init__\u001b[0;34m(self, input_dataset, map_func, force_synchronous, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_inter_op_parallelism \u001b[38;5;241m=\u001b[39m use_inter_op_parallelism\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preserve_cardinality \u001b[38;5;241m=\u001b[39m preserve_cardinality\n\u001b[0;32m--> 157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func \u001b[38;5;241m=\u001b[39m \u001b[43mstructured_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStructuredFunctionWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transformation_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_legacy_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_legacy_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_force_synchronous \u001b[38;5;241m=\u001b[39m force_synchronous\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m name\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/data/ops/structured_function.py:265\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m       warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    259\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    260\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    261\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    262\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    263\u001b[0m     fn_factory \u001b[38;5;241m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[0;32m--> 265\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function \u001b[38;5;241m=\u001b[39m \u001b[43mfn_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[1;32m    267\u001b[0m add_to_graph \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:1256\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_concrete_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1255\u001b[0m   \u001b[38;5;66;03m# Implements PolymorphicFunction.get_concrete_function.\u001b[39;00m\n\u001b[0;32m-> 1256\u001b[0m   concrete \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_concrete_function_garbage_collected\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1257\u001b[0m   concrete\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1258\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m concrete\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:1226\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1224\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1225\u001b[0m     initializers \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1226\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_initializers_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_uninitialized_variables(initializers)\n\u001b[1;32m   1229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m   1230\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m   1231\u001b[0m   \u001b[38;5;66;03m# version which is guaranteed to never create variables.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:696\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_scoped_tracing_options(\n\u001b[1;32m    692\u001b[0m     variable_capturing_scope,\n\u001b[1;32m    693\u001b[0m     tracing_compilation\u001b[38;5;241m.\u001b[39mScopeType\u001b[38;5;241m.\u001b[39mVARIABLE_CREATION,\n\u001b[1;32m    694\u001b[0m )\n\u001b[1;32m    695\u001b[0m \u001b[38;5;66;03m# Force the definition of the function for these arguments\u001b[39;00m\n\u001b[0;32m--> 696\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvalid_creator_scope\u001b[39m(\u001b[38;5;241m*\u001b[39munused_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwds):\n\u001b[1;32m    701\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:178\u001b[0m, in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    175\u001b[0m     args \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39minput_signature\n\u001b[1;32m    176\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 178\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mbind_graph_to_function:\n\u001b[1;32m    183\u001b[0m   concrete_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:283\u001b[0m, in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m   target_func_type \u001b[38;5;241m=\u001b[39m lookup_func_type\n\u001b[0;32m--> 283\u001b[0m concrete_function \u001b[38;5;241m=\u001b[39m \u001b[43m_create_concrete_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_func_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookup_func_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m   tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache\u001b[38;5;241m.\u001b[39madd(\n\u001b[1;32m    289\u001b[0m       concrete_function, current_func_context\n\u001b[1;32m    290\u001b[0m   )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:310\u001b[0m, in \u001b[0;36m_create_concrete_function\u001b[0;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[1;32m    303\u001b[0m   placeholder_bound_args \u001b[38;5;241m=\u001b[39m function_type\u001b[38;5;241m.\u001b[39mplaceholder_arguments(\n\u001b[1;32m    304\u001b[0m       placeholder_context\n\u001b[1;32m    305\u001b[0m   )\n\u001b[1;32m    307\u001b[0m disable_acd \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39mattributes \u001b[38;5;129;01mand\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mattributes\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    308\u001b[0m     attributes_lib\u001b[38;5;241m.\u001b[39mDISABLE_ACD, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    309\u001b[0m )\n\u001b[0;32m--> 310\u001b[0m traced_func_graph \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_control_dependencies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_acd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction_type_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_arg_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_placeholders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m transform\u001b[38;5;241m.\u001b[39mapply_func_graph_transforms(traced_func_graph)\n\u001b[1;32m    324\u001b[0m graph_capture_container \u001b[38;5;241m=\u001b[39m traced_func_graph\u001b[38;5;241m.\u001b[39mfunction_captures\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/framework/func_graph.py:1064\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[1;32m   1060\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m python_func(\u001b[38;5;241m*\u001b[39mfunc_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfunc_kwargs)\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m-> 1064\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mvariable_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_variables_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1065\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m   1066\u001b[0m     convert, func_outputs, expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;66;03m# flatten and unflatten func_args and func_kwargs to maintain parity\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;66;03m# from flattening which sorts by key\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/util/variable_utils.py:45\u001b[0m, in \u001b[0;36mconvert_variables_to_tensors\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_convert_resource_variable_to_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/util/nest.py:628\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnest.map_structure\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmap_structure\u001b[39m(func, \u001b[38;5;241m*\u001b[39mstructure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    544\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new structure by applying `func` to each atom in `structure`.\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \n\u001b[1;32m    546\u001b[0m \u001b[38;5;124;03m  Refer to [tf.nest](https://www.tensorflow.org/api_docs/python/tf/nest)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;124;03m    ValueError: If wrong keyword arguments are provided.\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnest_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnest_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModality\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCORE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstructure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py:1065\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(modality, func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new structure by applying `func` to each atom in `structure`.\u001b[39;00m\n\u001b[1;32m    969\u001b[0m \n\u001b[1;32m    970\u001b[0m \u001b[38;5;124;03m- For Modality.CORE: Refer to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;124;03m  ValueError: If wrong keyword arguments are provided.\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m modality \u001b[38;5;241m==\u001b[39m Modality\u001b[38;5;241m.\u001b[39mCORE:\n\u001b[0;32m-> 1065\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_tf_core_map_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstructure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m modality \u001b[38;5;241m==\u001b[39m Modality\u001b[38;5;241m.\u001b[39mDATA:\n\u001b[1;32m   1067\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _tf_data_map_structure(func, \u001b[38;5;241m*\u001b[39mstructure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py:1103\u001b[0m, in \u001b[0;36m_tf_core_map_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (_tf_core_flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m   1101\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m-> 1103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_tf_core_pack_sequence_as\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstructure\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mentries\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpand_composites\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpand_composites\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py:919\u001b[0m, in \u001b[0;36m_tf_core_pack_sequence_as\u001b[0;34m(structure, flat_sequence, expand_composites, sequence_fn)\u001b[0m\n\u001b[1;32m    912\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(flat_structure) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(flat_sequence):\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;66;03m# pylint: disable=raise-missing-from\u001b[39;00m\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    915\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not pack sequence. Structure had \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m atoms, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    916\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflat_sequence had \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m items.  Structure: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, flat_sequence: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    917\u001b[0m         \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mlen\u001b[39m(flat_structure), \u001b[38;5;28mlen\u001b[39m(flat_sequence), structure, flat_sequence)\n\u001b[1;32m    918\u001b[0m     )\n\u001b[0;32m--> 919\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msequence_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstructure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpacked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py:244\u001b[0m, in \u001b[0;36msequence_like\u001b[0;34m(instance, args)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(instance, _wrapt\u001b[38;5;241m.\u001b[39mObjectProxy):\n\u001b[1;32m    241\u001b[0m   \u001b[38;5;66;03m# For object proxies, first create the underlying type and then re-wrap it\u001b[39;00m\n\u001b[1;32m    242\u001b[0m   \u001b[38;5;66;03m# in the proxy type.\u001b[39;00m\n\u001b[1;32m    243\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(instance)(sequence_like(instance\u001b[38;5;241m.\u001b[39m__wrapped__, args))\n\u001b[0;32m--> 244\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(instance, CustomNestProtocol):\n\u001b[1;32m    245\u001b[0m   metadata \u001b[38;5;241m=\u001b[39m instance\u001b[38;5;241m.\u001b[39m__tf_flatten__()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    246\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m instance\u001b[38;5;241m.\u001b[39m__tf_unflatten__(metadata, \u001b[38;5;28mtuple\u001b[39m(args))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/typing.py:2029\u001b[0m, in \u001b[0;36m_ProtocolMeta.__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n\u001b[1;32m   2023\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2024\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_is_protocol:\n\u001b[1;32m   2025\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mhasattr\u001b[39m(instance, attr) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m   2026\u001b[0m             \u001b[38;5;66;03m# All *methods* can be blocked by setting them to None.\u001b[39;00m\n\u001b[1;32m   2027\u001b[0m             (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, attr, \u001b[38;5;28;01mNone\u001b[39;00m)) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   2028\u001b[0m              \u001b[38;5;28mgetattr\u001b[39m(instance, attr) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 2029\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_get_protocol_attrs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m):\n\u001b[1;32m   2030\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2031\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__instancecheck__\u001b[39m(instance)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/typing.py:1941\u001b[0m, in \u001b[0;36m_get_protocol_attrs\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m   1939\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1940\u001b[0m annotations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(base, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__annotations__\u001b[39m\u001b[38;5;124m'\u001b[39m, {})\n\u001b[0;32m-> 1941\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(base\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(annotations\u001b[38;5;241m.\u001b[39mkeys()):\n\u001b[1;32m   1942\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m attr\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_abc_\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m attr \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m EXCLUDED_ATTRIBUTES:\n\u001b[1;32m   1943\u001b[0m         attrs\u001b[38;5;241m.\u001b[39madd(attr)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    print(f\"Epoch {epoch+1}/1\")\n",
    "    \n",
    "    decoder.train()\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    total_d_loss = 0\n",
    "    total_g_loss = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    with tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch+1}\", leave=False) as t:\n",
    "        for batch_idx, batch in t:\n",
    "            tokens, words, sizes, chunks = batch\n",
    "            chunks = chunks.to(device)\n",
    "            \n",
    "            # ======================================== #\n",
    "            # ========= Encoder Forward Pass ========= #\n",
    "            # ======================================== #\n",
    "            word_embeddings = torch.tensor(encoder(words)).unsqueeze(1).to(device)\n",
    "    \n",
    "    \n",
    "            # print(word_embeddings.shape, chunks.shape) -> torch.Size([512, 1, 300]) torch.Size([512, 11, 64, 64])\n",
    "            \n",
    "            \n",
    "            # ======================================== #\n",
    "            # ========= Decoder Forward Pass ========= #\n",
    "            # ======================================== #\n",
    "            latent_vectors = decoder(word_embeddings)\n",
    "            # print(latent_vectors.shape) -> torch.Size([512, 11, 300])\n",
    "            \n",
    "            \n",
    "            # ========================================== #\n",
    "            # ============ Train GANs Model ============ #\n",
    "            # ========================================== #\n",
    "            \n",
    "            ## Preprocess chunks for generating images parallely\n",
    "            start_token = torch.randn(1, 1, 64, 64).to(device)\n",
    "            context_images = torch.cat([start_token.repeat(chunks.shape[0], 1, 1, 1), chunks[:, :-1]], dim=1)\n",
    "            # plt.imshow(torch.cat([img for img in context_images[0]], dim=1).numpy())\n",
    "            # print(chunks.shape, context_images.shape) -> torch.Size([512, 11, 64, 64]) torch.Size([512, 11, 64, 64])\n",
    "            noise = torch.randn(chunks.shape[0], max_seq_len, 100)\n",
    "            d_loss, g_loss = train_step(chunks.unsqueeze(2), words[0], latent_vectors, context_images.unsqueeze(2), noise, batch_idx)\n",
    "            \n",
    "            total_d_loss += d_loss\n",
    "            total_g_loss += g_loss\n",
    "    \n",
    "            # Update tqdm bar with loss info\n",
    "            t.set_postfix(D_Loss=f\"{d_loss:.4f}\", G_Loss=f\"{g_loss:.4f}\")\n",
    "            \n",
    "            # break\n",
    "        print(f\"Epoch {epoch+1}: D Loss: {total_d_loss/len(dataloader):.4f}, G Loss: {total_g_loss/len(dataloader):.4f}\")\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = {\n",
    "#     'disc_state_dict': discriminator.state_dict(),\n",
    "#     'gen_state_dict': generator.state_dict(),\n",
    "#     'decoder_state_dict': decoder.state_dict(),\n",
    "#     'disc_optimizer_state_dict': d_opt.state_dict(),\n",
    "#     'gen_optimizer_state_dict': g_opt.state_dict(),\n",
    "# }\n",
    "\n",
    "# torch.save(checkpoint, \"c_disc_unet_gen_checkpoint_2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_emb = encoder([('चबता',)])\n",
    "# word_emb = torch.tensor(word_emb).unsqueeze(1).to(device)\n",
    "# vecs = decoder(word_emb)\n",
    "# start_token_new = torch.randn(1, 1, 64, 64).to(device)\n",
    "# noise_new = torch.randn(1, 1, 100).to(device)\n",
    "\n",
    "# # fake_images = generator(latent_vectors, context_images, noise)\n",
    "# res_img = [start_token_new]\n",
    "# for i in range(11):\n",
    "#     print(vecs[0][i].unsqueeze(0).unsqueeze(0)[0][0][0], noise_new.shape)\n",
    "#     fake = generator(vecs[0][i].unsqueeze(0).unsqueeze(0), res_img[-1].unsqueeze(1), noise_new)\n",
    "#     res_img.append(fake)\n",
    "\n",
    "# img_pcs = []\n",
    "# for i in res_img:\n",
    "#     img_pcs.append(i.detach().cpu().numpy()[0][0])\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.imshow(np.hstack(img_pcs[1:]), cmap=\"gray\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6545018,
     "sourceId": 10670395,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
